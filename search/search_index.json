{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About the Site This site contains my notes on various subjects. As I only write down important or interesting parts for me, they may not be good summaries.","title":"About the Site"},{"location":"#about-the-site","text":"This site contains my notes on various subjects. As I only write down important or interesting parts for me, they may not be good summaries.","title":"About the Site"},{"location":"Notes/books/DDIA/chapter1/","text":"1. Reliable, scalable, and maintainable applications A data-intensive application is typically built from standard building blocks. They usually need to: Store data ( databases ) Speed up reads ( caches ) Search data ( search indexes ) Send a message to another process asynchronously ( stream processing ) Periodically crunch data ( batch processing ) Reliability To work correctly even in the face of adversity . Scalability Reasonable ways of dealing with growth. Maintainability Be able to work on it productively . Reliability Typical expectations: Application performs the function the user expected Tolerate the user making mistakes Its performance is good The system prevents abuse Systems that anticipate faults and can cope with them are called fault-tolerant or resilient . A fault is usually defined as one component of the system deviating from its spec , whereas failure is when the system as a whole stops providing the required service to the user. You should generally prefer tolerating faults over preventing faults . Hardware faults . Until recently redundancy of hardware components was sufficient for most applications. As data volumes increase, more applications use a larger number of machines, proportionally increasing the rate of hardware faults. There is a move towards systems that tolerate the loss of entire machines . A system that tolerates machine failure can be patched one node at a time, without downtime of the entire system ( rolling upgrade ). Software errors . It is unlikely that a large number of hardware components will fail at the same time. Software errors are a systematic error within the system, they tend to cause many more system failures than uncorrelated hardware faults. Cascading failures, where a small fault in one component triggers a fault in another component, which in turn triggers further faults Human errors . Humans are known to be unreliable. Configuration errors by operators are a leading cause of outages. You can make systems more reliable: Minimising the opportunities for error: with admin interfaces that make easy to do the \"right thing\" and discourage the \"wrong thing\". Provide fully featured non-production sandbox environments where people can explore and experiment safely. Automated testing. Detailed and clear monitoring / metrics. Scalability This is how do we cope with increased load. We need to succinctly describe the current load on the system; only then we can discuss growth questions. Twitter example Twitter main operations Post tweet: a user can publish a new message to their followers (4.6k req/sec, over 12k req/sec peak) Home timeline: a user can view tweets posted by the people they follow (300k req/sec) Two ways of implementing those operations: Posting a tweet simply inserts the new tweet into a global collection of tweets. When a user requests their home timeline, look up all the people they follow, find all the tweets for those users, and merge them (sorted by time). This could be done with a SQL JOIN . Maintain a cache for each user's home timeline. When a user posts a tweet , look up all the people who follow that user, and insert the new tweet into each of their home timeline caches. Approach 1, systems struggle to keep up with the load of home timeline queries. So the company switched to approach 2. The average rate of published tweets is almost two orders of magnitude lower than the rate of home timeline reads. Downside of approach 2 is that posting a tweet now requires a lot of extra work. Some users have over 30 million followers. A single tweet may result in over 30 million writes to home timelines. Twitter moved to an hybrid of both approaches. Tweets continue to be fanned out to home timelines but a small number of users with a very large number of followers are fetched separately and merged with that user's home timeline when it is read, like in approach 1. Describing Performance What happens when the load increases: How is the performance affected? How much do you need to increase your resources? In a batch processing system such as Hadoop, we usually care about throughput , or the number of records we can process per second. Throughput The number of records we can process per second Response time Duration until a client receives a response. Latency Duration that a request is waiting to be handled. It's common to see the average response time of a service reported. However, the mean is not very good metric if you want to know your \"typical\" response time, it does not tell you how many users actually experienced that delay. Better to use percentiles. Median ( 50th percentile or p50 ). Half of user requests are served in less than the median response time, and the other half take longer than the median Percentiles 95th , 99th and 99.9th ( p95 , p99 and p999 ) are good to figure out how bad your outliners are. Amazon describes response time requirements for internal services in terms of the 99.9th percentile because the customers with the slowest requests are often those who have the most data. The most valuable customers. On the other hand, optimising for the 99.99th percentile would be too expensive. Service level objectives (SLOs) and service level agreements (SLAs) are contracts that define the expected performance and availability of a service. An SLA may state the median response time to be less than 200ms and a 99th percentile under 1s. These metrics set expectations for clients of the service and allow customers to demand a refund if the SLA is not met. Queueing delays often account for large part of the response times at high percentiles. It is important to measure times on the client side. When generating load artificially, the client needs to keep sending requests independently of the response time. In practice, if there are calls in parallel, the end-user request still needs to wait for the slowest of the parallel calls to complete. The chance of getting a slow call increases if an end-user request requires multiple backend calls. Approaches for coping with load Scaling up or vertical scaling : Moving to a more powerful machine Scaling out or horizontal scaling : Distributing the load across multiple smaller machines. Elastic systems: Automatically add computing resources when detected load increase. Quite useful if load is unpredictable. Distributing stateless services across multiple machines is fairly straightforward. Taking stateful data systems from a single node to a distributed setup can introduce a lot of complexity. Until recently it was common wisdom to keep your database on a single node. Maintainability The majority of the cost of software is in its ongoing maintenance. There are three design principles for software systems: Operability Make it easy for operation teams to keep the system running. Simplicity Easy for new engineers to understand the system by removing as much complexity as possible. Evolvability Make it easy for engineers to make changes to the system in the future. Operability: making life easy for operations A good operations team is responsible for Monitoring and quickly restoring service if it goes into bad state Tracking down the cause of problems Keeping software and platforms up to date Keeping tabs on how different systems affect each other Anticipating future problems Establishing good practices and tools for development Perform complex maintenance tasks, like platform migration Maintaining the security of the system Defining processes that make operations predictable Preserving the organisation's knowledge about the system Good operability means making routine tasks easy. Simplicity: managing complexity When complexity makes maintenance hard, budget and schedules are often overrun. There is a greater risk of introducing bugs. Making a system simpler means removing accidental complexity, as non inherent in the problem that the software solves (as seen by users). One of the best tools we have for removing accidental complexity is abstraction that hides the implementation details behind clean and simple to understand APIs and facades. Evolvability: making change easy Agile working patterns provide a framework for adapting to change. Functional requirements What the application should do Nonfunctional requirements General properties like security, reliability, compliance, scalability, compatibility and maintainability.","title":"1. Reliable, scalable, and maintainable applications"},{"location":"Notes/books/DDIA/chapter1/#1-reliable-scalable-and-maintainable-applications","text":"A data-intensive application is typically built from standard building blocks. They usually need to: Store data ( databases ) Speed up reads ( caches ) Search data ( search indexes ) Send a message to another process asynchronously ( stream processing ) Periodically crunch data ( batch processing ) Reliability To work correctly even in the face of adversity . Scalability Reasonable ways of dealing with growth. Maintainability Be able to work on it productively .","title":"1. Reliable, scalable, and maintainable applications"},{"location":"Notes/books/DDIA/chapter1/#reliability","text":"Typical expectations: Application performs the function the user expected Tolerate the user making mistakes Its performance is good The system prevents abuse Systems that anticipate faults and can cope with them are called fault-tolerant or resilient . A fault is usually defined as one component of the system deviating from its spec , whereas failure is when the system as a whole stops providing the required service to the user. You should generally prefer tolerating faults over preventing faults . Hardware faults . Until recently redundancy of hardware components was sufficient for most applications. As data volumes increase, more applications use a larger number of machines, proportionally increasing the rate of hardware faults. There is a move towards systems that tolerate the loss of entire machines . A system that tolerates machine failure can be patched one node at a time, without downtime of the entire system ( rolling upgrade ). Software errors . It is unlikely that a large number of hardware components will fail at the same time. Software errors are a systematic error within the system, they tend to cause many more system failures than uncorrelated hardware faults. Cascading failures, where a small fault in one component triggers a fault in another component, which in turn triggers further faults Human errors . Humans are known to be unreliable. Configuration errors by operators are a leading cause of outages. You can make systems more reliable: Minimising the opportunities for error: with admin interfaces that make easy to do the \"right thing\" and discourage the \"wrong thing\". Provide fully featured non-production sandbox environments where people can explore and experiment safely. Automated testing. Detailed and clear monitoring / metrics.","title":"Reliability"},{"location":"Notes/books/DDIA/chapter1/#scalability","text":"This is how do we cope with increased load. We need to succinctly describe the current load on the system; only then we can discuss growth questions.","title":"Scalability"},{"location":"Notes/books/DDIA/chapter1/#twitter-example","text":"Twitter main operations Post tweet: a user can publish a new message to their followers (4.6k req/sec, over 12k req/sec peak) Home timeline: a user can view tweets posted by the people they follow (300k req/sec) Two ways of implementing those operations: Posting a tweet simply inserts the new tweet into a global collection of tweets. When a user requests their home timeline, look up all the people they follow, find all the tweets for those users, and merge them (sorted by time). This could be done with a SQL JOIN . Maintain a cache for each user's home timeline. When a user posts a tweet , look up all the people who follow that user, and insert the new tweet into each of their home timeline caches. Approach 1, systems struggle to keep up with the load of home timeline queries. So the company switched to approach 2. The average rate of published tweets is almost two orders of magnitude lower than the rate of home timeline reads. Downside of approach 2 is that posting a tweet now requires a lot of extra work. Some users have over 30 million followers. A single tweet may result in over 30 million writes to home timelines. Twitter moved to an hybrid of both approaches. Tweets continue to be fanned out to home timelines but a small number of users with a very large number of followers are fetched separately and merged with that user's home timeline when it is read, like in approach 1.","title":"Twitter example"},{"location":"Notes/books/DDIA/chapter1/#describing-performance","text":"What happens when the load increases: How is the performance affected? How much do you need to increase your resources? In a batch processing system such as Hadoop, we usually care about throughput , or the number of records we can process per second. Throughput The number of records we can process per second Response time Duration until a client receives a response. Latency Duration that a request is waiting to be handled. It's common to see the average response time of a service reported. However, the mean is not very good metric if you want to know your \"typical\" response time, it does not tell you how many users actually experienced that delay. Better to use percentiles. Median ( 50th percentile or p50 ). Half of user requests are served in less than the median response time, and the other half take longer than the median Percentiles 95th , 99th and 99.9th ( p95 , p99 and p999 ) are good to figure out how bad your outliners are. Amazon describes response time requirements for internal services in terms of the 99.9th percentile because the customers with the slowest requests are often those who have the most data. The most valuable customers. On the other hand, optimising for the 99.99th percentile would be too expensive. Service level objectives (SLOs) and service level agreements (SLAs) are contracts that define the expected performance and availability of a service. An SLA may state the median response time to be less than 200ms and a 99th percentile under 1s. These metrics set expectations for clients of the service and allow customers to demand a refund if the SLA is not met. Queueing delays often account for large part of the response times at high percentiles. It is important to measure times on the client side. When generating load artificially, the client needs to keep sending requests independently of the response time. In practice, if there are calls in parallel, the end-user request still needs to wait for the slowest of the parallel calls to complete. The chance of getting a slow call increases if an end-user request requires multiple backend calls.","title":"Describing Performance"},{"location":"Notes/books/DDIA/chapter1/#approaches-for-coping-with-load","text":"Scaling up or vertical scaling : Moving to a more powerful machine Scaling out or horizontal scaling : Distributing the load across multiple smaller machines. Elastic systems: Automatically add computing resources when detected load increase. Quite useful if load is unpredictable. Distributing stateless services across multiple machines is fairly straightforward. Taking stateful data systems from a single node to a distributed setup can introduce a lot of complexity. Until recently it was common wisdom to keep your database on a single node.","title":"Approaches for coping with load"},{"location":"Notes/books/DDIA/chapter1/#maintainability","text":"The majority of the cost of software is in its ongoing maintenance. There are three design principles for software systems: Operability Make it easy for operation teams to keep the system running. Simplicity Easy for new engineers to understand the system by removing as much complexity as possible. Evolvability Make it easy for engineers to make changes to the system in the future.","title":"Maintainability"},{"location":"Notes/books/DDIA/chapter1/#operability-making-life-easy-for-operations","text":"A good operations team is responsible for Monitoring and quickly restoring service if it goes into bad state Tracking down the cause of problems Keeping software and platforms up to date Keeping tabs on how different systems affect each other Anticipating future problems Establishing good practices and tools for development Perform complex maintenance tasks, like platform migration Maintaining the security of the system Defining processes that make operations predictable Preserving the organisation's knowledge about the system Good operability means making routine tasks easy.","title":"Operability: making life easy for operations"},{"location":"Notes/books/DDIA/chapter1/#simplicity-managing-complexity","text":"When complexity makes maintenance hard, budget and schedules are often overrun. There is a greater risk of introducing bugs. Making a system simpler means removing accidental complexity, as non inherent in the problem that the software solves (as seen by users). One of the best tools we have for removing accidental complexity is abstraction that hides the implementation details behind clean and simple to understand APIs and facades.","title":"Simplicity: managing complexity"},{"location":"Notes/books/DDIA/chapter1/#evolvability-making-change-easy","text":"Agile working patterns provide a framework for adapting to change. Functional requirements What the application should do Nonfunctional requirements General properties like security, reliability, compliance, scalability, compatibility and maintainability.","title":"Evolvability: making change easy"},{"location":"Notes/books/DDIA/chapter2/","text":"2. Data Models and Query Languages Most applications are built by layering one data model on top of another. Each layer hides the complexity of the layers below by providing a clean data model. These abstractions allow different groups of people to work effectively. Relational model vs document model The roots of relational databases lie in business data processing , transaction processing and batch processing . The goal was to hide the implementation details behind a cleaner interface. Not Only SQL has a few driving forces: Greater scalability preference for free and open source software Specialised query optimisations Desire for a more dynamic and expressive data model With a SQL model, if data is stored in a relational tables, an awkward translation layer is needed, this is called impedance mismatch . JSON model reduces the impedance mismatch and the lack of schema is often cited as an advantage. JSON representation has better locality than the multi-table SQL schema. All the relevant information is in one place, and one query is sufficient. In relational databases, it's normal to refer to rows in other tables by ID, because joins are easy. In document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak. If the database itself does not support joins, you have to emulate a join in application code by making multiple queries. The main arguments in favour of the document data model are schema flexibility, better performance due to locality, and sometimes closer data structures to the ones used by the applications. The relation model counters by providing better support for joins, and many-to-one and many-to-many relationships. If the data in your application has a document-like structure, then it's probably a good idea to use a document model. The relational technique of shredding can lead unnecessary complicated application code. The poor support for joins in document databases may or may not be a problem. Changes to records in document databases oftentimes have to rewrite the entire document. If you application does use many-to-many relationships, the document model becomes less appealing. Joins can be emulated in application code by making multiple requests. Using the document model can lead to significantly more complex application code and worse performance. Schema flexibility Most document databases do not enforce any schema on the data in documents. Arbitrary keys and values can be added to a document, when reading, clients have no guarantees as to what fields the documents may contain. Document databases are sometimes called schemaless , but maybe a more appropriate term is schema-on-read , in contrast to schema-on-write . Schema-on-read is similar to dynamic (runtime) type checking, whereas schema-on-write is similar to static (compile-time) type checking. The schema-on-read approach if the items on the collection don't have all the same structure (heterogeneous) Many different types of objects Data determined by external systems Data locality for queries If your application often needs to access the entire document, there is a performance advantage to this storage locality . The database typically needs to load the entire document, even if you access only a small portion of it. On updates, the entire document usually needs to be rewritten, it is recommended that you keep documents fairly small. Convergence of document and relational databases PostgreSQL does support JSON documents. RethinkDB supports relational-like joins in its query language and some MongoDB drivers automatically resolve database references. Relational and document databases are becoming more similar over time. Query languages for data SQL is a declarative query language. In an imperative language , you tell the computer to perform certain operations in order. In a declarative query language you just specify the pattern of the data you want, but not how to achieve that goal. A declarative query language hides implementation details of the database engine, making it possible for the database system to introduce performance improvements without requiring any changes to queries. Declarative languages often lend themselves to parallel execution while imperative code is very hard to parallelise across multiple cores because it specifies instructions that must be performed in a particular order. Declarative languages specify only the pattern of the results, not the algorithm that is used to determine results. Graph-like data models If many-to-many relationships are very common in your application, it becomes more natural to start modelling your data as a graph. A graph consists of vertices ( nodes or entities ) and edges ( relationships or arcs ). Well-known algorithms can operate on these graphs, like the shortest path between two points, or popularity of a web page. There are several ways of structuring and querying the data. The property graph model (implemented by Neo4j, Titan, and Infinite Graph) and the triple-store model (implemented by Datomic, AllegroGraph, and others). There are also three declarative query languages for graphs: Cypher, SPARQL, and Datalog. Property graphs Each vertex consists of: Unique identifier Outgoing edges Incoming edges Collection of properties (key-value pairs) Each edge consists of: Unique identifier Vertex at which the edge starts ( tail vertex ) Vertex at which the edge ends ( head vertex ) Label to describe the kind of relationship between the two vertices A collection of properties (key-value pairs) Graphs provide a great deal of flexibility for data modelling. Graphs are good for evolvability.","title":"2. Data Models and Query Languages"},{"location":"Notes/books/DDIA/chapter2/#2-data-models-and-query-languages","text":"Most applications are built by layering one data model on top of another. Each layer hides the complexity of the layers below by providing a clean data model. These abstractions allow different groups of people to work effectively.","title":"2. Data Models and Query Languages"},{"location":"Notes/books/DDIA/chapter2/#relational-model-vs-document-model","text":"The roots of relational databases lie in business data processing , transaction processing and batch processing . The goal was to hide the implementation details behind a cleaner interface. Not Only SQL has a few driving forces: Greater scalability preference for free and open source software Specialised query optimisations Desire for a more dynamic and expressive data model With a SQL model, if data is stored in a relational tables, an awkward translation layer is needed, this is called impedance mismatch . JSON model reduces the impedance mismatch and the lack of schema is often cited as an advantage. JSON representation has better locality than the multi-table SQL schema. All the relevant information is in one place, and one query is sufficient. In relational databases, it's normal to refer to rows in other tables by ID, because joins are easy. In document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak. If the database itself does not support joins, you have to emulate a join in application code by making multiple queries. The main arguments in favour of the document data model are schema flexibility, better performance due to locality, and sometimes closer data structures to the ones used by the applications. The relation model counters by providing better support for joins, and many-to-one and many-to-many relationships. If the data in your application has a document-like structure, then it's probably a good idea to use a document model. The relational technique of shredding can lead unnecessary complicated application code. The poor support for joins in document databases may or may not be a problem. Changes to records in document databases oftentimes have to rewrite the entire document. If you application does use many-to-many relationships, the document model becomes less appealing. Joins can be emulated in application code by making multiple requests. Using the document model can lead to significantly more complex application code and worse performance.","title":"Relational model vs document model"},{"location":"Notes/books/DDIA/chapter2/#schema-flexibility","text":"Most document databases do not enforce any schema on the data in documents. Arbitrary keys and values can be added to a document, when reading, clients have no guarantees as to what fields the documents may contain. Document databases are sometimes called schemaless , but maybe a more appropriate term is schema-on-read , in contrast to schema-on-write . Schema-on-read is similar to dynamic (runtime) type checking, whereas schema-on-write is similar to static (compile-time) type checking. The schema-on-read approach if the items on the collection don't have all the same structure (heterogeneous) Many different types of objects Data determined by external systems","title":"Schema flexibility"},{"location":"Notes/books/DDIA/chapter2/#data-locality-for-queries","text":"If your application often needs to access the entire document, there is a performance advantage to this storage locality . The database typically needs to load the entire document, even if you access only a small portion of it. On updates, the entire document usually needs to be rewritten, it is recommended that you keep documents fairly small.","title":"Data locality for queries"},{"location":"Notes/books/DDIA/chapter2/#convergence-of-document-and-relational-databases","text":"PostgreSQL does support JSON documents. RethinkDB supports relational-like joins in its query language and some MongoDB drivers automatically resolve database references. Relational and document databases are becoming more similar over time.","title":"Convergence of document and relational databases"},{"location":"Notes/books/DDIA/chapter2/#query-languages-for-data","text":"SQL is a declarative query language. In an imperative language , you tell the computer to perform certain operations in order. In a declarative query language you just specify the pattern of the data you want, but not how to achieve that goal. A declarative query language hides implementation details of the database engine, making it possible for the database system to introduce performance improvements without requiring any changes to queries. Declarative languages often lend themselves to parallel execution while imperative code is very hard to parallelise across multiple cores because it specifies instructions that must be performed in a particular order. Declarative languages specify only the pattern of the results, not the algorithm that is used to determine results.","title":"Query languages for data"},{"location":"Notes/books/DDIA/chapter2/#graph-like-data-models","text":"If many-to-many relationships are very common in your application, it becomes more natural to start modelling your data as a graph. A graph consists of vertices ( nodes or entities ) and edges ( relationships or arcs ). Well-known algorithms can operate on these graphs, like the shortest path between two points, or popularity of a web page. There are several ways of structuring and querying the data. The property graph model (implemented by Neo4j, Titan, and Infinite Graph) and the triple-store model (implemented by Datomic, AllegroGraph, and others). There are also three declarative query languages for graphs: Cypher, SPARQL, and Datalog.","title":"Graph-like data models"},{"location":"Notes/books/DDIA/chapter2/#property-graphs","text":"Each vertex consists of: Unique identifier Outgoing edges Incoming edges Collection of properties (key-value pairs) Each edge consists of: Unique identifier Vertex at which the edge starts ( tail vertex ) Vertex at which the edge ends ( head vertex ) Label to describe the kind of relationship between the two vertices A collection of properties (key-value pairs) Graphs provide a great deal of flexibility for data modelling. Graphs are good for evolvability.","title":"Property graphs"},{"location":"Notes/books/DDIA/chapter3/","text":"3. Storage and retrieval Databases need to do two things: store the data and give the data back to you. Data structures that power up your database Many databases use a log , which is append-only data file. Real databases have more issues to deal with tho (concurrency control, reclaiming disk space so the log doesn't grow forever and handling errors and partially written records). A log is an append-only sequence of records In order to efficiently find the value for a particular key, we need a different data structure: an index . An index is an additional structure that is derived from the primary data. Well-chosen indexes speed up read queries, but every index slows down writes. Hash Indexes The simplest possible indexing strategy is this: keep an in-memory hash map where every key is mapped to a byte offset in the data file\u2014the location at which the value can be found. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote (this works both for inserting new keys and for updating existing keys). When you want to look up a value, use the hash map to find the offset in the data file, seek to that location, and read the value. So we only ever append to a file - how do we avoid eventually running out of disk space? A good solution is to break the log into segments of a certain size by closing a segment file when it reaches a certain size, and making subsequent writes to a new segment file. We can then perform compaction on these segments, while merging segments simultaneously, as illustrated in Figure 3-3. After the merging process is complete, we switch read requests to use the new merged segment instead of the old segments, and the old segment files can simply be deleted. Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to find a value for a key, we first check the most recent segment hash map; if the key is not present we check the second-most recent segment and so on. The merging process keeps the number of segments small, so lookups don't need to check many hash maps. Some issues that are important in a real implementation: File format. It is simpler to use binary format. Deleting records. Append special deletion record to the data file ( tombstone ) that tells the merging process to discard previous values. Crash recovery. If restarted, the in-memory hash maps are lost. You can recover from reading each segment but that would take long time. Bitcask speeds up recovery by storing a snapshot of each segment hash map on disk. Partially written records. The database may crash at any time. Bitcask includes checksums allowing corrupted parts of the log to be detected and ignored. Concurrency control. As writes are appended to the log in a strictly sequential order, a common implementation is to have a single writer thread. Segments are immutable, so they can be read concurrently by multiple threads. Append-only design turns out to be good for several reasons: Appending and segment merging are sequential write operations, much faster than random writes, especially on magnetic spinning-disks. Concurrency and crash recovery are much simpler. Merging old segments avoids files getting fragmented over time. Hash table has its limitations too: The hash table must fit in memory. It is difficult to make an on-disk hash map perform well. Range queries are not efficient. SSTables and LSM-Trees We introduce a new requirement to segment files: we require that the sequence of key-value pairs is sorted by key . We call this Sorted String Table , or SSTable . We require that each key only appears once within each merged segment file (compaction already ensures that). SSTables have few big advantages over log segments with hash indexes Merging segments is simple and efficient (we can use algorithms like mergesort ). When multiple segments contain the same key, we can keep the value from the most recent segment and discard the values in older segments. You no longer need to keep an index of all the keys in memory. For a key like handiwork , when you know the offsets for the keys handback and handsome , you know handiwork must appear between those two. You can jump to the offset for handback and scan from there until you find handiwork , if not, the key is not present. You still need an in-memory index to tell you the offsets for some of the keys. One key for every few kilobytes of segment file is sufficient. Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk. This saves disk space and reduces I/O bandwidth use. Constructing and maintaining SSTables How do we get the data sorted in the first place? With red-black trees or AVL trees, you can insert keys in any order and read them back in sorted order. When a write comes in, add it to an in-memory balanced tree structure ( memtable ). When the memtable gets bigger than some threshold (~ few megabytes), write it out to disk as an SSTable file. Writes can continue to a new memtable instance. On a read request, try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc. From time to time, run merging and compaction in the background to discard overwritten and deleted values. If the database crashes, the most recent writes are lost. We can keep a separate log on disk to which every write is immediately appended. That log is not in sorted order, but that doesn't matter, because its only purpose is to restore the memtable after crash. Every time the memtable is written out to an SSTable, the log can be discarded. Making an LSM-tree out of SSTables The algorithm described here is essentialy what is used in LevelDB and RocksDB (backs Facebook MySQL), key-value storage engine libraries. Similar storage engines are used in Cassandra and Hbase both of which were inspired by Google's Bigtable (which introduced SSTable and memtable) Log Structure Merge-Trees Storage engines that are based on this principle of merging and compacting sorted files. LSM-tree enables fast writes by buffering incoming data in memory and flushing it as independent sorted batches to storage whenever the buffer is full. To enable fast reads, LSM-tree sort-merges batches in storage to restrict the number that reads have to search, and it also uses in-memory Bloom filters to enable point reads to probabilistically skip accessing batches that do not contain a target entry. LSM-tree algorithm can be slow when looking up keys that don't exist in the database. To optimise this, storage engines often use additional Bloom filters (a memory-efficient data structure for approximating the contents of a set). There are also different strategies to determine the order and timing of how SSTables are compacted and merged. Mainly two size-tiered and leveled compaction. LevelDB and RocksDB use leveled compaction, HBase use size-tiered, and Cassandra supports both. In size-tiered compaction, newer and smaller SSTables are successively merged into older and larger SSTables. In leveled compaction, the key range is split up into smaller SSTables and older data is moved into separate \"levels\", which allows the compaction to use less disk space. Tiering (size) vs Leveling Compaction Below is not from the book but from a great video lecture Scaling Write-Intensive Key-Value Stores The more greedy we want merging to be, the higher the cost of writes is going to be because we're going to be on average merging each entry more times. But the more greedy merging is going to be, we will have better reads because reads will have fewer runs in the system to have access. Tiering Write optimized Leveling Read optimized In a Tiered LSM tree, each level just gathers runs from the previous level and only when it reaches capacity does it merge these runs. We do less work per write here. On the other hand, with a Level LSM tree, a merge operation occurs at a level as soon as a new run comes in from the previous level. When R = 2, the performances converge as there would only be 1 run per level. If R becomes very large, we never merge so it becomes a log. B-trees This is the most widely used indexing structure. B-trees keep key-value pairs sorted by key, which allows efficient key-value lookups and range queries. The log-structured indexes break the database down into variable-size segments typically several megabytes or more. B-trees break the database down into fixed-size blocks or pages , traditionally 4KB. One page is designated as the root and you start from there. The page contains several keys and references to child pages. If you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk. If you want to add new key, find the page and add it to the page. If there isn't enough free space in the page to accommodate the new key, it is split in two half-full pages, and the parent page is updated to account for the new subdivision of key ranges. Trees remain balanced . A B-tree with n keys always has a depth of O (log n ). Also depends on branching factor. Making B-trees reliable The basic underlying write operation of a B-tree is to overwrite a page on disk with new data. It is assumed that the overwrite does not change the location of the page, all references to that page remain intact. This is a big contrast to log-structured indexes such as LSM-trees, which only append to files. Some operations require several different pages to be overwritten. When you split a page, you need to write the two pages that were split, and also overwrite their parent. If the database crashes after only some of the pages have been written, you end up with a corrupted index. It is common to include an additional data structure on disk: a write-ahead log (WAL, also know as the redo log ). Careful concurrency control is required if multiple threads are going to access, typically done protecting the tree internal data structures with latches (lightweight locks). B-trees and LSM-trees LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads. Reads are typically slower on LSM-tress as they have to check several different data structures and SSTables at different stages of compaction. Advantages of LSM-trees LSM-trees are typically able to sustain higher write throughput than B-trees, party because they sometimes have lower write amplification : a write to the database results in multiple writes to disk. The more a storage engine writes to disk, the fewer writes per second it can handle. LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. B-trees tend to leave disk space unused due to fragmentation: when a page is split or when a row cannot fit into an existing page, some space in a page remains unused. Since LSM trees are not page oriented and periodicially rewrite SSTables to remove fragmentation, they have lower storage overhead, especially when using leveled compaction. Downsides of LSM-trees Compaction process can sometimes interfere with the performance of ongoing reads and writes. B-trees can be more predictable. The bigger the database, the the more disk bandwidth is required for compaction. Compaction cannot keep up with the rate of incoming writes, if not configured properly you can run out of disk space. On B-trees, each key exists in exactly one place in the index. This offers strong transactional semantics. Transaction isolation is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree. Facebook MyRocks In the past, Facebook used InnoDB, a B+Tree based storage engine as the backend. The challenge was to find an index structure using less space and write amplification [1]. LSM-tree [2] has the potential to greatly improve these two bottlenecks. https://vldb.org/pvldb/vol13/p3217-matsunobu.pdf Other indexing structures We've only discussed key-value indexes, which are like primary key index. There are also secondary indexes . A secondary index can be easily constructed from a key-value index. The main difference is that in a secondary index, the indexed values are not necessarily unique. There are two ways of doing this: making each value in the index a list of matching row identifiers or by making each key unique by appending a row identifier to it. Keeping everything in memory Disks have two significant advantages: they are durable, and they have lower cost per gigabyte than RAM. It's quite feasible to keep them entirely in memory, this has lead to in-memory databases. Key-value stores, such as Memcached are intended for cache only, it's acceptable for data to be lost if the machine is restarted. Other in-memory databases aim for durability, with special hardware, writing a log of changes to disk, writing periodic snapshots to disk or by replicating in-memory sate to other machines. When an in-memory database is restarted, it needs to reload its state, either from disk or over the network from a replica. The disk is merely used as an append-only log for durability, and reads are served entirely from memory. Products such as VoltDB, MemSQL, and Oracle TimesTime are in-memory databases. Redis and Couchbase provide weak durability. Counterintuitively, the performance advantage of in-memory databases is not due to the fact that they don\u2019t need to read from disk. Even a disk-based storage engine may never need to read from disk if you have enough memory, because the operating system caches recently used disk blocks in memory anyway. Rather, they can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk Transaction processing or analytics? A transaction is a group of reads and writes that form a logical unit, this pattern became known as online transaction processing (OLTP). Data analytics has very different access patterns. A query would need to scan over a huge number of records, only reading a few columns per record, and calculates aggregate statistics. These queries are often written by business analysts, and fed into reports. This pattern became known for online analytics processing (OLAP). Data warehousing A data warehouse is a separate database that analysts can query to their heart's content without affecting OLTP operations. It contains read-only copy of the dat in all various OLTP systems in the company. Data is extracted out of OLTP databases (through periodic data dump or a continuous stream of update), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse (process Extract-Transform-Load or ETL). A data warehouse is most commonly relational, but the internals of the systems can look quite different. Amazon RedShift is hosted version of ParAccel. Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill. Some of them are based on ideas from Google's Dremel. Data warehouses are used in fairly formulaic style known as a star schema . Facts are captured as individual events, because this allows maximum flexibility of analysis later. The fact table can become extremely large. Dimensions represent the who , what , where , when , how and why of the event. The name \"star schema\" comes from the fact than when the table relationships are visualised, the fact table is in the middle, surrounded by its dimension tables, like the rays of a star. Fact tables often have over 100 columns, sometimes several hundred. Dimension tables can also be very wide. Column-oriented storage In a row-oriented storage engine, when you do a query that filters on a specific field, the engine will load all those rows with all their fields into memory, parse them and filter out the ones that don't meet the requirement. This can take a long time. Column-oriented storage is simple: don't store all the values from one row together, but store all values from each column together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in a query, which can save a lot of work. Column-oriented storage often lends itself very well to compression as the sequences of values for each column look quite repetitive, which is a good sign for compression. A technique that is particularly effective in data warehouses is bitmap encoding . Bitmap indexes are well suited for all kinds of queries that are common in a data warehouse. Cassandra and HBase have a concept of column families , which they inherited from Bigtable. Besides reducing the volume of data that needs to be loaded from disk, column-oriented storage layouts are also good for making efficient use of CPU cycles ( vectorised processing ). Column-oriented storage, compression, and sorting helps to make read queries faster and make sense in data warehouses, where most of the load consist on large read-only queries run by analysts. The downside is that writes are more difficult. An update-in-place approach, like B-tree use, is not possible with compressed columns. If you insert a row in the middle of a sorted table, you would most likely have to rewrite all column files. It's worth mentioning materialised aggregates as some cache of the counts ant the sums that queries use most often. A way of creating such a cache is with a materialised view , on a relational model this is usually called a virtual view : a table-like object whose contents are the results of some query. A materialised view is an actual copy of the query results, written in disk, whereas a virtual view is just a shortcut for writing queries. When the underlying data changes, a materialised view needs to be updated, because it is denormalised copy of the data. Database can do it automatically, but writes would become more expensive. A common special case of a materialised view is know as a data cube or OLAP cube , a grid of aggregates grouped by different dimensions.","title":"3. Storage and retrieval"},{"location":"Notes/books/DDIA/chapter3/#3-storage-and-retrieval","text":"Databases need to do two things: store the data and give the data back to you.","title":"3. Storage and retrieval"},{"location":"Notes/books/DDIA/chapter3/#data-structures-that-power-up-your-database","text":"Many databases use a log , which is append-only data file. Real databases have more issues to deal with tho (concurrency control, reclaiming disk space so the log doesn't grow forever and handling errors and partially written records). A log is an append-only sequence of records In order to efficiently find the value for a particular key, we need a different data structure: an index . An index is an additional structure that is derived from the primary data. Well-chosen indexes speed up read queries, but every index slows down writes.","title":"Data structures that power up your database"},{"location":"Notes/books/DDIA/chapter3/#hash-indexes","text":"The simplest possible indexing strategy is this: keep an in-memory hash map where every key is mapped to a byte offset in the data file\u2014the location at which the value can be found. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote (this works both for inserting new keys and for updating existing keys). When you want to look up a value, use the hash map to find the offset in the data file, seek to that location, and read the value. So we only ever append to a file - how do we avoid eventually running out of disk space? A good solution is to break the log into segments of a certain size by closing a segment file when it reaches a certain size, and making subsequent writes to a new segment file. We can then perform compaction on these segments, while merging segments simultaneously, as illustrated in Figure 3-3. After the merging process is complete, we switch read requests to use the new merged segment instead of the old segments, and the old segment files can simply be deleted. Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to find a value for a key, we first check the most recent segment hash map; if the key is not present we check the second-most recent segment and so on. The merging process keeps the number of segments small, so lookups don't need to check many hash maps. Some issues that are important in a real implementation: File format. It is simpler to use binary format. Deleting records. Append special deletion record to the data file ( tombstone ) that tells the merging process to discard previous values. Crash recovery. If restarted, the in-memory hash maps are lost. You can recover from reading each segment but that would take long time. Bitcask speeds up recovery by storing a snapshot of each segment hash map on disk. Partially written records. The database may crash at any time. Bitcask includes checksums allowing corrupted parts of the log to be detected and ignored. Concurrency control. As writes are appended to the log in a strictly sequential order, a common implementation is to have a single writer thread. Segments are immutable, so they can be read concurrently by multiple threads. Append-only design turns out to be good for several reasons: Appending and segment merging are sequential write operations, much faster than random writes, especially on magnetic spinning-disks. Concurrency and crash recovery are much simpler. Merging old segments avoids files getting fragmented over time. Hash table has its limitations too: The hash table must fit in memory. It is difficult to make an on-disk hash map perform well. Range queries are not efficient.","title":"Hash Indexes"},{"location":"Notes/books/DDIA/chapter3/#sstables-and-lsm-trees","text":"We introduce a new requirement to segment files: we require that the sequence of key-value pairs is sorted by key . We call this Sorted String Table , or SSTable . We require that each key only appears once within each merged segment file (compaction already ensures that). SSTables have few big advantages over log segments with hash indexes Merging segments is simple and efficient (we can use algorithms like mergesort ). When multiple segments contain the same key, we can keep the value from the most recent segment and discard the values in older segments. You no longer need to keep an index of all the keys in memory. For a key like handiwork , when you know the offsets for the keys handback and handsome , you know handiwork must appear between those two. You can jump to the offset for handback and scan from there until you find handiwork , if not, the key is not present. You still need an in-memory index to tell you the offsets for some of the keys. One key for every few kilobytes of segment file is sufficient. Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk. This saves disk space and reduces I/O bandwidth use.","title":"SSTables and LSM-Trees"},{"location":"Notes/books/DDIA/chapter3/#b-trees","text":"This is the most widely used indexing structure. B-trees keep key-value pairs sorted by key, which allows efficient key-value lookups and range queries. The log-structured indexes break the database down into variable-size segments typically several megabytes or more. B-trees break the database down into fixed-size blocks or pages , traditionally 4KB. One page is designated as the root and you start from there. The page contains several keys and references to child pages. If you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk. If you want to add new key, find the page and add it to the page. If there isn't enough free space in the page to accommodate the new key, it is split in two half-full pages, and the parent page is updated to account for the new subdivision of key ranges. Trees remain balanced . A B-tree with n keys always has a depth of O (log n ). Also depends on branching factor.","title":"B-trees"},{"location":"Notes/books/DDIA/chapter3/#b-trees-and-lsm-trees","text":"LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads. Reads are typically slower on LSM-tress as they have to check several different data structures and SSTables at different stages of compaction.","title":"B-trees and LSM-trees"},{"location":"Notes/books/DDIA/chapter3/#other-indexing-structures","text":"We've only discussed key-value indexes, which are like primary key index. There are also secondary indexes . A secondary index can be easily constructed from a key-value index. The main difference is that in a secondary index, the indexed values are not necessarily unique. There are two ways of doing this: making each value in the index a list of matching row identifiers or by making each key unique by appending a row identifier to it.","title":"Other indexing structures"},{"location":"Notes/books/DDIA/chapter3/#transaction-processing-or-analytics","text":"A transaction is a group of reads and writes that form a logical unit, this pattern became known as online transaction processing (OLTP). Data analytics has very different access patterns. A query would need to scan over a huge number of records, only reading a few columns per record, and calculates aggregate statistics. These queries are often written by business analysts, and fed into reports. This pattern became known for online analytics processing (OLAP).","title":"Transaction processing or analytics?"},{"location":"Notes/books/DDIA/chapter3/#data-warehousing","text":"A data warehouse is a separate database that analysts can query to their heart's content without affecting OLTP operations. It contains read-only copy of the dat in all various OLTP systems in the company. Data is extracted out of OLTP databases (through periodic data dump or a continuous stream of update), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse (process Extract-Transform-Load or ETL). A data warehouse is most commonly relational, but the internals of the systems can look quite different. Amazon RedShift is hosted version of ParAccel. Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill. Some of them are based on ideas from Google's Dremel. Data warehouses are used in fairly formulaic style known as a star schema . Facts are captured as individual events, because this allows maximum flexibility of analysis later. The fact table can become extremely large. Dimensions represent the who , what , where , when , how and why of the event. The name \"star schema\" comes from the fact than when the table relationships are visualised, the fact table is in the middle, surrounded by its dimension tables, like the rays of a star. Fact tables often have over 100 columns, sometimes several hundred. Dimension tables can also be very wide.","title":"Data warehousing"},{"location":"Notes/books/DDIA/chapter3/#column-oriented-storage","text":"In a row-oriented storage engine, when you do a query that filters on a specific field, the engine will load all those rows with all their fields into memory, parse them and filter out the ones that don't meet the requirement. This can take a long time. Column-oriented storage is simple: don't store all the values from one row together, but store all values from each column together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in a query, which can save a lot of work. Column-oriented storage often lends itself very well to compression as the sequences of values for each column look quite repetitive, which is a good sign for compression. A technique that is particularly effective in data warehouses is bitmap encoding . Bitmap indexes are well suited for all kinds of queries that are common in a data warehouse. Cassandra and HBase have a concept of column families , which they inherited from Bigtable. Besides reducing the volume of data that needs to be loaded from disk, column-oriented storage layouts are also good for making efficient use of CPU cycles ( vectorised processing ). Column-oriented storage, compression, and sorting helps to make read queries faster and make sense in data warehouses, where most of the load consist on large read-only queries run by analysts. The downside is that writes are more difficult. An update-in-place approach, like B-tree use, is not possible with compressed columns. If you insert a row in the middle of a sorted table, you would most likely have to rewrite all column files. It's worth mentioning materialised aggregates as some cache of the counts ant the sums that queries use most often. A way of creating such a cache is with a materialised view , on a relational model this is usually called a virtual view : a table-like object whose contents are the results of some query. A materialised view is an actual copy of the query results, written in disk, whereas a virtual view is just a shortcut for writing queries. When the underlying data changes, a materialised view needs to be updated, because it is denormalised copy of the data. Database can do it automatically, but writes would become more expensive. A common special case of a materialised view is know as a data cube or OLAP cube , a grid of aggregates grouped by different dimensions.","title":"Column-oriented storage"},{"location":"Notes/books/DDIA/chapter4/","text":"4. Encoding and evolution Change to an application's features also requires a change to data it stores. Relational databases conforms to one schema although that schema can be changed, there is one schema in force at any point in time. Schema-on-read (or schemaless) contain a mixture of older and newer data formats. In large applications changes don't happen instantaneously. You want to perform a rolling upgrade and deploy a new version to a few nodes at a time, gradually working your way through all the nodes without service downtime. Old and new versions of the code, and old and new data formats, may potentially all coexist. We need to maintain compatibility in both directions Backward compatibility newer code can read data that was written by older code. Forward compatibility older code can read data that was written by newer code. Formats for encoding data Programs usually work with data in atleast two different representations: In memory When you want to write data to a file or send it over the network, you have to encode it Thus, you need a translation between the two representations. In-memory representation to byte sequence is called encoding ( serialisation or marshalling ), and the reverse is called decoding ( parsing , deserialisation or unmarshalling ). Programming languages come with built-in support for encoding in-memory objects into byte sequences, but is usually a bad idea to use them. Precisely because of a few problems. Often tied to a particular programming language. The decoding process needs to be able to instantiate arbitrary classes and this is frequently a security hole. Versioning Efficiency Standardised encodings can be written and read by many programming languages. JSON, XML, and Binary Variants JSON, XML, and CSV are human-readable and popular specially as data interchange formats, but they have some subtle problems: Ambiguity around the encoding of numbers and dealing with large numbers Support of Unicode character strings, but no support for binary strings. People get around this by encoding binary data as Base64, which increases the data size by 33%. There is optional schema support for both XML and JSON CSV does not have any schema, so it is up to the application to define the meaning Binary encoding JSON is less verbose than XML, but both still use a lot of space compared to binary formats. There are binary encodings for JSON (MesagePack, BSON, BJSON, UBJSON, BISON and Smile), similar thing for XML (WBXML and Fast Infoset). Apache Thrift and Protocol Buffers Apache Thrift and Protocol buffers are binary encoding libraries that are based on the same principle. They require a schema for any data that is encoded. 1 2 3 4 5 struct Person { 1 : required string userName , 2 : optional i64 favoriteNumber , 3 : optional list < string > interests } Thrift offers two different protocols: BinaryProtocol there are no field names like userName , favouriteNumber . Instead the data contains field tags , which are numbers ( 1 , 2 ) CompactProtocol which is equivalent to BinaryProtocol but it packs the same information in less space. It packs the field type and the tag number into the same byte. Protocol Buffers are very similar to Thrift's CompactProtocol, bit packing is a bit different and that might allow smaller compression. Field tags and schema evolution Schemas inevitable need to change over time ( schema evolution ), how do Thrift and Protocol Buffers handle schema changes while keeping backward and forward compatibility changes? Forward compatible support . As with new fields you add new tag numbers, old code trying to read new code, it can simply ignore not recognised tags. Backwards compatible support . As long as each field has a unique tag number, new code can always read old data. Every field you add after initial deployment of schema must be optional or have a default value. Removing fields is just like adding a field with backward and forward concerns reversed. You can only remove a field that is optional, and you can never use the same tag again. What about changing the data type of a field? There is a risk that values will lose precision or get truncated. Avro Apache Avro is another binary format that has two schema languages, one intended for human editing (Avro IDL), and one (based on JSON) that is more easily machine-readable. You go go through the fields in the order they appear in the schema and use the schema to tell you the datatype of each field. Any mismatch in the schema between the reader and the writer would mean incorrectly decoded data. What about schema evolution? When an application wants to encode some data, it encodes the data using whatever version of the schema it knows ( writer's schema ). When an application wants to decode some data, it is expecting the data to be in some schema ( reader's schema ). In Avro the writer's schema and the reader's schema don't have to be the same . The Avro library resolves the differences by looking at the writer's schema and the reader's schema. Forward compatibility means you can have a new version of the schema as writer and an old version of the schema as reader. Conversely, backward compatibility means that you can have a new version of the schema as reader and an old version as writer. To maintain compatibility, you may only add or remove a field that has a default value. If you were to add a field that has no default value, new readers wouldn't be able to read data written by old writers. Changing the datatype of a field is possible, provided that Avro can convert the type. Changing the name of a filed is tricky (backward compatible but not forward compatible). The schema is identified encoded in the data. In a large file with lots of records, the writer of the file can just include the schema at the beginning of the file. On a database with individually written records, you cannot assume all the records will have the same schema, so you have to include a version number at the beginning of every encoded record. While sending records over the network, you can negotiate the schema version on connection setup. Avro is friendlier to dynamically generated schemas (dumping into a file the database). You can fairly easily generate an Avro schema in JSON. If the database schema changes, you can just generate a new Avro schema for the updated database schema and export data in the new Avro schema. By contrast with Thrift and Protocol Buffers, every time the database schema changes, you would have to manually update the mappings from database column names to field tags. Although textual formats such as JSON, XML and CSV are widespread, binary encodings based on schemas are also a viable option. As they have nice properties: Can be much more compact, since they can omit field names from the encoded data. Schema is a valuable form of documentation, required for decoding, you can be sure it is up to date. Database of schemas allows you to check forward and backward compatibility changes. Generate code from the schema is useful, since it enables type checking at compile time. Modes of dataflow Different ways data flows between processes Dataflow Through Databases The process that writes to the database encodes the data, and the process that reads from the database decodes it. A value in the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running. When a new version of your application is deployed, you may entirely replace the old version with the new version within a few minutes. The same is not true in databases, the five-year-old data will still be there, in the original encoding, unless you have explicitly rewritten it. Data outlives code . Rewriting ( migrating ) is expensive, most relational databases allow simple schema changes, such as adding a new column with a null default value without rewriting existing data. When an old row is read, the database fills in null s for any columns that are missing. Dataflow Through Services: REST and RPC You have processes that need to communicate over a network of clients and servers . The servers expose an API over the network, and the clients can connect to the servers to make requests to that API. The API exposed by the server is known as a service. Services are similar to databases, each service should be owned by one team. and that team should be able to release versions of the service frequently, without having to coordinate with other teams. We should expect old and new versions of servers and clients to be running at the same time. Web services When HTTP is used as the underlying protocol for talking to the service, it is called a web service. There are two popular approaches to web services: REST and SOAP. They are almost diametrically opposed in terms of philosophy, and often the subject of heated debate among their respective proponents. REST is not a protocol, but rather a design philosophy that builds upon the principles of HTTP. It emphasizes simple data formats, using URLs for identifying resources and using HTTP features for cache control, authentication, and content type negotiation. REST has been gaining popularity compared to SOAP, at least in the context of cross-organizational service integration [36], and is often associated with microservices [31]. An API designed according to the principles of REST is called RESTful. By contrast, SOAP is an XML-based protocol for making network API requests. Although it is most commonly used over HTTP, it aims to be independent from HTTP and avoids using most HTTP features. Instead, it comes with a sprawling and complex multitude of related standards (the web service framework, known as WS-*) that add various features. Amazon Coral is an example of SOAP as each service has a model.xml and types.xml defining the objects and operations. The problems with RPCs Remote procedure calls (RPC) tries to make a request to a remote network service look the same as calling a function or method in your programming language, it seems convenient at first but the approach is flawed: A network request is unpredictable while a local function call is not. A network request may return without a result, due a timeout . Retrying will cause the action to be performed multiple times, unless you build a mechanism for deduplication ( idempotence ). A network request is much slower than a function call, and its latency is wildly variable. Parameters need to be encoded into a sequence of bytes that can be sent over the network and becomes problematic with larger objects. The RPC framework must translate datatypes from one language to another, not all languages have the same types. There is no point trying to make a remote service look too much like a local object in your programming language, because it's a fundamentally different thing. New generation of RPC frameworks are more explicit about the fact that a remote request is different from a local function call. Fiangle and Rest.li use features ( promises ) to encapsulate asynchronous actions. RESTful API has some significant advantages like being good for experimentation and debugging. REST seems to be the predominant style for public APIs. The main focus of RPC frameworks is on requests between services owned by the same organisation, typically within the same datacenter. Dataflow via Message Passing In an asynchronous message-passing systems, a client's request (usually called a message ) is delivered to another process with low latency. The message goes via an intermediary called a message broker ( message queue or message-oriented middleware ) which stores the message temporarily. This has several advantages compared to direct RPC: It can act as a buffer if the recipient is unavailable or overloaded It can automatically redeliver messages to a process that has crashed and prevent messages from being lost It avoids the sender needing to know the IP address and port number of the recipient (useful in a cloud environment) It allows one message to be sent to several recipients Decouples the sender from the recipient The communication happens only in one direction. The sender doesn't wait for the message to be delivered, but simply sends it and then forgets about it ( asynchronous ). Open source implementations for message brokers are RabbitMQ, ActiveMQ, HornetQ, NATS, SQS/SNS, and Apache Kafka. One process sends a message to a named queue or topic and the broker ensures that the message is delivered to one or more consumers or subscribers to that queue or topic. Message brokers typically don't enforce a particular data model, you can use any encoding format. An actor model is a programming model for concurrency in a single process. Rather than dealing with threads (and their complications), logic is encapsulated in actors . Each actor typically represent one client or entity, it may have some local state, and it communicates with other actors by sending and receiving asynchronous messages. Message deliver is not guaranteed. Since each actor processes only one message at a time, it doesn't need to worry about threads. In distributed actor frameworks , this programming model is used to scale an application across multiple nodes. It basically integrates a message broker and the actor model into a single framework. Akka uses Java's built-in serialisation by default, which does not provide forward or backward compatibility. You can replace it with something like Protocol Buffers and the ability to do rolling upgrades. Orleans by default uses custom data encoding format that does not support rolling upgrade deployments. In Erlang OTP it is surprisingly hard to make changes to record schemas. Summary Rolling upgrades allow new versions of a service to be released without downtime. During rolling upgrades, or for various other reasons, we must assume that different nodes are running the different versions of our application\u2019s code. Thus, it is important that all data flowing around the system is encoded in a way that provides backward compatibility (new code can read old data) and forward compatibility (old code can read new data). We discussed several data encoding formats and their compatibility properties: Programming language\u2013specific encodings are restricted to a single programming language and often fail to provide forward and backward compatibility. Textual formats like JSON, XML, and CSV are widespread, and their compatibility depends on how you use them. They have optional schema languages, which are sometimes helpful and sometimes a hindrance. These formats are somewhat vague about datatypes, so you have to be careful with things like numbers and binary strings. Binary schema\u2013driven formats like Thrift, Protocol Buffers, and Avro allow compact, efficient encoding with clearly defined forward and backward compatibility semantics. The schemas can be useful for documentation and code generation in statically typed languages. However, they have the downside that data needs to be decoded before it is human-readable We also discussed several modes of dataflow, illustrating different scenarios in which data encodings are important: Databases, where the process writing to the database encodes the data and the process reading from the database decodes it RPC and REST APIs, where the client encodes a request, the server decodes the request and encodes a response, and the client finally decodes the response Asynchronous message passing (using message brokers or actors), where nodes communicate by sending each other messages that are encoded by the sender and decoded by the recipient","title":"4. Encoding and evolution"},{"location":"Notes/books/DDIA/chapter4/#4-encoding-and-evolution","text":"Change to an application's features also requires a change to data it stores. Relational databases conforms to one schema although that schema can be changed, there is one schema in force at any point in time. Schema-on-read (or schemaless) contain a mixture of older and newer data formats. In large applications changes don't happen instantaneously. You want to perform a rolling upgrade and deploy a new version to a few nodes at a time, gradually working your way through all the nodes without service downtime. Old and new versions of the code, and old and new data formats, may potentially all coexist. We need to maintain compatibility in both directions Backward compatibility newer code can read data that was written by older code. Forward compatibility older code can read data that was written by newer code.","title":"4. Encoding and evolution"},{"location":"Notes/books/DDIA/chapter4/#formats-for-encoding-data","text":"Programs usually work with data in atleast two different representations: In memory When you want to write data to a file or send it over the network, you have to encode it Thus, you need a translation between the two representations. In-memory representation to byte sequence is called encoding ( serialisation or marshalling ), and the reverse is called decoding ( parsing , deserialisation or unmarshalling ). Programming languages come with built-in support for encoding in-memory objects into byte sequences, but is usually a bad idea to use them. Precisely because of a few problems. Often tied to a particular programming language. The decoding process needs to be able to instantiate arbitrary classes and this is frequently a security hole. Versioning Efficiency Standardised encodings can be written and read by many programming languages.","title":"Formats for encoding data"},{"location":"Notes/books/DDIA/chapter4/#json-xml-and-binary-variants","text":"JSON, XML, and CSV are human-readable and popular specially as data interchange formats, but they have some subtle problems: Ambiguity around the encoding of numbers and dealing with large numbers Support of Unicode character strings, but no support for binary strings. People get around this by encoding binary data as Base64, which increases the data size by 33%. There is optional schema support for both XML and JSON CSV does not have any schema, so it is up to the application to define the meaning","title":"JSON, XML, and Binary Variants"},{"location":"Notes/books/DDIA/chapter4/#apache-thrift-and-protocol-buffers","text":"Apache Thrift and Protocol buffers are binary encoding libraries that are based on the same principle. They require a schema for any data that is encoded. 1 2 3 4 5 struct Person { 1 : required string userName , 2 : optional i64 favoriteNumber , 3 : optional list < string > interests } Thrift offers two different protocols: BinaryProtocol there are no field names like userName , favouriteNumber . Instead the data contains field tags , which are numbers ( 1 , 2 ) CompactProtocol which is equivalent to BinaryProtocol but it packs the same information in less space. It packs the field type and the tag number into the same byte. Protocol Buffers are very similar to Thrift's CompactProtocol, bit packing is a bit different and that might allow smaller compression.","title":"Apache Thrift and Protocol Buffers"},{"location":"Notes/books/DDIA/chapter4/#avro","text":"Apache Avro is another binary format that has two schema languages, one intended for human editing (Avro IDL), and one (based on JSON) that is more easily machine-readable. You go go through the fields in the order they appear in the schema and use the schema to tell you the datatype of each field. Any mismatch in the schema between the reader and the writer would mean incorrectly decoded data. What about schema evolution? When an application wants to encode some data, it encodes the data using whatever version of the schema it knows ( writer's schema ). When an application wants to decode some data, it is expecting the data to be in some schema ( reader's schema ). In Avro the writer's schema and the reader's schema don't have to be the same . The Avro library resolves the differences by looking at the writer's schema and the reader's schema. Forward compatibility means you can have a new version of the schema as writer and an old version of the schema as reader. Conversely, backward compatibility means that you can have a new version of the schema as reader and an old version as writer. To maintain compatibility, you may only add or remove a field that has a default value. If you were to add a field that has no default value, new readers wouldn't be able to read data written by old writers. Changing the datatype of a field is possible, provided that Avro can convert the type. Changing the name of a filed is tricky (backward compatible but not forward compatible). The schema is identified encoded in the data. In a large file with lots of records, the writer of the file can just include the schema at the beginning of the file. On a database with individually written records, you cannot assume all the records will have the same schema, so you have to include a version number at the beginning of every encoded record. While sending records over the network, you can negotiate the schema version on connection setup. Avro is friendlier to dynamically generated schemas (dumping into a file the database). You can fairly easily generate an Avro schema in JSON. If the database schema changes, you can just generate a new Avro schema for the updated database schema and export data in the new Avro schema. By contrast with Thrift and Protocol Buffers, every time the database schema changes, you would have to manually update the mappings from database column names to field tags. Although textual formats such as JSON, XML and CSV are widespread, binary encodings based on schemas are also a viable option. As they have nice properties: Can be much more compact, since they can omit field names from the encoded data. Schema is a valuable form of documentation, required for decoding, you can be sure it is up to date. Database of schemas allows you to check forward and backward compatibility changes. Generate code from the schema is useful, since it enables type checking at compile time.","title":"Avro"},{"location":"Notes/books/DDIA/chapter4/#modes-of-dataflow","text":"Different ways data flows between processes","title":"Modes of dataflow"},{"location":"Notes/books/DDIA/chapter4/#dataflow-through-databases","text":"The process that writes to the database encodes the data, and the process that reads from the database decodes it. A value in the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running. When a new version of your application is deployed, you may entirely replace the old version with the new version within a few minutes. The same is not true in databases, the five-year-old data will still be there, in the original encoding, unless you have explicitly rewritten it. Data outlives code . Rewriting ( migrating ) is expensive, most relational databases allow simple schema changes, such as adding a new column with a null default value without rewriting existing data. When an old row is read, the database fills in null s for any columns that are missing.","title":"Dataflow Through Databases"},{"location":"Notes/books/DDIA/chapter4/#dataflow-through-services-rest-and-rpc","text":"You have processes that need to communicate over a network of clients and servers . The servers expose an API over the network, and the clients can connect to the servers to make requests to that API. The API exposed by the server is known as a service. Services are similar to databases, each service should be owned by one team. and that team should be able to release versions of the service frequently, without having to coordinate with other teams. We should expect old and new versions of servers and clients to be running at the same time.","title":"Dataflow Through Services: REST and RPC"},{"location":"Notes/books/DDIA/chapter4/#dataflow-via-message-passing","text":"In an asynchronous message-passing systems, a client's request (usually called a message ) is delivered to another process with low latency. The message goes via an intermediary called a message broker ( message queue or message-oriented middleware ) which stores the message temporarily. This has several advantages compared to direct RPC: It can act as a buffer if the recipient is unavailable or overloaded It can automatically redeliver messages to a process that has crashed and prevent messages from being lost It avoids the sender needing to know the IP address and port number of the recipient (useful in a cloud environment) It allows one message to be sent to several recipients Decouples the sender from the recipient The communication happens only in one direction. The sender doesn't wait for the message to be delivered, but simply sends it and then forgets about it ( asynchronous ). Open source implementations for message brokers are RabbitMQ, ActiveMQ, HornetQ, NATS, SQS/SNS, and Apache Kafka. One process sends a message to a named queue or topic and the broker ensures that the message is delivered to one or more consumers or subscribers to that queue or topic. Message brokers typically don't enforce a particular data model, you can use any encoding format. An actor model is a programming model for concurrency in a single process. Rather than dealing with threads (and their complications), logic is encapsulated in actors . Each actor typically represent one client or entity, it may have some local state, and it communicates with other actors by sending and receiving asynchronous messages. Message deliver is not guaranteed. Since each actor processes only one message at a time, it doesn't need to worry about threads. In distributed actor frameworks , this programming model is used to scale an application across multiple nodes. It basically integrates a message broker and the actor model into a single framework. Akka uses Java's built-in serialisation by default, which does not provide forward or backward compatibility. You can replace it with something like Protocol Buffers and the ability to do rolling upgrades. Orleans by default uses custom data encoding format that does not support rolling upgrade deployments. In Erlang OTP it is surprisingly hard to make changes to record schemas.","title":"Dataflow via Message Passing"},{"location":"Notes/books/DDIA/chapter4/#summary","text":"Rolling upgrades allow new versions of a service to be released without downtime. During rolling upgrades, or for various other reasons, we must assume that different nodes are running the different versions of our application\u2019s code. Thus, it is important that all data flowing around the system is encoded in a way that provides backward compatibility (new code can read old data) and forward compatibility (old code can read new data). We discussed several data encoding formats and their compatibility properties: Programming language\u2013specific encodings are restricted to a single programming language and often fail to provide forward and backward compatibility. Textual formats like JSON, XML, and CSV are widespread, and their compatibility depends on how you use them. They have optional schema languages, which are sometimes helpful and sometimes a hindrance. These formats are somewhat vague about datatypes, so you have to be careful with things like numbers and binary strings. Binary schema\u2013driven formats like Thrift, Protocol Buffers, and Avro allow compact, efficient encoding with clearly defined forward and backward compatibility semantics. The schemas can be useful for documentation and code generation in statically typed languages. However, they have the downside that data needs to be decoded before it is human-readable We also discussed several modes of dataflow, illustrating different scenarios in which data encodings are important: Databases, where the process writing to the database encodes the data and the process reading from the database decodes it RPC and REST APIs, where the client encodes a request, the server decodes the request and encodes a response, and the client finally decodes the response Asynchronous message passing (using message brokers or actors), where nodes communicate by sending each other messages that are encoded by the sender and decoded by the recipient","title":"Summary"},{"location":"Notes/books/DDIA/chapter5/","text":"5. Replication Reasons why you might want to replicate data: To keep data geographically close to your users Increase availability Increase read throughput The difficulty in replication lies in handling changes to replicated data. Popular algorithms for replicating changes between nodes: single-leader , multi-leader , and leaderless replication. Leaders and followers Each node that stores a copy of the database is called a replica . Every write to the database needs to be processed by every replica. The most common solution for this is called leader-based replication ( active/passive or master-slave replication ). One of the replicas is designated the leader ( master or primary ). Writes to the database must send requests to the leader. Other replicas are known as followers ( read replicas , slaves , secondaries or hot stanbys ). The leader sends the data change to all of its followers as part of a replication log or change stream . Reads can be query the leader or any of the followers, while writes are only accepted on the leader. MySQL, Oracle Data Guard, SQL Server's AlwaysOn Availability Groups, MongoDB, RethinkDB, Espresso, Kafka and RabbitMQ are examples of these kind of databases. Synchronous vs asynchronous The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. The disadvantage is that it the synchronous follower doesn't respond, the write cannot be processed. It's impractical for all followers to be synchronous. If you enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous. This guarantees up-to-date copy of the data on at least two nodes (this is sometimes called semi-synchronous ). Often, leader-based replication is asynchronous. Writes are not guaranteed to be durable, the main advantage of this approach is that the leader can continue processing writes. Setting up new followers Copying data files from one node to another is typically not sufficient. Setting up a follower can usually be done without downtime. The process looks like: Take a snapshot of the leader's database Copy the snapshot to the follower node Follower requests data changes that have happened since the snapshot was taken Once follower processed the backlog of data changes since snapshot, it has caught up . Handling node outages How does high availability work with leader-based replication? Follower failure: catchup recovery On its local disk, each follower keeps a log of the data changes it has received from the leader. If a follower crashes and is restarted, or if the network between the leader and the follower is temporarily interrupted, the follower can recover quite easily: from its log, it knows the last transaction that was processed before the fault occur\u2010 red. Thus, the follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected. When it has applied these changes, it has caught up to the leader and can continue receiving a stream of data changes as before. Leader failure: failover Failover One of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader and followers need to start consuming data changes from the new leader. Automatic failover consists: Determining that the leader has failed. If a node does not respond in a period of time it's considered dead. Choosing a new leader. The best candidate for leadership is usually the replica with the most up-to-date changes from the old leader. Reconfiguring the system to use the new leader. The system needs to ensure that the old leader becomes a follower and recognises the new leader. Things that could go wrong: If asynchronous replication is used, the new leader may have received conflicting writes in the meantime. Discarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents. It could happen that two nodes both believe that they are the leader ( split brain ). Data is likely to be lost or corrupted. What is the right time before the leader is declared dead? A longer timeout means a longer time to recovery in the case where the leader fails. However, if the timeout is too short, there could be unnecessary failovers. For these reasons, some operation teams prefer to perform failovers manually, even if the software supports automatic failover. Implementation of replication logs Statement-based replication The leader logs every statement and sends it to its followers (every INSERT , UPDATE or DELETE ). This type of replication has some problems: Non-deterministic functions such as NOW() or RAND() will generate different values on replicas. Statements that depend on existing data, like auto-increments, must be executed in the same order in each replica. Statements with side effects may result on different results on each replica. A solution to this is to replace any nondeterministic function with a fixed return value in the leader. However, because there are so many edge cases, other replication methods are now generally preferred. Write-ahead log (WAL) shipping The log is an append-only sequence of bytes containing all writes to the database. The leader can send it to its followers. This way of replication is used in PostgresSQL and Oracle. The main disadvantage is that the log describes the data at a very low level (like which bytes were changed in which disk blocks), coupling it to the storage engine. Usually is not possible to run different versions of the database in leaders and followers. This can have a big operational impact, like making it impossible to have a zero-downtime upgrade of the database. Logical (row-based) log replication Basically a sequence of records describing writes to database tables at the granularity of a row: For an inserted row, the new values of all columns. For a deleted row, the information that uniquely identifies that column. For an updated row, the information to uniquely identify that row and all the new values of the columns. A transaction that modifies several rows, generates several of such logs, followed by a record indicating that the transaction was committed. MySQL binlog uses this approach. Since logical log is decoupled from the storage engine internals, it's easier to make it backwards compatible. Logical logs are also easier for external applications to parse, useful for data warehouses, custom indexes and caches ( change data capture ). Trigger-based replication There are some situations were you may need to move replication up to the application layer. A trigger lets you register custom application code that is automatically executed when a data change occurs. This is a good opportunity to log this change into a separate table, from which it can be read by an external process. Main disadvantages is that this approach has greater overheads, is more prone to bugs but it may be useful due to its flexibility. Problems with Replication Lag Being able to tolerate node failures is just one reason for wanting replication. As mentioned in the introduction to Part II, other reasons are scalability (processing more requests than a single machine can handle) and latency (placing replicas geographically closer to users) In a read-scaling architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this only realistically works on asynchronous replication. The more nodes you have, the likelier is that one will be down, so a fully synchronous configuration would be unreliable. With an asynchronous approach, a follower may fall behind, leading to inconsistencies in the database ( eventual consistency ). The replication lag could be a fraction of a second or several seconds or even minutes. The problems that may arise and how to solve them. Reading your own writes Read-after-write consistency also known as read-your-writes consistency is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. How to implement it: When reading something that the user may have modified, read it from the leader. For example, user profile information on a social network is normally only editable by the owner. A simple rule is always read the user's own profile from the leader. If most things in the application are potentially editable by the user, that approach won\u2019t be effective, as most things would have to be read from the leader (negating the benefit of read scaling). In that case, other criteria may be used to decide whether to read from the leader. You could track the time of the latest update and, for one minute after the last update, make all reads from the leader. The client can remember the timestamp of the most recent write, then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp. If your replicas are distributed across multiple datacenters, then any request needs to be routed to the datacenter that contains the leader. Another complication is that the same user is accessing your service from multiple devices, you may want to provide cross-device read-after-write consistency. Some additional issues to consider: Remembering the timestamp of the user's last update becomes more difficult because the code running on one device doesn\u2019t know what updates have happened on the other device. . The metadata will need to be centralised. If replicas are distributed across datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter. You may need to route requests from all of a user's devices to the same datacenter. See Facebooks memcached implementation for how they implement this with a cache layer aswell. Monotonic reads Because of followers falling behind, it's possible for a user to see things moving backward in time . When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward. It\u2019s a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency. Make sure that each user always makes their reads from the same replica. The replica can be chosen based on a hash of the user ID. If the replica fails, the user's queries will need to be rerouted to another replica. Consistent prefix reads 1 2 3 4 Mr. Poons How far into the future can you see, Mrs. Cake? Mrs. Cake About ten seconds usually, Mr. Poons. Someone shouldn't see Mrs. Cake's message first. Preventing this kind of anomaly requires another type of guarantee: consistent prefix reads. This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. If the database always applies writes in the same order, reads always see a consistent prefix, so this anomaly cannot happen. However, in many distributed databases, different partitions operate independently, so there is no global ordering of writes: when a user reads from the database, they may see some parts of the database in an older state and some in a newer state. One solution is to make sure that any writes that are causally related to each other are written to the same partition\u2014but in some applications that cannot be done efficiently. There are also algorithms that explicitly keep track of causal dependencies, a topic that we will return to in \u201cThe \u201chappens-before\u201d relationship and concurrency\u201d. Multi-leader replication Leader-based replication has one major downside: there is only one leader, and all writes must go through it. A natural extension is to allow more than one node to accept writes ( multi-leader , master-master or active/active replication) where each leader simultaneously acts as a follower to the other leaders. Use cases for multi-leader replication It rarely makes sense to use multi-leader setup within a single datacenter. Multi-datacenter operation You can have a leader in each datacenter. Within each datacenter, regular leader-follower replication is used. Between datacenters, each datacenter leader replicates its changes to the leaders in other datacenters. Compared to a single-leader replication model deployed in multi-datacenters Performance With single-leader, every write must go across the internet to wherever the leader is, adding significant latency. In multi-leader every write is processed in the local datacenter and replicated asynchronously to other datacenters. The network delay is hidden from users and perceived performance may be better. Tolerance of datacenter outages In single-leader if the datacenter with the leader fails, failover can promote a follower in another datacenter. In multi-leader, each datacenter can continue operating independently from others. Tolerance of network problems Traffic between datacenters usually goes over the public internet, which may be less reliable than the local network within a datacenter. Single-leader is very sensitive to problems in this inter-datacenter link as writes are made synchronously over this link. Multi-leader with asynchronous replication can tolerate network problems better. Multi-leader replication is implemented with Tungsten Replicator for MySQL, BDR for PostgreSQL or GoldenGate for Oracle. Although multi-leader replication has advantages, it also has a big downside: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved (indicated as \u201cconflict resolution\u201d in Figure 5-6). Traffic between datacenters usually goes over the public internet, which may be less reliable than the local network within a datacenter It's common to fall on subtle configuration pitfalls. Autoincrementing keys, triggers and integrity constraints can be problematic. Multi-leader replication is often considered dangerous territory and avoided if possible. Clients with offline operation If you have an application that needs to continue to work while it is disconnected from the internet, every device that has a local database can act as a leader, and there will be some asynchronous multi-leader replication process (imagine, a Calendar application). CouchDB is designed for this mode of operation. Collaborative editing Real-time collaborative editing applications allow several people to edit a document simultaneously. Like Etherpad or Google Docs. The user edits a document, the changes are instantly applied to their local replica and asynchronously replicated to the server and any other user. If you want to avoid editing conflicts, you must the lock the document before a user can edit it. For faster collaboration, you may want to make the unit of change very small (like a keystroke) and avoid locking. This approach allows multiple users to edit simultaneously, but it also brings all the challenges of multi-leader replication, including requiring conflict resolution Handling write conflicts The biggest problem with multi-leader replication is when conflict resolution is required. This problem does not happen in a single-leader database. Synchronous vs asynchronous conflict detection In single-leader the second writer can be blocked and wait the first one to complete, forcing the user to retry the write. On multi-leader if both writes are successful, the conflict is only detected asynchronously later in time. In principle, you could make the conflict detection synchronous\u2014i.e., wait for the write to be replicated to all replicas before telling the user that the write was successful. However, by doing so, you would lose the main advantage of multi-leader replication: allowing each replica to accept writes independently. If you want synchronous conflict detection, you might as well just use single-leader replication. Conflict avoidance The simplest strategy for dealing with conflicts is to avoid them. If all writes for a particular record go through the sane leader, then conflicts cannot occur. On an application where a user can edit their own data, you can ensure that requests from a particular user are always routed to the same datacenter and use the leader in that datacenter for reading and writing. Converging toward a consistent state On single-leader, the last write determines the final value of the field. In multi-leader, it's not clear what the final value should be. The database must resolve the conflict in a convergent way, all replicas must arrive a the same final value when all changes have been replicated. Different ways of achieving convergent conflict resolution. Give each write a unique ID (timestamp, long random number, UUID, or a has of the key and value), pick the write with the highest ID as the winner and throw away the other writes. This is known as last write wins (LWW) and it is dangerously prone to data loss. Give each replica a unique ID, writes that originated at a higher-numbered replica always take precedence. This approach also implies data loss. Somehow merge the values together. Record the conflict and write application code that resolves it a to some later time (perhaps prompting the user). Custom conflict resolution Multi-leader replication tools let you write conflict resolution logic using application code. On write As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler. On read All the conflicting writes are stored. On read, multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict. CouchDB works this way. Multi-leader replication topologies A replication topology describes the communication paths along which writes are propagated from one node to another. The most general topology is all-to-all in which every leader sends its writes to every other leader. MySQL uses circular topology , where each nodes receives writes from one node and forwards those writes to another node. Another popular topology has the shape of a star , one designated node forwards writes to all of the other nodes. In circular and star topologies a write might need to pass through multiple nodes before they reach all replicas. To prevent infinite replication loops each node is given a unique identifier and the replication log tags each write with the identifiers of the nodes it has passed through. When a node fails it can interrupt the flow of replication messages. In all-to-all topology fault tolerance is better as messages can travel along different paths avoiding a single point of failure. It has some issues too, some network links may be faster than others and some replication messages may \"overtake\" others. To order events correctly. there is a technique called version vectors . PostgresSQL BDR does not provide casual ordering of writes, and Tungsten Replicator for MySQL doesn't even try to detect conflicts. Leaderless replication Simply put, any replica can directly accept writes from clients. Databases like look like Amazon's in-house Dynamo datastore. Riak , Cassandra and Voldemort follow the Dynamo style . Writing to the Database When a Node Is Down In a leaderless configuration, failover does not exist. Clients send the write to w replicas in parallel. Read requests are also sent to several nodes in parallel . The client may get different responses. Version numbers are used to determine which value is newer. Eventually, all the data is copied to every replica. After a unavailable node comes back online, it has two different mechanisms to catch up: Read repair and anti-entropy Read repair. When a client makes a read from several nodes in parallel, it can detect any stale responses. This approach works well for values that are frequently read. Anti-entropy process. There is a background process that constantly looks for differences in data between replicas and copies any missing data from one replica to he other. It does not copy writes in any particular order. Quorums for reading and writing If there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. As long as w + r > n , we expect to get an up-to-date value when reading. r and w values are called quorum reads and writes. r and w are the minimum number of votes required for the read or write to be valid. A common choice is to make n and odd number (typically 3 or 5) and to set w = r = ( n + 1)/2 (rounded up). For example, a workload with few writes and many reads may benefit from setting w = n and r = 1. This makes reads faster, but has the disadvantage that just one failed node causes all database writes to fail. Limitations: Sloppy quorum, the w writes may end up on different nodes than the r reads, so there is no longer a guaranteed overlap. If two writes occur concurrently, and is not clear which one happened first, the only safe solution is to merge them. Writes can be lost due to clock skew. If a write happens concurrently with a read, the write may be reflected on only some of the replicas. If a write succeeded on some replicas but failed on others, it is not rolled back on the replicas where it succeeded. Reads may or may not return the value from that write. If a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas storing the new value may break the quorum condition. Dynamo-style databases are generally optimised for use cases that can tolerate eventual consistency. Sloppy quorums and hinted handoff Leaderless replication may be appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads. It's likely that the client won't be able to connect to some database nodes during a network interruption. Is it better to return errors to all requests for which we cannot reach quorum of w or r nodes? Or should we accept writes anyway, and write them to some nodes that are reachable but aren't among the n nodes on which the value usually lives? The latter is known as sloppy quorum : writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n \"home\" nodes for a value. Once the network interruption is fixed, any writes are sent to the appropriate \"home\" nodes ( hinted handoff ). Sloppy quorums are useful for increasing write availability: as long as any w nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of n . Sloppy quorums are optional in all common Dynamo implementations. In Riak they are enabled by default, and in Cassandra and Voldemort they are disabled by default. Multi-datacenter operations Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgement from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on cross-datacenter link. The higher-latency writes to other datacenters are often configured to happen asynchronously, although there is some flexibility in the configuration Detecting concurrent writes In order to become eventually consistent, the replicas should converge toward the same value. If you want to avoid losing data, you application developer, need to know a lot about the internals of your database's conflict handling. Last write wins (discarding concurrent writes) Even though the writes don't have a natural ordering, we can force an arbitrary order on them. We can attach a timestamp to each write and pick the most recent. There are some situations such caching on which lost writes are acceptable. If losing data is not acceptable, LWW is a poor choice for conflict resolution. The \"happens-before\" relationship and concurrency An operation A happens before another operation B if B knows about A, or depends on A, or builds upon A in some way. Whether one operation happens before another operation is the key to defining what concurrency means. We can simply say that to operations are concurrent if neither happens before the other. Either A happened before B, or B happened before A, or A and B are concurrent. Capturing the happens-before relationship The server can determine whether two operations are concurrent by looking at the version numbers. The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along the value written. Client reads a key, the server returns all values that have not been overwritten, as well as the latest version number. A client must read a key before writing. Client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. Server receives a write with a particular version number, it can overwrite all values with that version number or below, but it must keep all values with a higher version number. Merging concurrently written values No data is silently dropped. It requires clients do some extra work, they have to clean up afterward by merging the concurrently written values. Riak calls these concurrent values siblings . Merging sibling values is the same problem as conflict resolution in multi-leader replication. A simple approach is to just pick one of the values on a version number or timestamp (last write wins). You may need to do something more intelligent in application code to avoid losing data. If you want to allow people to remove things, union of siblings may not yield the right result. An item cannot simply be deleted from the database when it is removed, the system must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings ( tombstone ). Merging siblings in application code is complex and error-prone, there are efforts to design data structures that can perform this merging automatically (CRDTs). Version vectors We need a version number per replica as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas. The collection of version numbers from all the replicas is called a version vector . Version vector are sent from the database replicas to clients when values are read, and need to be sent back to the database when a value is subsequently written. Riak calls this casual context . Version vectors allow the database to distinguish between overwrites and concurrent writes. Summary In this chapter we looked at the issue of replication. Replication can serve several purposes: High availability Keeping the system running, even when one machine (or several machines, or an entire datacenter) goes down Disconnected operation Allowing an application to continue working when there is a network interruption Latency Placing data geographically close to users, so that users can interact with it faster Scalability Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas We discussed three main approaches to replication: Single-leader, Multi-leader, and leaderless. Each approach has advantages and disadvantages. Single-leader replication is popular because it is fairly easy to understand and there is no conflict resolution to worry about. Multi-leader and leaderless replication can be more robust in the presence of faulty nodes, network interruptions, and latency spikes\u2014at the cost of being harder to reason about and providing only very weak consistency guarantees.","title":"5. Replication"},{"location":"Notes/books/DDIA/chapter5/#5-replication","text":"Reasons why you might want to replicate data: To keep data geographically close to your users Increase availability Increase read throughput The difficulty in replication lies in handling changes to replicated data. Popular algorithms for replicating changes between nodes: single-leader , multi-leader , and leaderless replication.","title":"5. Replication"},{"location":"Notes/books/DDIA/chapter5/#leaders-and-followers","text":"Each node that stores a copy of the database is called a replica . Every write to the database needs to be processed by every replica. The most common solution for this is called leader-based replication ( active/passive or master-slave replication ). One of the replicas is designated the leader ( master or primary ). Writes to the database must send requests to the leader. Other replicas are known as followers ( read replicas , slaves , secondaries or hot stanbys ). The leader sends the data change to all of its followers as part of a replication log or change stream . Reads can be query the leader or any of the followers, while writes are only accepted on the leader. MySQL, Oracle Data Guard, SQL Server's AlwaysOn Availability Groups, MongoDB, RethinkDB, Espresso, Kafka and RabbitMQ are examples of these kind of databases.","title":"Leaders and followers"},{"location":"Notes/books/DDIA/chapter5/#synchronous-vs-asynchronous","text":"The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. The disadvantage is that it the synchronous follower doesn't respond, the write cannot be processed. It's impractical for all followers to be synchronous. If you enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous. This guarantees up-to-date copy of the data on at least two nodes (this is sometimes called semi-synchronous ). Often, leader-based replication is asynchronous. Writes are not guaranteed to be durable, the main advantage of this approach is that the leader can continue processing writes.","title":"Synchronous vs asynchronous"},{"location":"Notes/books/DDIA/chapter5/#setting-up-new-followers","text":"Copying data files from one node to another is typically not sufficient. Setting up a follower can usually be done without downtime. The process looks like: Take a snapshot of the leader's database Copy the snapshot to the follower node Follower requests data changes that have happened since the snapshot was taken Once follower processed the backlog of data changes since snapshot, it has caught up .","title":"Setting up new followers"},{"location":"Notes/books/DDIA/chapter5/#handling-node-outages","text":"How does high availability work with leader-based replication?","title":"Handling node outages"},{"location":"Notes/books/DDIA/chapter5/#implementation-of-replication-logs","text":"","title":"Implementation of replication logs"},{"location":"Notes/books/DDIA/chapter5/#problems-with-replication-lag","text":"Being able to tolerate node failures is just one reason for wanting replication. As mentioned in the introduction to Part II, other reasons are scalability (processing more requests than a single machine can handle) and latency (placing replicas geographically closer to users) In a read-scaling architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this only realistically works on asynchronous replication. The more nodes you have, the likelier is that one will be down, so a fully synchronous configuration would be unreliable. With an asynchronous approach, a follower may fall behind, leading to inconsistencies in the database ( eventual consistency ). The replication lag could be a fraction of a second or several seconds or even minutes. The problems that may arise and how to solve them.","title":"Problems with Replication Lag"},{"location":"Notes/books/DDIA/chapter5/#reading-your-own-writes","text":"Read-after-write consistency also known as read-your-writes consistency is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. How to implement it: When reading something that the user may have modified, read it from the leader. For example, user profile information on a social network is normally only editable by the owner. A simple rule is always read the user's own profile from the leader. If most things in the application are potentially editable by the user, that approach won\u2019t be effective, as most things would have to be read from the leader (negating the benefit of read scaling). In that case, other criteria may be used to decide whether to read from the leader. You could track the time of the latest update and, for one minute after the last update, make all reads from the leader. The client can remember the timestamp of the most recent write, then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp. If your replicas are distributed across multiple datacenters, then any request needs to be routed to the datacenter that contains the leader. Another complication is that the same user is accessing your service from multiple devices, you may want to provide cross-device read-after-write consistency. Some additional issues to consider: Remembering the timestamp of the user's last update becomes more difficult because the code running on one device doesn\u2019t know what updates have happened on the other device. . The metadata will need to be centralised. If replicas are distributed across datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter. You may need to route requests from all of a user's devices to the same datacenter. See Facebooks memcached implementation for how they implement this with a cache layer aswell.","title":"Reading your own writes"},{"location":"Notes/books/DDIA/chapter5/#monotonic-reads","text":"Because of followers falling behind, it's possible for a user to see things moving backward in time . When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward. It\u2019s a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency. Make sure that each user always makes their reads from the same replica. The replica can be chosen based on a hash of the user ID. If the replica fails, the user's queries will need to be rerouted to another replica.","title":"Monotonic reads"},{"location":"Notes/books/DDIA/chapter5/#consistent-prefix-reads","text":"1 2 3 4 Mr. Poons How far into the future can you see, Mrs. Cake? Mrs. Cake About ten seconds usually, Mr. Poons. Someone shouldn't see Mrs. Cake's message first. Preventing this kind of anomaly requires another type of guarantee: consistent prefix reads. This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. If the database always applies writes in the same order, reads always see a consistent prefix, so this anomaly cannot happen. However, in many distributed databases, different partitions operate independently, so there is no global ordering of writes: when a user reads from the database, they may see some parts of the database in an older state and some in a newer state. One solution is to make sure that any writes that are causally related to each other are written to the same partition\u2014but in some applications that cannot be done efficiently. There are also algorithms that explicitly keep track of causal dependencies, a topic that we will return to in \u201cThe \u201chappens-before\u201d relationship and concurrency\u201d.","title":"Consistent prefix reads"},{"location":"Notes/books/DDIA/chapter5/#multi-leader-replication","text":"Leader-based replication has one major downside: there is only one leader, and all writes must go through it. A natural extension is to allow more than one node to accept writes ( multi-leader , master-master or active/active replication) where each leader simultaneously acts as a follower to the other leaders.","title":"Multi-leader replication"},{"location":"Notes/books/DDIA/chapter5/#use-cases-for-multi-leader-replication","text":"It rarely makes sense to use multi-leader setup within a single datacenter.","title":"Use cases for multi-leader replication"},{"location":"Notes/books/DDIA/chapter5/#handling-write-conflicts","text":"The biggest problem with multi-leader replication is when conflict resolution is required. This problem does not happen in a single-leader database.","title":"Handling write conflicts"},{"location":"Notes/books/DDIA/chapter5/#multi-leader-replication-topologies","text":"A replication topology describes the communication paths along which writes are propagated from one node to another. The most general topology is all-to-all in which every leader sends its writes to every other leader. MySQL uses circular topology , where each nodes receives writes from one node and forwards those writes to another node. Another popular topology has the shape of a star , one designated node forwards writes to all of the other nodes. In circular and star topologies a write might need to pass through multiple nodes before they reach all replicas. To prevent infinite replication loops each node is given a unique identifier and the replication log tags each write with the identifiers of the nodes it has passed through. When a node fails it can interrupt the flow of replication messages. In all-to-all topology fault tolerance is better as messages can travel along different paths avoiding a single point of failure. It has some issues too, some network links may be faster than others and some replication messages may \"overtake\" others. To order events correctly. there is a technique called version vectors . PostgresSQL BDR does not provide casual ordering of writes, and Tungsten Replicator for MySQL doesn't even try to detect conflicts.","title":"Multi-leader replication topologies"},{"location":"Notes/books/DDIA/chapter5/#leaderless-replication","text":"Simply put, any replica can directly accept writes from clients. Databases like look like Amazon's in-house Dynamo datastore. Riak , Cassandra and Voldemort follow the Dynamo style .","title":"Leaderless replication"},{"location":"Notes/books/DDIA/chapter5/#writing-to-the-database-when-a-node-is-down","text":"In a leaderless configuration, failover does not exist. Clients send the write to w replicas in parallel. Read requests are also sent to several nodes in parallel . The client may get different responses. Version numbers are used to determine which value is newer. Eventually, all the data is copied to every replica. After a unavailable node comes back online, it has two different mechanisms to catch up:","title":"Writing to the Database When a Node Is Down"},{"location":"Notes/books/DDIA/chapter5/#sloppy-quorums-and-hinted-handoff","text":"Leaderless replication may be appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads. It's likely that the client won't be able to connect to some database nodes during a network interruption. Is it better to return errors to all requests for which we cannot reach quorum of w or r nodes? Or should we accept writes anyway, and write them to some nodes that are reachable but aren't among the n nodes on which the value usually lives? The latter is known as sloppy quorum : writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n \"home\" nodes for a value. Once the network interruption is fixed, any writes are sent to the appropriate \"home\" nodes ( hinted handoff ). Sloppy quorums are useful for increasing write availability: as long as any w nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of n . Sloppy quorums are optional in all common Dynamo implementations. In Riak they are enabled by default, and in Cassandra and Voldemort they are disabled by default.","title":"Sloppy quorums and hinted handoff"},{"location":"Notes/books/DDIA/chapter5/#detecting-concurrent-writes","text":"In order to become eventually consistent, the replicas should converge toward the same value. If you want to avoid losing data, you application developer, need to know a lot about the internals of your database's conflict handling.","title":"Detecting concurrent writes"},{"location":"Notes/books/DDIA/chapter5/#version-vectors","text":"We need a version number per replica as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas. The collection of version numbers from all the replicas is called a version vector . Version vector are sent from the database replicas to clients when values are read, and need to be sent back to the database when a value is subsequently written. Riak calls this casual context . Version vectors allow the database to distinguish between overwrites and concurrent writes.","title":"Version vectors"},{"location":"Notes/books/DDIA/chapter5/#summary","text":"In this chapter we looked at the issue of replication. Replication can serve several purposes: High availability Keeping the system running, even when one machine (or several machines, or an entire datacenter) goes down Disconnected operation Allowing an application to continue working when there is a network interruption Latency Placing data geographically close to users, so that users can interact with it faster Scalability Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas We discussed three main approaches to replication: Single-leader, Multi-leader, and leaderless. Each approach has advantages and disadvantages. Single-leader replication is popular because it is fairly easy to understand and there is no conflict resolution to worry about. Multi-leader and leaderless replication can be more robust in the presence of faulty nodes, network interruptions, and latency spikes\u2014at the cost of being harder to reason about and providing only very weak consistency guarantees.","title":"Summary"},{"location":"Notes/books/DDIA/chapter6/","text":"6. Partitioning Replication, for very large datasets or very high query throughput is not sufficient, we need to break the data up into partitions ( sharding ). Basically, each partition is a small database of its own. The main reason for wanting to partition data is scalability , query load can be load can be distributed across many processors. Throughput can be scaled by adding more node. Certain queries can be executed on only one partition. Large, complex queries can potentially be parallelized across many nodes. Partitioning and replication Each record belongs to exactly one partition, it may still be stored on several nodes for fault tolerance. A node may store more than one partition. If a leader-follower replication model is used, the combination of partitioning and replication can look like Figure 6-1. Partition of key-value data Our goal with partitioning is to spread the data and the query load evenly across nodes. If partition is unfair, so that some partitions have more data or queries than others, we call it skewed . It makes partitioning much less effective. A partition with disproportionately high load is called a hot spot . The simplest approach is to assign records to nodes randomly. The main disadvantage is that if you are trying to read a particular item, you have no way of knowing which node it is on, so you have to query all nodes in parallel. Partition by key range Assign a continuous range of keys to each partition, like the volumes of a paper encyclopedia. Boundaries might be chose manually by an administrator, or the database can choose them automatically. On each partition, keys are in sorted order so scans are easy. The downside is that certain access patterns can lead to hot spots. For example, if the key is a timestamp, then the partitions correspond to ranges of time - e.g. one partition per day. Since we write data from the sensors to the database as the measurements happen, all the writes will go to the same partition while no writes will go to the others. Partitioning by hash of key A good hash function takes skewed data and makes it uniformly distributed. There is no need to be cryptographically strong (Cassandra uses MD5). You can assign each partition a range of hashes. The boundaries can be evenly spaced or they can be chosen pseudorandomly ( consistent hashing ). Unfortunately we lose the ability to do efficient range queries. Keys that were once adjacent are now scattered across all the partitions. Any range query has to be sent to all partitions. Cassandra achieves a compromise between the two partitioning strategies. Cassandra allows declaring a compound primary key consisting of several columns. Only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data in Cassandra's SSTables. A query therefore cannot search for a range of values within the first column of the key, but if it specifies a fixed value for the first column, it can perform an efficient range scan over the other columns of the key. This is good for one-to-many relationships. For example, one user may post many updates so we can have our key be (user_id, update_timestamp) . All records for each user will be on the same partition. Skewed workloads and relieving hot spots You can't avoid hot spots entirely. For example, you may end up with large volume of writes to the same key when a celebrity tweets and many people comment. It's the responsibility of the application to reduce the skew. A simple technique is to add a random number to the beginning or end of the key. However, splitting writes across different keys makes reads now to do some extra work as it has to read all the possible split keys. Partitioning and secondary indexes The situation gets more complicated if secondary indexes are involved. A secondary index usually doesn't identify the record uniquely. They don't map neatly to partitions. Local Secondary Indexes (Partitioning secondary indexes by Document) Each partition maintains its secondary indexes, covering only the documents in that partition ( local index ). It's called document partitioned, because the information related to a document is stored on its own partition. Writes will only go to the partition that contains the primary key. For reads, you need to send the query to all partitions, and combine all the results you get back ( scatter/gather ). This is prone to tail latency amplification and is widely used in MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud and VoltDB. Global Secondary Indexe (Partitioning secondary indexes by Term) We construct a global index that covers data in all partitions. However, we can't just store that index on one node. The global index must also be partitioned so it doesn't become the bottleneck. It is called the term-partitioned because the term we're looking for determines the partition of the index. Partitioning by term can be useful for range scans, whereas partitioning on a hash of the term gives a more even distribution load. The advantage is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. The downside of a global index is that writes are slower and complicated because a write may now affect multiple partitions (every indexed term in the document might be on a different partition). Rebalancing partitions Over time, the requirements of the database may change. Query throughput increases, so you want to add mroe CPUs to handle the load. The dataset size increases, so you want to add more disks and RAM to store it. A machine fails, and other machines need ot take over the failed machine. Strategies for rebalancing How not to do it: Hash mod n The problem with mod N is that if the number of nodes N changes, most of the keys will need to be moved from one node to another. Personally, I think this is fine as long as you use Consistent Hashing in a ring with virtual nodes. Placing the nodes in a ring make it so that adding/removing a node only affects the load for 1 node. The problem with just a ring is that load can become skewed especially when a node is removed. The solution is to create virtual servers/nodes. Generate K replica ids for each server id (designing K hash functions while maintaining random uniformity and consistency is hard)Generating K replica ids is easy: xxx gives K replicas xxx + '1', xxx + '2', ..., xxx + 'K'. Then you take these replicas and generate K points on the ring with the same hash function. So when a node goes down, it's not one point on the ring that goes away. Its K points that go away affecting K other nodes, but this load is now split up on the remaining nodes evenly. Fixed number of partitions Create many more partitions than there are nodes and assign several partitions to each node. If a node is added to the cluster, we can steal a few partitions from every existing node until partitions are fairly distributed once again. The number of partitions does not change, nor does the assignment of keys to partitions. The only thing that change is the assignment of partitions to nodes. This is used in Riak, Elasticsearch, Couchbase, and Voldemport. You need to choose a high enough number of partitions to accomodate future growth. Neither too big or too small. Dynamic partitioning The number of partitions adapts to the total data volume. An empty database starts with an empty partition. While the dataset is small, all writes have to processed by a single node while the others nodes sit idle. HBase and MongoDB allow an initial set of partitions to be configured ( pre-splitting ). Partitioning proportionally to nodes Cassandra and Ketama make the number of partitions proportional to the number of nodes. Have a fixed number of partitions per node . This approach also keeps the size of each partition fairly stable. When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of 1/2 of each of those split partition while leaving the other half of each partition in place. Automatic versus manual rebalancing Fully automated rebalancing may seem convenient but the process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress. It can be good to have a human in the loop for rebalancing. You may avoid operational surprises. Request routing This problem is also called service discovery . There are different approaches: Allow clients to contact any node and make them handle the request directly, or forward the request to the appropriate node. Send all requests from clients to a routing tier first that acts as a partition-aware load balancer. Make clients aware of the partitioning and the assignment of partitions to nodes. In many cases the problem is: how does the component making the routing decision learn about changes in the assignment of partitions to nodes? Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. The routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. HBase, SolrCloud and Kafka use ZooKeeper to track partition assignment. MongoDB relies on its own config server . Cassandra and Riak take a different approach: they use a gossip protocol . Parallel query execution Massively parallel processing (MPP) relational database products are much more sophisticated in the types of queries they support.","title":"6. Partitioning"},{"location":"Notes/books/DDIA/chapter6/#6-partitioning","text":"Replication, for very large datasets or very high query throughput is not sufficient, we need to break the data up into partitions ( sharding ). Basically, each partition is a small database of its own. The main reason for wanting to partition data is scalability , query load can be load can be distributed across many processors. Throughput can be scaled by adding more node. Certain queries can be executed on only one partition. Large, complex queries can potentially be parallelized across many nodes.","title":"6. Partitioning"},{"location":"Notes/books/DDIA/chapter6/#partitioning-and-replication","text":"Each record belongs to exactly one partition, it may still be stored on several nodes for fault tolerance. A node may store more than one partition. If a leader-follower replication model is used, the combination of partitioning and replication can look like Figure 6-1.","title":"Partitioning and replication"},{"location":"Notes/books/DDIA/chapter6/#partition-of-key-value-data","text":"Our goal with partitioning is to spread the data and the query load evenly across nodes. If partition is unfair, so that some partitions have more data or queries than others, we call it skewed . It makes partitioning much less effective. A partition with disproportionately high load is called a hot spot . The simplest approach is to assign records to nodes randomly. The main disadvantage is that if you are trying to read a particular item, you have no way of knowing which node it is on, so you have to query all nodes in parallel.","title":"Partition of key-value data"},{"location":"Notes/books/DDIA/chapter6/#partition-by-key-range","text":"Assign a continuous range of keys to each partition, like the volumes of a paper encyclopedia. Boundaries might be chose manually by an administrator, or the database can choose them automatically. On each partition, keys are in sorted order so scans are easy. The downside is that certain access patterns can lead to hot spots. For example, if the key is a timestamp, then the partitions correspond to ranges of time - e.g. one partition per day. Since we write data from the sensors to the database as the measurements happen, all the writes will go to the same partition while no writes will go to the others.","title":"Partition by key range"},{"location":"Notes/books/DDIA/chapter6/#partitioning-by-hash-of-key","text":"A good hash function takes skewed data and makes it uniformly distributed. There is no need to be cryptographically strong (Cassandra uses MD5). You can assign each partition a range of hashes. The boundaries can be evenly spaced or they can be chosen pseudorandomly ( consistent hashing ). Unfortunately we lose the ability to do efficient range queries. Keys that were once adjacent are now scattered across all the partitions. Any range query has to be sent to all partitions. Cassandra achieves a compromise between the two partitioning strategies. Cassandra allows declaring a compound primary key consisting of several columns. Only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data in Cassandra's SSTables. A query therefore cannot search for a range of values within the first column of the key, but if it specifies a fixed value for the first column, it can perform an efficient range scan over the other columns of the key. This is good for one-to-many relationships. For example, one user may post many updates so we can have our key be (user_id, update_timestamp) . All records for each user will be on the same partition.","title":"Partitioning by hash of key"},{"location":"Notes/books/DDIA/chapter6/#skewed-workloads-and-relieving-hot-spots","text":"You can't avoid hot spots entirely. For example, you may end up with large volume of writes to the same key when a celebrity tweets and many people comment. It's the responsibility of the application to reduce the skew. A simple technique is to add a random number to the beginning or end of the key. However, splitting writes across different keys makes reads now to do some extra work as it has to read all the possible split keys.","title":"Skewed workloads and relieving hot spots"},{"location":"Notes/books/DDIA/chapter6/#partitioning-and-secondary-indexes","text":"The situation gets more complicated if secondary indexes are involved. A secondary index usually doesn't identify the record uniquely. They don't map neatly to partitions.","title":"Partitioning and secondary indexes"},{"location":"Notes/books/DDIA/chapter6/#local-secondary-indexes-partitioning-secondary-indexes-by-document","text":"Each partition maintains its secondary indexes, covering only the documents in that partition ( local index ). It's called document partitioned, because the information related to a document is stored on its own partition. Writes will only go to the partition that contains the primary key. For reads, you need to send the query to all partitions, and combine all the results you get back ( scatter/gather ). This is prone to tail latency amplification and is widely used in MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud and VoltDB.","title":"Local Secondary Indexes (Partitioning secondary indexes by Document)"},{"location":"Notes/books/DDIA/chapter6/#global-secondary-indexe-partitioning-secondary-indexes-by-term","text":"We construct a global index that covers data in all partitions. However, we can't just store that index on one node. The global index must also be partitioned so it doesn't become the bottleneck. It is called the term-partitioned because the term we're looking for determines the partition of the index. Partitioning by term can be useful for range scans, whereas partitioning on a hash of the term gives a more even distribution load. The advantage is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. The downside of a global index is that writes are slower and complicated because a write may now affect multiple partitions (every indexed term in the document might be on a different partition).","title":"Global Secondary Indexe (Partitioning secondary indexes by Term)"},{"location":"Notes/books/DDIA/chapter6/#rebalancing-partitions","text":"Over time, the requirements of the database may change. Query throughput increases, so you want to add mroe CPUs to handle the load. The dataset size increases, so you want to add more disks and RAM to store it. A machine fails, and other machines need ot take over the failed machine.","title":"Rebalancing partitions"},{"location":"Notes/books/DDIA/chapter6/#strategies-for-rebalancing","text":"","title":"Strategies for rebalancing"},{"location":"Notes/books/DDIA/chapter6/#automatic-versus-manual-rebalancing","text":"Fully automated rebalancing may seem convenient but the process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress. It can be good to have a human in the loop for rebalancing. You may avoid operational surprises.","title":"Automatic versus manual rebalancing"},{"location":"Notes/books/DDIA/chapter6/#request-routing","text":"This problem is also called service discovery . There are different approaches: Allow clients to contact any node and make them handle the request directly, or forward the request to the appropriate node. Send all requests from clients to a routing tier first that acts as a partition-aware load balancer. Make clients aware of the partitioning and the assignment of partitions to nodes. In many cases the problem is: how does the component making the routing decision learn about changes in the assignment of partitions to nodes? Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. The routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. HBase, SolrCloud and Kafka use ZooKeeper to track partition assignment. MongoDB relies on its own config server . Cassandra and Riak take a different approach: they use a gossip protocol .","title":"Request routing"},{"location":"Notes/books/DDIA/chapter6/#parallel-query-execution","text":"Massively parallel processing (MPP) relational database products are much more sophisticated in the types of queries they support.","title":"Parallel query execution"},{"location":"Notes/books/DDIA/chapter7/","text":"7. Transactions Transaction a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails (abort, rollback). The slippery concept of a transaction There emerged a popular belief that transactions were the antithesis of scalability, and that any large-scale system would have to abandon transactions in order to maintain good performance and high availability. On the other hand, transactional guarantees are some\u2010 times presented by database vendors as an essential requirement for \u201cserious applica\u2010 tions\u201d with \u201cvaluable data.\u201d Both viewpoints are pure hyperbole. The truth is not that simple: like every other technical design choice, transactions have advantages and limitations. In order to understand those trade-offs, let\u2019s go into the details of the guarantees that transactions can provide\u2014both in normal operation and in various extreme (but realistic) circumstances. The Meaning of ACID Atomicity Is not about concurrency. It is what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed. The databse must discard or undo any writes it has made so far. Abortability would have been a better term than atomicity . Consistency Invariants on your data must always be true. The idea of consistency depends on the application's notion of invariants. Atomicity, isolation, and durability are properties of the database, whereas consistency (in an ACID sense) is a property of the application. Isolation Concurrently executing transactions are isolated from each other. It's also called serializability , each transaction can pretend that it is the only transaction running on the entire database, and the result is the same as if they had run serially (one after the other). In practice, serializable isolation is rarely used, because it carries a performance penalty Durability Once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes. In a single-node database this means the data has been written to nonvolatile storage. In a replicated database it means the data has been successfully copied to some number of nodes. Single-Object and Multi-Object Operations To display the number of unread messages for a user, you could query something like: 1 SELECT COUNT ( * ) FROM emails WHERE recipient_id = 2 AND unread_flag = true However, this query can be slow if there are many emails so you decide to store the number of unread messages in a seperate field (denormalization). Above, user 2 experiences an anomaly: the mailbox listing shows an unread message, but the counter shows 0 unread messages because the counter increment has not yet happened. Isolation would have prevented this issue by ensuring that user 2 sees both the updates or neither Above illustrates the need for atomicity: if an error occurs somewhere over the course of the transaction, the contents of the mailbox and the unread counter might become out of sync. In an atomic transaction, if the update to the counter fails, the transaction is aborted and the inserted email is rolled back. Single-object writes Storage engines almost universally provide atomicity and isolation on the level of a single object. Atomicity can be implemented using a log for crash recovery, and isolation can be implemented using a lock on each object, allowing only one thread to access an object at any one time. The need for multi-object transactions In a relational data model, a row in one table often has a foreign key reference in another table. We need to ensure both are updated and valid. In a document data model, the fields that need to be updated together are often within the same document, which is trated as a single object. However, document databases lacking join functionality encourage denormalization. Transactions prevent denormalized data from going out of sync. In databases with secondary indexes, we must need the indexes to be in sync. Weak isolation levels Concurrency issues (race conditions) come into play when one transaction reads data that is concurrently modified by another transaction, or when two transactions try to simultaneously modify the same data. Databases have long tried to hide concurrency issues by providing transaction isolation . In practice, is not that simple. Serializable isolation has a performance cost. It's common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all. Weak isolation levels used in practice: Read committed It makes two guarantees: When reading from the database, you will only see data that has been committed (no dirty reads ). Writes by a transaction only become visible to others when that transaction commits. When writing to the database, you will only overwrite data that has been committed (no dirty writes ). Dirty writes are prevented usually by delaying the second write until the first write's transaction has committed or aborted. Implementing read committed Most commonly, databases prevent dirty writes by using row-level locks: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object. It must then hold that lock until the transaction is committed or aborted. Databases prevent dirty reads by remembering the old committed value and the new value set by the transaction that currently holds the write lock for every object being written. While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value. Snapshot Isolation and Repeatable Read There are still plenty of ways in which you can have concurrency bugs when using read committed isolation. Below is an example. This anomaly is called Nonrepeatable read or read skew , when you read at the same time you committed a change you may see temporal and inconsistent results. There are some situations that cannot tolerate such temporal inconsistencies: Backups. During the time that the backup process is running, writes will continue to be made to the database. If you need to restore from such a backup, inconsistencies can become permanent. Analytic queries and integrity checks. You may get nonsensical results if they observe parts of the database at different points in time. Snapshot isolation is the most common solution. Each transaction reads from a consistent snapshot of the database. It is good for long-running, read-only queries such as backups and analytics. Implementing snapshot isolation The implementation of snapshots typically use write locks to prevent dirty writes. However, reads do not require any locks. Readers never block writers, and writers never block readers . The database must potentially keep several different committed versions of an object ( multi-version concurrency control or MVCC). Read committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot for an entire transaction. Whenever a transaction writes anything to the database, the data it writes is tagged with the transaction ID of the writer. All writes made by transactions with a later transaction ID are ignored, regardless of whether those transactions have committed. Snapshot isolation is called serializable in Oracle, and repeatable read in PostgreSQL and MySQL. Preventing lost updates The lost update problem can occur if an application reads some value from the database, modifies it, and writes it back. If two transactions do this concurrently, one of the modifications can be lost (later write clobbers the earlier write). Atomic write operations A solution for this it to avoid the need to implement read-modify-write cycles and provide atomic operations such as 1 UPDATE counters SET value = value + 1 WHERE key = 'foo' ; Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. MongoDB provides atomic operations for making local modifications, and Redis provides atomic operations for modifying data structures. Explicit locking If the database's built-in atomic operations doesn't provide the necessary functionality, the application explicitly lock objects that are going to be updated. 1 2 3 SELECT * FROM figures WHERE name = 'robot' AND game_id = 222 FOR UPDATE ; The FOR UPDATE clause in SQL queries indicates that the database should take a lock on all rows returned by this query. Automatically detecting lost updates Allow them to execute in parallel, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle. MySQL/InnoDB's repeatable read does not detect lost updates. Compare-and-set If the current value does not match with what you previously read, the update has no effect. 1 2 UPDATE wiki_pages SET content = 'new content' WHERE id = 1234 AND content = 'old content' ; Conflict resolution and replication With multi-leader or leaderless replication, compare-and-set do not apply. A common approach in replicated databases is to allow concurrent writes to create several conflicting versions of a value (also know as siblings ), and to use application code or special data structures to resolve and merge these versions after the fact. Write skew and phantoms Imagine Alice and Bob are two on-call doctors for a particular shift. Imagine both the request to leave because they are feeling unwell. Unfortunately they happen to click the button to go off call at approximately the same time. A SELECT query checks whether some condition is satisfied. If satisfied, the application makes a write and commits. The effect of this write changes the precondition of the decision of 1. Since database is using snapshot isolation, both checks return 2. Both transactions commit, and now no doctor is on call. The requirement of having at least one doctor has been violated. Write skew is a generalization of the lost update problem. Write skew can occur if two transactions read the same objects, and then update some of those objects. In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing). Ways to prevent write skew are a bit more restricted: Atomic operations don't help as things involve more objects. Automatically prevent write skew requires true serializable isolation. The second-best option in this case is probably to explicitly lock the rows that the transaction depends on. 1 2 3 4 5 6 7 8 9 10 11 12 BEGIN TRANSACTION ; SELECT * FROM doctors WHERE on_call = true AND shift_id = 1234 FOR UPDATE ; UPDATE doctors SET on_call = false WHERE name = 'Alice' AND shift_id = 1234 ; COMMIT ; Serializability This is the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially , without concurrency. Basically, the database prevents all possible race conditions. There are three techniques for achieving this: Executing transactions in serial order Two-phase locking Serializable snapshot isolation. Actual serial execution The simplest way of removing concurrency problems is to remove concurrency entirely and execute only one transaction at a time, in serial order, on a single thread. This approach is implemented by VoltDB/H-Store, Redis and Datomic. Encapsulating transactions in stored procedures With interactive style of transaction, a lot of time is spent in network communication between the application and the database. For this reason, systems with single-threaded serial transaction processing don't allow interactive multi-statement transactions. The application must submit the entire transaction code to the database ahead of time, as a stored procedure , so all the data required by the transaction is in memory and the procedure can execute very fast. There are a few pros and cons for stored procedures: Each database vendor has its own language for stored procedures. They usually look quite ugly and archaic from today's point of view, and they lack the ecosystem of libraries. It's harder to debug, more awkward to keep in version control and deploy, trickier to test, and difficult to integrate with monitoring. With stored procedures and in-memory data, executing all transactions on a single thread becomes feasible. As they don\u2019t need to wait for I/O and they avoid the overhead of other concurrency control mechanisms, they can achieve quite good throughput on a single thread. Partitioning Executing all transactions serially limits the transaction throughput to the speed of a single CPU. In order to scale to multiple CPU cores you can potentially partition your data and each partition can have its own transaction processing thread. You can give each CPU core its own partition. For any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions. They will be vastly slower than single-partition transactions. Two-phase locking (2PL) Two-phase locking (2PL) sounds similar to two-phase commit (2PC) but be aware that they are completely different things. Several transactions are allowed to concurrently read the same object as long as nobody is writing it. When somebody wants to write (modify or delete) an object, exclusive access is required. Writers don't just block other writers; they also block readers. Readers block writers. It protects against all the race conditions discussed earlier. Implementation of Two-phase locking Blocking readers and writers is implemented by a having lock on each object in the database. The lock can either be in shared mode or in exclusive mode . The lock is used as follows: If a transaction wants to read an object, it must first acquire a lock in shared mode. If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). First phase is when the locks are acquired, second phase is when all the locks are released. It can happen that transaction A is stuck waiting for transaction B to release its lock, and vice versa ( deadlock ). The performance for transaction throughput and response time of queries are significantly worse under two-phase locking than under weak isolation. A transaction may have to wait for several others to complete before it can do anything. Databases running 2PL can have unstable latencies, and they can be very slow at high percentiles. One slow transaction, or one transaction that accesses a lot of data and acquires many locks can cause the rest of the system to halt. Predicate locks With phantoms , one transaction may change the results of another transaction's search query. In order to prevent phantoms, we need a predicate lock . Rather than a lock belonging to a particular object, it belongs to all objects that match some search condition. Predicate locks applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms). If two-phase locking includes predicate locks, the database prevents all forms of write skew and other race conditions, and so its isolation becomes serializable. Index-range locks Predicate locks do not perform well. Checking for matching locks becomes time-consuming and for that reason most databases implement index-range locking where a range of values for a particular index is specified. It's safe to simplify a predicate by making it match a greater set of objects. These locks are not as precise as predicate locks would be (they may lock a bigger range than necessary), but since they have much lower overheads, they are a good compromise. Serializable snapshot isolation (SSI) It provides full serializability but only has a small performance penalty compared to snapshot isolation. SSI is fairly new and might become the new default in the future. Pesimistic versus optimistic concurrency control Two-phase locking is called pessimistic concurrency control because if anything might possibly go wrong, it's better to wait. Serial execution is pessimistic to the extreme as is equivalent to each transaction having an exclusive lock on the entire database. Serializable snapshot isolation is optimistic concurrency control technique. Instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. The database is responsible for checking whether anything bad happened. If so, the transaction is aborted and has to be retried. It performs badly if there is high contention (many transactions trying to access the same objects) as this leads to a high number of transaction aborts. If there is enough spare capacity, and if contention between transactions is not too high, optimistic concurrency control techniques tend to perform better than pessimistic ones. SSI is based on snapshot isolation, reads within a transaction are made from a consistent snapshot of the database. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort. Decisions based on an outdated premise As seen in the hospital oncall example, under snapshot isolation, the result from the original query may no longer be up-to-date by the time the transaction commits, because the data may have been modified in the meantime. The transaction is taking an action based on a premise. The database knows which transactions may have acted on an outdated premise and need to be aborted by: Detecting reads of a stale MVCC object version. The database needs to track when a transaction ignores another transaction's writes due to MVCC visibility rules. When a transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted. Detecting writes that affect prior reads. As with two-phase locking, SSI uses index-range locks except that it does not block other transactions. When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. It simply notifies the transactions that the data they read may no longer be up to date. Performance of serializable snapshot isolation Compared to two-phase locking, the big advantage of SSI is that one transaction doesn't need to block waiting for locks held by another transaction. Writers don't block readers, and vice versa. This design principle makes query latency much more predictable and less variable. In particular, read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads Compared to serial execution, SSI is not limited to the throughput of a single CPU core. Transactions can read and write data in multiple partitions while ensuring serializable isolation. The rate of aborts significantly affects the overall performance of SSI. SSI requires that read-write transactions be fairly short (long-running read-only transactions may be okay).","title":"7. Transactions"},{"location":"Notes/books/DDIA/chapter7/#7-transactions","text":"Transaction a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails (abort, rollback).","title":"7. Transactions"},{"location":"Notes/books/DDIA/chapter7/#the-slippery-concept-of-a-transaction","text":"There emerged a popular belief that transactions were the antithesis of scalability, and that any large-scale system would have to abandon transactions in order to maintain good performance and high availability. On the other hand, transactional guarantees are some\u2010 times presented by database vendors as an essential requirement for \u201cserious applica\u2010 tions\u201d with \u201cvaluable data.\u201d Both viewpoints are pure hyperbole. The truth is not that simple: like every other technical design choice, transactions have advantages and limitations. In order to understand those trade-offs, let\u2019s go into the details of the guarantees that transactions can provide\u2014both in normal operation and in various extreme (but realistic) circumstances.","title":"The slippery concept of a transaction"},{"location":"Notes/books/DDIA/chapter7/#the-meaning-of-acid","text":"Atomicity Is not about concurrency. It is what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed. The databse must discard or undo any writes it has made so far. Abortability would have been a better term than atomicity . Consistency Invariants on your data must always be true. The idea of consistency depends on the application's notion of invariants. Atomicity, isolation, and durability are properties of the database, whereas consistency (in an ACID sense) is a property of the application. Isolation Concurrently executing transactions are isolated from each other. It's also called serializability , each transaction can pretend that it is the only transaction running on the entire database, and the result is the same as if they had run serially (one after the other). In practice, serializable isolation is rarely used, because it carries a performance penalty Durability Once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes. In a single-node database this means the data has been written to nonvolatile storage. In a replicated database it means the data has been successfully copied to some number of nodes.","title":"The Meaning of ACID"},{"location":"Notes/books/DDIA/chapter7/#single-object-and-multi-object-operations","text":"To display the number of unread messages for a user, you could query something like: 1 SELECT COUNT ( * ) FROM emails WHERE recipient_id = 2 AND unread_flag = true However, this query can be slow if there are many emails so you decide to store the number of unread messages in a seperate field (denormalization). Above, user 2 experiences an anomaly: the mailbox listing shows an unread message, but the counter shows 0 unread messages because the counter increment has not yet happened. Isolation would have prevented this issue by ensuring that user 2 sees both the updates or neither Above illustrates the need for atomicity: if an error occurs somewhere over the course of the transaction, the contents of the mailbox and the unread counter might become out of sync. In an atomic transaction, if the update to the counter fails, the transaction is aborted and the inserted email is rolled back.","title":"Single-Object and Multi-Object Operations"},{"location":"Notes/books/DDIA/chapter7/#weak-isolation-levels","text":"Concurrency issues (race conditions) come into play when one transaction reads data that is concurrently modified by another transaction, or when two transactions try to simultaneously modify the same data. Databases have long tried to hide concurrency issues by providing transaction isolation . In practice, is not that simple. Serializable isolation has a performance cost. It's common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all. Weak isolation levels used in practice:","title":"Weak isolation levels"},{"location":"Notes/books/DDIA/chapter7/#read-committed","text":"It makes two guarantees: When reading from the database, you will only see data that has been committed (no dirty reads ). Writes by a transaction only become visible to others when that transaction commits. When writing to the database, you will only overwrite data that has been committed (no dirty writes ). Dirty writes are prevented usually by delaying the second write until the first write's transaction has committed or aborted.","title":"Read committed"},{"location":"Notes/books/DDIA/chapter7/#snapshot-isolation-and-repeatable-read","text":"There are still plenty of ways in which you can have concurrency bugs when using read committed isolation. Below is an example. This anomaly is called Nonrepeatable read or read skew , when you read at the same time you committed a change you may see temporal and inconsistent results. There are some situations that cannot tolerate such temporal inconsistencies: Backups. During the time that the backup process is running, writes will continue to be made to the database. If you need to restore from such a backup, inconsistencies can become permanent. Analytic queries and integrity checks. You may get nonsensical results if they observe parts of the database at different points in time. Snapshot isolation is the most common solution. Each transaction reads from a consistent snapshot of the database. It is good for long-running, read-only queries such as backups and analytics.","title":"Snapshot Isolation and Repeatable Read"},{"location":"Notes/books/DDIA/chapter7/#preventing-lost-updates","text":"The lost update problem can occur if an application reads some value from the database, modifies it, and writes it back. If two transactions do this concurrently, one of the modifications can be lost (later write clobbers the earlier write).","title":"Preventing lost updates"},{"location":"Notes/books/DDIA/chapter7/#write-skew-and-phantoms","text":"Imagine Alice and Bob are two on-call doctors for a particular shift. Imagine both the request to leave because they are feeling unwell. Unfortunately they happen to click the button to go off call at approximately the same time. A SELECT query checks whether some condition is satisfied. If satisfied, the application makes a write and commits. The effect of this write changes the precondition of the decision of 1. Since database is using snapshot isolation, both checks return 2. Both transactions commit, and now no doctor is on call. The requirement of having at least one doctor has been violated. Write skew is a generalization of the lost update problem. Write skew can occur if two transactions read the same objects, and then update some of those objects. In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing). Ways to prevent write skew are a bit more restricted: Atomic operations don't help as things involve more objects. Automatically prevent write skew requires true serializable isolation. The second-best option in this case is probably to explicitly lock the rows that the transaction depends on. 1 2 3 4 5 6 7 8 9 10 11 12 BEGIN TRANSACTION ; SELECT * FROM doctors WHERE on_call = true AND shift_id = 1234 FOR UPDATE ; UPDATE doctors SET on_call = false WHERE name = 'Alice' AND shift_id = 1234 ; COMMIT ;","title":"Write skew and phantoms"},{"location":"Notes/books/DDIA/chapter7/#serializability","text":"This is the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially , without concurrency. Basically, the database prevents all possible race conditions. There are three techniques for achieving this: Executing transactions in serial order Two-phase locking Serializable snapshot isolation.","title":"Serializability"},{"location":"Notes/books/DDIA/chapter7/#actual-serial-execution","text":"The simplest way of removing concurrency problems is to remove concurrency entirely and execute only one transaction at a time, in serial order, on a single thread. This approach is implemented by VoltDB/H-Store, Redis and Datomic.","title":"Actual serial execution"},{"location":"Notes/books/DDIA/chapter7/#two-phase-locking-2pl","text":"Two-phase locking (2PL) sounds similar to two-phase commit (2PC) but be aware that they are completely different things. Several transactions are allowed to concurrently read the same object as long as nobody is writing it. When somebody wants to write (modify or delete) an object, exclusive access is required. Writers don't just block other writers; they also block readers. Readers block writers. It protects against all the race conditions discussed earlier.","title":"Two-phase locking (2PL)"},{"location":"Notes/books/DDIA/chapter7/#serializable-snapshot-isolation-ssi","text":"It provides full serializability but only has a small performance penalty compared to snapshot isolation. SSI is fairly new and might become the new default in the future.","title":"Serializable snapshot isolation (SSI)"},{"location":"Notes/books/DDIA/chapter8/","text":"8. The Trouble With Distributed Systems Faults and Partial Failures A program on a single computer either works or it doesn't. There is no reason why software should be flaky (non deterministic). In a distributed systems we have no choice but to confront the messy reality of the physical world. There will be parts that are broken in an unpredictable way, while others work. Partial failures are nondeterministic . Things will unpredicably fail. We need to accept the possibility of partial failure and build fault-tolerant mechanism into the software. We need to build a reliable system from unreliable components. Unreliable Networks Focusing on shared-nothing systems the network is the only way machines communicate. The internet and most internal networks are asynchronous packet networks . A message is sent and the network gives no guarantees as to when it will arrive, or whether it will arrive at all. Things that could go wrong: Request lost Request waiting in a queue to be delivered later Remote node may have failed Remote node may have temporarily stoped responding Response has been lost on the network The response has been delayed and will be delivered later If you send a request to another node and don't receive a response, it is impossible to tell why. The usual way of handling this issue is a timeout : after some time you give up waiting and assume that the response is not going to arrive. Detecting Faults Many systems need to automatically detect faulty nodes. For example A load balancer needs to stop sending requests to a node that is dead In a distributed database with single-leader replication, if the leader fails, one of the followers needs to be promoted to be the new leader Nobody is immune to network problems. You do need to know how your software reacts to network problems to ensure that the system can recover from them. It may make sense to deliberately trigger network problems and test the system's response. If you want to be sure that a request was successful, you need a positive response from the application itself. If something has gone wrong, you have to assume that you will get no response at all. Timeouts and unbounded delays A long timeout means a long wait until a node is declared dead. A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead (when it could be a slowdown). Premature declaring a node is problematic, if the node is actually alive the action may end up being performed twice. When a node is declared dead, its responsibilities need to be transferred to other nodes, which places additional load on other nodes and the network. If the system is already struggling with high load, declaring nodes dead prematurely can make the problem worse. In particular, it could happen that the node actually wasn\u2019t dead but only slow to respond due to overload; transferring its load to other nodes can cause a cascading failure. Network congestion and queueing Different nodes try to send packets simultaneously to the same destination, the network switch must queue them and feed them to the destination one by one. The switch will discard packets when filled up. If CPU cores are busy, the request is queued by the operative system, until applications are ready to handle it. In virtual environments, the operative system is often paused while another virtual machine uses a CPU core. The VM queues the incoming data. TCP performs flow control , in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node. This means additional queuing at the sender. Moreover, TCP considers a packet to be lost if it is not acknowledged within some timeout (which is calculated from observed round-trip times), and lost packets are automatically retransmitted. Although the application does not see the packet loss and retransmission, it does see the resulting delay (waiting for the timeout to expire, and then waiting for the retransmitted packet to be acknowledged). You can choose timeouts experimentally by measuring the distribution of network round-trip times over an extended period. Systems can continually measure response times and their variability ( jitter ), and automatically adjust timeouts according to the observed response time distribution. TCP vs UDP Some latency-sensitive applications, such as videoconferencing and Voice over IP (VoIP), use UDP rather than TCP. It\u2019s a trade-off between reliability and variability of delays: as UDP does not perform flow control and does not retransmit lost packets, it avoids some of the reasons for variable network delays (although it is still suscepti\u2010 ble to switch queues and scheduling delays). UDP is a good choice in situations where delayed data is worthless. For example, in a VoIP phone call, there probably isn\u2019t enough time to retransmit a lost packet before its data is due to be played over the loudspeakers. In this case, there\u2019s no point in retransmitting the packet\u2014the application must instead fill the missing packet\u2019s time slot with silence (causing a brief interruption in the sound) and move on in the stream. The retry happens at the human layer instead. (\u201cCould you repeat that please? The sound just cut out for a moment.\u201d) Synchronous vs asynchronous networks A telephone network estabilishes a circuit , we say is synchronous even as the data passes through several routers as it does not suffer from queing. The maximum end-to-end latency of the network is fixed ( bounded delay ). A circuit is a fixed amount of reserved bandwidth which nobody else can use while the circuit is established, whereas packets of a TCP connection opportunistically use whatever network bandwidth is available. Using circuits for bursty data transfers wastes network capacity and makes transfer unnecessary slow. By contrast, TCP dynamically adapts the rate of data transfer to the available network capacity. The internet shares network bandwidth dynamically. Senders push and jostle with each other to get their packets over the wire as quickly as possible, and the network switches decide which packet to send (i.e., the bandwidth allocation) from one moment to the next. This approach has the downside of queueing, but the advantage is that it maximizes utilization of the wire. The wire has a fixed cost, so if you utilize it better, each byte you send over the wire is cheaper. We have to assume that network congestion, queueing, and unbounded delays will happen. Consequently, there's no \"correct\" value for timeouts, they need to be determined experimentally. Unreliable Clocks The time when a message is received is always later than the time when it is sent, we don't know how much later due to network delays. This makes difficult to determine the order of which things happened when multiple machines are involved. Each machine on the network has its own clock, slightly faster or slower than the other machines. It is possible to synchronise clocks with Network Time Protocol (NTP). Time-of-day clocks Return the current date and time according to some calendar ( wall-clock time ). If the local clock is too far ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time. This makes it is unsuitable for measuring elapsed time. Monotonic clocks System.nanoTime() . They are guaranteed to always move forward. The difference between clock reads can tell you how much time elapsed beween two checks. The absolute value of the clock is meaningless. NTP allows the clock rate to be sped up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock to jump forward or backward . In a distributed system, using a monotonic clock for measuring elapsed time (peg: timeouts), is usually fine . If some piece of software is relying on an accurately synchronised clock, the result is more likely to be silent and subtle data loss than a dramatic crash. You need to carefully monitor the clock offsets between all the machines. Relying on Synchronized Clocks Timestamps for ordering events It is tempting, but dangerous to rely on clocks for ordering of events across multiple nodes. This usually imply that last write wins (LWW), often used in both multi-leader replication and leaderless databases like Cassandra and Riak, and data-loss may happen. The definition of \"recent\" also depends on local time-of-day clock, which may well be incorrect. Logical clocks , based on counters instead of oscillating quartz crystal, are safer alternative for ordering events. Logical clocks do not measure time of the day or elapsed time, only relative ordering of events. This contrasts with time-of-the-day and monotic clocks (also known as physical clocks ). Clock readings have a confidence interval It doesn't make sense to think of a clock reading as a point in time, it is more like a range of times, within a confidence internval: for example, 95% confident that the time now is between 10.3 and 10.5. The most common implementation of snapshot isolation requires a monotonically increasing transaction ID. Spanner implements snapshot isolation across datacenters by using clock's confidence interval. If you have two confidence internvals where 1 2 A = [A earliest, A latest] B = [B earliest, B latest] And those two intervals do not overlap ( A earliest < A latest < B earliest < B latest ), then B definetively happened after A. Spanner deliberately waits for the length of the confidence interval before commiting a read-write transaction, so their confidence intervals do not overlap. Spanner needs to keep the clock uncertainty as small as possible, that's why Google deploys a GPS receiver or atomic clock in each datacenter. Process pauses How does a node know that it is still leader? One option is for the leader to obtain a lease from other nodes (similar to a lock with a timeout). It will be the leader until the lease expires; to remain leader, the node must periodically renew the lease. If the node fails, another node can takeover when it expires. 1 2 3 4 5 6 7 8 9 10 11 12 while ( true ) { request = getIncomingRequest (); // Ensure that the lease always has at least 10 seconds remaining if ( lease . expiryTimeMillis - System . currentTimeMillis () < 10000 ) { lease = lease . renew (); } if ( lease . isValid ()) { process ( request ); } } Suppose we have the above request-handling loop. Whats wrong with this code? It's relying on synchronized clocks. The expiry time on the lease is set by a differnet machine, and its being compared to the local system clock. If clocks are out of sync, we can experience weird behavior. Even if we change the protocol to only use the local monotonic clock, there is another problem: the code assumes that very little time passes between the point that it checks the time ( System.currentTimeMillis() ) and the time when the request is processed ( process(request) ). Normally this code runs very quickly, so the 10 second buffer is more than enough to ensure that the lease doesn't expire in the middle of processing a request. However, what if there is an unexpected pause in the execution of the program? Forexample, imagine the thread stops for 15 seconds around the line lease.isValid() before finally continuing. In that case, it\u2019s likely that the lease will have expired by the time the request is processed, and another node has already taken over as leader. However, there is nothing to tell this thread that it was paused for so long, so this code won\u2019t notice that the lease has expired until the next iteration of the loop by which time it may have already done something unsafe by processing the request. Process pauses can happen for many reasons. Garbage collector (stop the world) Virtual machine can be suspended In laptops execution may be suspended Operating system context-switches Synchronous disk access Swapping to disk (paging) Unix process can be stopped ( SIGSTOP ) Response time guarantees There are systems that require software to respond before a specific deadline ( real-time operating system, or RTOS ). Library functions must document their worst-case execution times; dynamic memory allocation may be restricted or disallowed and enormous amount of testing and measurement must be done. Garbage collection could be treated like brief planned outages. If the runtime can warn the application that a node soon requires a GC pause, the application can stop sending new requests to that node and perform GC while no requests are in progress. A variant of this idea is to use the garbage collector only for short-lived objects and to restart the process periodically. Knowledge, Truth and Lies A node cannot necessarily trust its own judgement of a situation. Many distributed systems rely on a quorum (voting among the nodes). Commonly, the quorum is an absolute majority of more than half of the nodes. Fencing tokens Incorrect implementation of a distributed lock: client 1 believes that it still has a valid lease, even though it has expired, and thus corrupts a file in storage. Making access to storage safe by allowing writes only in the order of increasing fencing tokens. Assume every time the lock server grants a lock or a lease, it also returns a fencing token , which is a number that increases every time a lock is granted (incremented by the lock service). Then we can require every time a client sends a write request to the storage service, it must include its current fencing token. The storage server remembers that it has already processed a write with a higher token number, so it rejects the request with the last token. If ZooKeeper is used as lock service, the transaciton ID zcid or the node version cversion can be used as a fencing token. Byzantine faults Fencing tokens can detect and block a node that is inadvertently acting in error. Distributed systems become much harder if there is a risk that nodes may \"lie\" ( byzantine fault ). A system is Byzantine fault-tolerant if it continues to operate correctly even if some of the nodes are malfunctioning. Aerospace environments Multiple participating organisations, some participants may attempt ot cheat or defraud others","title":"8. The Trouble With Distributed Systems"},{"location":"Notes/books/DDIA/chapter8/#8-the-trouble-with-distributed-systems","text":"","title":"8. The Trouble With Distributed Systems"},{"location":"Notes/books/DDIA/chapter8/#faults-and-partial-failures","text":"A program on a single computer either works or it doesn't. There is no reason why software should be flaky (non deterministic). In a distributed systems we have no choice but to confront the messy reality of the physical world. There will be parts that are broken in an unpredictable way, while others work. Partial failures are nondeterministic . Things will unpredicably fail. We need to accept the possibility of partial failure and build fault-tolerant mechanism into the software. We need to build a reliable system from unreliable components.","title":"Faults and Partial Failures"},{"location":"Notes/books/DDIA/chapter8/#unreliable-networks","text":"Focusing on shared-nothing systems the network is the only way machines communicate. The internet and most internal networks are asynchronous packet networks . A message is sent and the network gives no guarantees as to when it will arrive, or whether it will arrive at all. Things that could go wrong: Request lost Request waiting in a queue to be delivered later Remote node may have failed Remote node may have temporarily stoped responding Response has been lost on the network The response has been delayed and will be delivered later If you send a request to another node and don't receive a response, it is impossible to tell why. The usual way of handling this issue is a timeout : after some time you give up waiting and assume that the response is not going to arrive.","title":"Unreliable Networks"},{"location":"Notes/books/DDIA/chapter8/#detecting-faults","text":"Many systems need to automatically detect faulty nodes. For example A load balancer needs to stop sending requests to a node that is dead In a distributed database with single-leader replication, if the leader fails, one of the followers needs to be promoted to be the new leader Nobody is immune to network problems. You do need to know how your software reacts to network problems to ensure that the system can recover from them. It may make sense to deliberately trigger network problems and test the system's response. If you want to be sure that a request was successful, you need a positive response from the application itself. If something has gone wrong, you have to assume that you will get no response at all.","title":"Detecting Faults"},{"location":"Notes/books/DDIA/chapter8/#timeouts-and-unbounded-delays","text":"A long timeout means a long wait until a node is declared dead. A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead (when it could be a slowdown). Premature declaring a node is problematic, if the node is actually alive the action may end up being performed twice. When a node is declared dead, its responsibilities need to be transferred to other nodes, which places additional load on other nodes and the network. If the system is already struggling with high load, declaring nodes dead prematurely can make the problem worse. In particular, it could happen that the node actually wasn\u2019t dead but only slow to respond due to overload; transferring its load to other nodes can cause a cascading failure.","title":"Timeouts and unbounded delays"},{"location":"Notes/books/DDIA/chapter8/#network-congestion-and-queueing","text":"Different nodes try to send packets simultaneously to the same destination, the network switch must queue them and feed them to the destination one by one. The switch will discard packets when filled up. If CPU cores are busy, the request is queued by the operative system, until applications are ready to handle it. In virtual environments, the operative system is often paused while another virtual machine uses a CPU core. The VM queues the incoming data. TCP performs flow control , in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node. This means additional queuing at the sender. Moreover, TCP considers a packet to be lost if it is not acknowledged within some timeout (which is calculated from observed round-trip times), and lost packets are automatically retransmitted. Although the application does not see the packet loss and retransmission, it does see the resulting delay (waiting for the timeout to expire, and then waiting for the retransmitted packet to be acknowledged). You can choose timeouts experimentally by measuring the distribution of network round-trip times over an extended period. Systems can continually measure response times and their variability ( jitter ), and automatically adjust timeouts according to the observed response time distribution.","title":"Network congestion and queueing"},{"location":"Notes/books/DDIA/chapter8/#synchronous-vs-asynchronous-networks","text":"A telephone network estabilishes a circuit , we say is synchronous even as the data passes through several routers as it does not suffer from queing. The maximum end-to-end latency of the network is fixed ( bounded delay ). A circuit is a fixed amount of reserved bandwidth which nobody else can use while the circuit is established, whereas packets of a TCP connection opportunistically use whatever network bandwidth is available. Using circuits for bursty data transfers wastes network capacity and makes transfer unnecessary slow. By contrast, TCP dynamically adapts the rate of data transfer to the available network capacity. The internet shares network bandwidth dynamically. Senders push and jostle with each other to get their packets over the wire as quickly as possible, and the network switches decide which packet to send (i.e., the bandwidth allocation) from one moment to the next. This approach has the downside of queueing, but the advantage is that it maximizes utilization of the wire. The wire has a fixed cost, so if you utilize it better, each byte you send over the wire is cheaper. We have to assume that network congestion, queueing, and unbounded delays will happen. Consequently, there's no \"correct\" value for timeouts, they need to be determined experimentally.","title":"Synchronous vs asynchronous networks"},{"location":"Notes/books/DDIA/chapter8/#unreliable-clocks","text":"The time when a message is received is always later than the time when it is sent, we don't know how much later due to network delays. This makes difficult to determine the order of which things happened when multiple machines are involved. Each machine on the network has its own clock, slightly faster or slower than the other machines. It is possible to synchronise clocks with Network Time Protocol (NTP). Time-of-day clocks Return the current date and time according to some calendar ( wall-clock time ). If the local clock is too far ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time. This makes it is unsuitable for measuring elapsed time. Monotonic clocks System.nanoTime() . They are guaranteed to always move forward. The difference between clock reads can tell you how much time elapsed beween two checks. The absolute value of the clock is meaningless. NTP allows the clock rate to be sped up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock to jump forward or backward . In a distributed system, using a monotonic clock for measuring elapsed time (peg: timeouts), is usually fine . If some piece of software is relying on an accurately synchronised clock, the result is more likely to be silent and subtle data loss than a dramatic crash. You need to carefully monitor the clock offsets between all the machines.","title":"Unreliable Clocks"},{"location":"Notes/books/DDIA/chapter8/#relying-on-synchronized-clocks","text":"","title":"Relying on Synchronized Clocks"},{"location":"Notes/books/DDIA/chapter8/#process-pauses","text":"How does a node know that it is still leader? One option is for the leader to obtain a lease from other nodes (similar to a lock with a timeout). It will be the leader until the lease expires; to remain leader, the node must periodically renew the lease. If the node fails, another node can takeover when it expires. 1 2 3 4 5 6 7 8 9 10 11 12 while ( true ) { request = getIncomingRequest (); // Ensure that the lease always has at least 10 seconds remaining if ( lease . expiryTimeMillis - System . currentTimeMillis () < 10000 ) { lease = lease . renew (); } if ( lease . isValid ()) { process ( request ); } } Suppose we have the above request-handling loop. Whats wrong with this code? It's relying on synchronized clocks. The expiry time on the lease is set by a differnet machine, and its being compared to the local system clock. If clocks are out of sync, we can experience weird behavior. Even if we change the protocol to only use the local monotonic clock, there is another problem: the code assumes that very little time passes between the point that it checks the time ( System.currentTimeMillis() ) and the time when the request is processed ( process(request) ). Normally this code runs very quickly, so the 10 second buffer is more than enough to ensure that the lease doesn't expire in the middle of processing a request. However, what if there is an unexpected pause in the execution of the program? Forexample, imagine the thread stops for 15 seconds around the line lease.isValid() before finally continuing. In that case, it\u2019s likely that the lease will have expired by the time the request is processed, and another node has already taken over as leader. However, there is nothing to tell this thread that it was paused for so long, so this code won\u2019t notice that the lease has expired until the next iteration of the loop by which time it may have already done something unsafe by processing the request. Process pauses can happen for many reasons. Garbage collector (stop the world) Virtual machine can be suspended In laptops execution may be suspended Operating system context-switches Synchronous disk access Swapping to disk (paging) Unix process can be stopped ( SIGSTOP )","title":"Process pauses"},{"location":"Notes/books/DDIA/chapter8/#knowledge-truth-and-lies","text":"A node cannot necessarily trust its own judgement of a situation. Many distributed systems rely on a quorum (voting among the nodes). Commonly, the quorum is an absolute majority of more than half of the nodes.","title":"Knowledge, Truth and Lies"},{"location":"Notes/books/DDIA/chapter8/#fencing-tokens","text":"Incorrect implementation of a distributed lock: client 1 believes that it still has a valid lease, even though it has expired, and thus corrupts a file in storage. Making access to storage safe by allowing writes only in the order of increasing fencing tokens. Assume every time the lock server grants a lock or a lease, it also returns a fencing token , which is a number that increases every time a lock is granted (incremented by the lock service). Then we can require every time a client sends a write request to the storage service, it must include its current fencing token. The storage server remembers that it has already processed a write with a higher token number, so it rejects the request with the last token. If ZooKeeper is used as lock service, the transaciton ID zcid or the node version cversion can be used as a fencing token.","title":"Fencing tokens"},{"location":"Notes/books/DDIA/chapter8/#byzantine-faults","text":"Fencing tokens can detect and block a node that is inadvertently acting in error. Distributed systems become much harder if there is a risk that nodes may \"lie\" ( byzantine fault ). A system is Byzantine fault-tolerant if it continues to operate correctly even if some of the nodes are malfunctioning. Aerospace environments Multiple participating organisations, some participants may attempt ot cheat or defraud others","title":"Byzantine faults"},{"location":"Notes/papers/Cluster%20Management/raft/","text":"Raft Raft: In Search of an Understandable Consensus Algorithm A consensus algorithm for managing a replicated log MIT Notes Part1 Part2 MIT FAQ Part1 Part2 Raft Basics Raft decomposes the consensus problem into three relatively independent subproblems Leader Election Log replication Safety Properties Election Safety : at most one leader can be elected in a given term. Leader Append-Only : a leader never overwrites or deletes entries in its log; it only appends new entries. Log Matching : if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. Leader Completeness : if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. State Machine Safety : if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. Raft divides time into terms of arbitrary length. Used extensively in leader election and log matching. 1. Leader Election When servers start up, they begin as followers Leader sends periodic heartbeats (AppendEntries RPC with no log entries) to all followers to maintain authority. If follower receives no communication over election timeout , then it assumes there is no viable leader and begins an election to choose a new leader. Begin an election: follower increments its current term and transitions to candidate state, votes for itself and issue RequestVoteRPCs in parallel to each else Candidate continue in this state until one of three things happens: a. it wins b. other wins c. no one wins Candidate wins if it receives the majority of votes, voting is first-come-first-served. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If its term is larger or equal than itself's, the candidate recognizes the leader as legit and returns to follower state Case : No one wins -> after timeout, new election (150ms - 300ms) - Raft uses randomized election timeouts to ensure that split votes are rare. 2. Log Replication I. Steps Leader appends log entry to its own log Send AppendEntry to every server When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. II. Commit A log entry is committed once the leader that created the entry has replicated it on a majority of the servers Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines This also commits all preceding entries in the leader's log, including entries created by previous leaders Leader keep tracks of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs. Once a follower learns that a log entry is committed, it applies the entry to its local state machine. III. Inconsistency Handling Leader maintains a nextIndex fields for each follower. It initializes nextIndex values to the index just after the last one in its log. If follower's log is inconsistent with the leader's, the AppendEntries RPC consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC 3. Safety Ensure each state machine executes same commands in the same order I. Election Restriction Candidate only elected as leader when its log is at least as up-to-date as any other log in that majority Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. Term1 > Term2 -> Term1 wins Term1 == Term2 -> Log with greater length wins II. Committing entries from previous terms Never commits log entries from previous terms by counting replicas. Only log entries from the leader's current term are committed by counting replicas Once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. Log Compaction InstallSnapshot RPC As Raft log grows without bound, it occupies more space and takes more time to replay on server startup. Snapshotting - saves entire system state to a snapshot on stable storage and the entire log up to that point is discarded. Each server takes snapshots independently covering just the committed entries in its log. Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind. This happens when the leader has already discarded the next log entry that it needs to send to a follower. Summary","title":"Raft"},{"location":"Notes/papers/Cluster%20Management/raft/#raft","text":"Raft: In Search of an Understandable Consensus Algorithm A consensus algorithm for managing a replicated log MIT Notes Part1 Part2 MIT FAQ Part1 Part2","title":"Raft"},{"location":"Notes/papers/Cluster%20Management/raft/#raft-basics","text":"Raft decomposes the consensus problem into three relatively independent subproblems Leader Election Log replication Safety Properties Election Safety : at most one leader can be elected in a given term. Leader Append-Only : a leader never overwrites or deletes entries in its log; it only appends new entries. Log Matching : if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. Leader Completeness : if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. State Machine Safety : if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. Raft divides time into terms of arbitrary length. Used extensively in leader election and log matching.","title":"Raft Basics"},{"location":"Notes/papers/Cluster%20Management/raft/#1-leader-election","text":"When servers start up, they begin as followers Leader sends periodic heartbeats (AppendEntries RPC with no log entries) to all followers to maintain authority. If follower receives no communication over election timeout , then it assumes there is no viable leader and begins an election to choose a new leader. Begin an election: follower increments its current term and transitions to candidate state, votes for itself and issue RequestVoteRPCs in parallel to each else Candidate continue in this state until one of three things happens: a. it wins b. other wins c. no one wins Candidate wins if it receives the majority of votes, voting is first-come-first-served. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If its term is larger or equal than itself's, the candidate recognizes the leader as legit and returns to follower state Case : No one wins -> after timeout, new election (150ms - 300ms) - Raft uses randomized election timeouts to ensure that split votes are rare.","title":"1. Leader Election"},{"location":"Notes/papers/Cluster%20Management/raft/#2-log-replication","text":"","title":"2. Log Replication"},{"location":"Notes/papers/Cluster%20Management/raft/#i-steps","text":"Leader appends log entry to its own log Send AppendEntry to every server When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client.","title":"I. Steps"},{"location":"Notes/papers/Cluster%20Management/raft/#ii-commit","text":"A log entry is committed once the leader that created the entry has replicated it on a majority of the servers Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines This also commits all preceding entries in the leader's log, including entries created by previous leaders Leader keep tracks of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs. Once a follower learns that a log entry is committed, it applies the entry to its local state machine.","title":"II. Commit"},{"location":"Notes/papers/Cluster%20Management/raft/#iii-inconsistency-handling","text":"Leader maintains a nextIndex fields for each follower. It initializes nextIndex values to the index just after the last one in its log. If follower's log is inconsistent with the leader's, the AppendEntries RPC consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC","title":"III. Inconsistency Handling"},{"location":"Notes/papers/Cluster%20Management/raft/#3-safety","text":"Ensure each state machine executes same commands in the same order","title":"3. Safety"},{"location":"Notes/papers/Cluster%20Management/raft/#i-election-restriction","text":"Candidate only elected as leader when its log is at least as up-to-date as any other log in that majority Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. Term1 > Term2 -> Term1 wins Term1 == Term2 -> Log with greater length wins","title":"I. Election Restriction"},{"location":"Notes/papers/Cluster%20Management/raft/#ii-committing-entries-from-previous-terms","text":"Never commits log entries from previous terms by counting replicas. Only log entries from the leader's current term are committed by counting replicas Once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property.","title":"II. Committing entries from previous terms"},{"location":"Notes/papers/Cluster%20Management/raft/#log-compaction","text":"","title":"Log Compaction"},{"location":"Notes/papers/Cluster%20Management/raft/#installsnapshot-rpc","text":"As Raft log grows without bound, it occupies more space and takes more time to replay on server startup. Snapshotting - saves entire system state to a snapshot on stable storage and the entire log up to that point is discarded. Each server takes snapshots independently covering just the committed entries in its log. Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind. This happens when the leader has already discarded the next log entry that it needs to send to a follower.","title":"InstallSnapshot RPC"},{"location":"Notes/papers/Cluster%20Management/raft/#summary","text":"","title":"Summary"},{"location":"Notes/papers/Cluster%20Management/zookeeper/","text":"ZooKeeper ZooKeeper: Wait-free coordination for Internet-scale systems https://dzone.com/articles/zookeeper-in-15-minutes A service for coordinating processes of distributed applications. MIT Notes , FAQ Basics ZooKeeper provides a coordination kernel for clients to implement primitives for dynamic configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper relaxes the conditions provided by Raft: reads are eventually consistent and can be served by replicas which increases read throughput. Zookeeper guarantees : 1) linearizable writes 2) FIFO client ordering for all operations, all requests from a given client are executed in the order that they were sent by the client. ZooKeeper target workload read to write ratio is 2:1 to 100:1 with data in the MB range. Best for Read Heavy applications such as storing configuration information since many servers will read this data and the data is small. Also, servers will pass a watch flag on reads on the configuration files and will be notified whenever they change. Service Overview znode ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. Three types of znodes: regular ephemeral (automatically removed when corresponding session terminates). sequential (when a file is created with a given name, ZooKeeper appends a number. ZooKeeper guarantees to never repeat a number if several clients try to write.) znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB). Client API ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages. Operations create(path, data, flags(regular, ephemeral, sequential)): returns error if alraedy exists unless sequential create delete(path, version): deletes if version matches exists(path, watch): watch is bool getData(path, watch) setData(path, data, version): sets if version matches getChildren(path, watch) sync(path): waits for all pending writes to complete Implementation ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. Snapshots are fuzzy since ZooKeeper state is not locked when taking the snapshot. After reboot, ZooKeeper constructs a consistent snapshot by replaying all log entries from the point at which the snapshot started. Because updates in Zookeeper are idempotent and delivered in the same order, the application-state will be correct after reboot. Atomic Broadcast Zab is an atomic broadcast protocol, uses simple majority quorums to decide on a proposal. Leader executes the requests and broadcasts the change to the ZooKeeper state through Zab. Zab guarantees the changes broadcast by a leader are delivered in order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes. TCP for transport so message order is maintained by the network. Use log to keep track of proposals as the write-ahead log for the in-memory database. Client Server Interactions Read is handled locally in memory. Each read request is processed and tagged with a zxid that corresponds to the last transaction seen by the server. Zxid defines the partial order of the read requests with respect to the write requests. Drawback: not guaranteeing precedence order for read operations, read may return a stale vlue. Should use sync flag to indicate follower to sync with leader. Sync: place sync operation at the end of the queue of requests between the leader and the server executing the call to sync. If pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction. Heartbeat: send heartbeat after the session has been idle for s/3 ms and switch to a new server if it has not heard from a server for 2s/3 ms. s is session timeout in ms. Use Cases Dynamic Configuration In A Distributed Application. Processes startup with the full pathname of z c , a znode storing dynamic configuration. Set watch flag to true, read config file, upon notified and read new configuration, again set the watch flag to true Rendezvous Client creates rendezvous node, z r and the full path nameof z r as a startup parameter of the master and worker processes. When the master starts it fills in zr with information about addresses and ports it is using. When workers start, they read zr with watch set to true. Group Membership Designate node, z g to represent the group. When a process member of the group starts, it creates an ephemeral child znode under z g . Configuration Service creates a watch on z_g and whenever that znode changes or any of its children, it will get a notification. Processes can then obtain group information by simply listing the children of z_g . Mini Transactions - Effect is that we can achieve atomic operations. Example of atomic counter: 1 2 3 4 while true : x , v = getData ( \"f\" ) if setData ( \"f\" , x + 1 , v ): break sleep Simple Locks Without Herd Effect (Scalable Locks)","title":"ZooKeeper"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#zookeeper","text":"ZooKeeper: Wait-free coordination for Internet-scale systems https://dzone.com/articles/zookeeper-in-15-minutes A service for coordinating processes of distributed applications. MIT Notes , FAQ","title":"ZooKeeper"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#basics","text":"ZooKeeper provides a coordination kernel for clients to implement primitives for dynamic configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper relaxes the conditions provided by Raft: reads are eventually consistent and can be served by replicas which increases read throughput. Zookeeper guarantees : 1) linearizable writes 2) FIFO client ordering for all operations, all requests from a given client are executed in the order that they were sent by the client. ZooKeeper target workload read to write ratio is 2:1 to 100:1 with data in the MB range. Best for Read Heavy applications such as storing configuration information since many servers will read this data and the data is small. Also, servers will pass a watch flag on reads on the configuration files and will be notified whenever they change.","title":"Basics"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#service-overview","text":"","title":"Service Overview"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#znode","text":"ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. Three types of znodes: regular ephemeral (automatically removed when corresponding session terminates). sequential (when a file is created with a given name, ZooKeeper appends a number. ZooKeeper guarantees to never repeat a number if several clients try to write.) znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB).","title":"znode"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#client-api","text":"ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages.","title":"Client API"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#implementation","text":"ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. Snapshots are fuzzy since ZooKeeper state is not locked when taking the snapshot. After reboot, ZooKeeper constructs a consistent snapshot by replaying all log entries from the point at which the snapshot started. Because updates in Zookeeper are idempotent and delivered in the same order, the application-state will be correct after reboot.","title":"Implementation"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#atomic-broadcast","text":"Zab is an atomic broadcast protocol, uses simple majority quorums to decide on a proposal. Leader executes the requests and broadcasts the change to the ZooKeeper state through Zab. Zab guarantees the changes broadcast by a leader are delivered in order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes. TCP for transport so message order is maintained by the network. Use log to keep track of proposals as the write-ahead log for the in-memory database.","title":"Atomic Broadcast"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#client-server-interactions","text":"Read is handled locally in memory. Each read request is processed and tagged with a zxid that corresponds to the last transaction seen by the server. Zxid defines the partial order of the read requests with respect to the write requests. Drawback: not guaranteeing precedence order for read operations, read may return a stale vlue. Should use sync flag to indicate follower to sync with leader. Sync: place sync operation at the end of the queue of requests between the leader and the server executing the call to sync. If pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction. Heartbeat: send heartbeat after the session has been idle for s/3 ms and switch to a new server if it has not heard from a server for 2s/3 ms. s is session timeout in ms.","title":"Client Server Interactions"},{"location":"Notes/papers/Cluster%20Management/zookeeper/#use-cases","text":"Dynamic Configuration In A Distributed Application. Processes startup with the full pathname of z c , a znode storing dynamic configuration. Set watch flag to true, read config file, upon notified and read new configuration, again set the watch flag to true Rendezvous Client creates rendezvous node, z r and the full path nameof z r as a startup parameter of the master and worker processes. When the master starts it fills in zr with information about addresses and ports it is using. When workers start, they read zr with watch set to true. Group Membership Designate node, z g to represent the group. When a process member of the group starts, it creates an ephemeral child znode under z g . Configuration Service creates a watch on z_g and whenever that znode changes or any of its children, it will get a notification. Processes can then obtain group information by simply listing the children of z_g . Mini Transactions - Effect is that we can achieve atomic operations. Example of atomic counter: 1 2 3 4 while true : x , v = getData ( \"f\" ) if setData ( \"f\" , x + 1 , v ): break sleep Simple Locks Without Herd Effect (Scalable Locks)","title":"Use Cases"},{"location":"Notes/papers/Compute/distributedTransactions/","text":"Distributed Transactions Distributed Transactions MIT Notes FAQ Background Problem: lots of data records, sharded on multiple servers, lots of clients Correct behavior of a xactions: ACID A: Atomicity, all writes or none, despite failures C: obeys application-specific invariants I: Isolation, no interference between xactions -- serializable D: Durability, committed writes are permanent Distributed Transactions have two big components: concurrency control (to provide isolation/serializability) atomic commit (to provide atomicity despite failure) Serializable Definition : there exists some serial order of those concurrent transactions that would, if followed, lead to the same ending state.* an easy model for programmers: they can write complex transactions while ignoring concurrency It allows parallel execution of transactions on different records example transactions 1 2 3 4 5 6 7 8 9 10 11 12 x and y are bank balances -- records in database tables x and y are on different servers (maybe at different banks) x and y start out as $10 T1 and T2 are transactions T1: transfer $1 from x to y T2: audit, to check that no money is lost T1: T2: begin_xaction begin_xaction add(x, 1) tmp1 = get(x) add(y, -1) tmp2 = get(y) end_xaction print tmp1, tmp2 end_xaction execute concurrent transactions T1 and T2 1 2 3 T1; T2 : x=11 y=9 \"11,9\" T2; T1 : x=11 y=9 \"10,10\" the results for the two differ; either is OK Concurrency Control Distributed transactions have two big components: concurrency control (to provide isolation/serializability) atomic commit (to provide atomicity despite failure) Two classes of concurrency control for transactions: pessimistic: lock records before use conflicts cause delays (waiting for locks) optimistic: use records without locking commit checks if reads/writes were serializable conflict causes abort+retry called Optimistic Concurrency Control (OCC) pessimistic is faster if conflicts are frequent optimistic is faster if conflicts are rare Two-Phase locking Two-phase locking is a pessimistic form of concurrency control and one way to achieve serializability: a transaction must acquire a record's lock before using it a transaction must hold its locks until after commit or abort can result in deadlock. Systems must be smart enough to detect and abort. Atomic Commit - Two Phase Commit A bunch of computers are cooperating on some task, each computer has a different role. We want to ensure atomicity: all execute, or none execute. Challenges : failure and performance Process Data is sharded among multiple servers Transactions run on \"transaction coordinators\" (TCs) For each read/write, TC sends RPC to relevant shard server Each is a \"participant\" who manages locks for its shard of the data There may be many concurrent transactions, many TCs TC assigns unique transaction ID (TID) to each transaction Every message, every table entry tagged with TID to avoid confusion 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 TC A B |----put------>|(lock data) |-----------get---------->|(lock data) | | |---prepare--->| |----------prepare-------->| | |<---yes/no----| |<-----------yes/no--------| | | |-commit/abort->|(commit/abort and release lock) |---------commit/abort---->|(commit/abort and release lock) |<----ack-----| |<-------------ack---------| Why is this correct so far? Neither A or B can commit unless they both agreed. Failure tolerance What if B crashes and restarts? If B sent YES before crash, B must remember (since it wrote to log despite crash)! Because A might have received a COMMIT and committed. So B must be able to commit (or not) even after a reboot. What if TC crashes and restarts? If TC might have sent COMMIT before crash, TC must remember! Since one worker may already have committed. write log to disk before sending COMMIT msgs. repeat COMMIT if it crashes and reboots, or if a participants asks. What if TC never gets a YES/NO from B? Perhaps B crashed and didn't recover; perhaps network is broken. TC can time out, and abort (since has not sent any COMMIT msgs). What if B times out or crashes while waiting for PREPARE from TC? B has not yet responded to PREPARE, so TC can't have decided commit B can unilaterally abort, and release locks respond NO to future PREPARE What if B replied YES to PREPARE, but doesn't receive COMMIT or ABORT? B cannot decide to abort it, because TC might have gotten YES from both, and sent out COMMIT to A, but crashed before sending to B. cannot do anything, just wait for TC came back Two-phase commit perspective Used in sharded DBs when a transaction uses data on multiple shards Bad reputation - Thus usually used only in a single small domain slow: multiple rounds of messages slow: disk writes locks are held over the prepare/commit exchanges; blocks other xactions TC crash can cause indefinite blocking, with locks held TC crash can cause indefinite blocking, with locks held Raft Comparison Raft and two-phase commit solve different problems! Use Raft to get high availability by replicating i.e. to be able to operate when some servers are crashed the servers all do the same thing Use 2PC when each participant does something different And all of them must do their part 2PC does not help availability since all servers must be up to get anything done Raft does not ensure that all servers do something since only a majority have to be alive What if you want high availability and atomic commit? The TC and servers should each be replicated with Raft Run two-phase commit among the replicated services Then you can tolerate failures and still make progress Whats described is basically Spanner","title":"Distributed Transactions"},{"location":"Notes/papers/Compute/distributedTransactions/#distributed-transactions","text":"Distributed Transactions MIT Notes FAQ","title":"Distributed Transactions"},{"location":"Notes/papers/Compute/distributedTransactions/#background","text":"Problem: lots of data records, sharded on multiple servers, lots of clients Correct behavior of a xactions: ACID A: Atomicity, all writes or none, despite failures C: obeys application-specific invariants I: Isolation, no interference between xactions -- serializable D: Durability, committed writes are permanent Distributed Transactions have two big components: concurrency control (to provide isolation/serializability) atomic commit (to provide atomicity despite failure)","title":"Background"},{"location":"Notes/papers/Compute/distributedTransactions/#serializable","text":"Definition : there exists some serial order of those concurrent transactions that would, if followed, lead to the same ending state.* an easy model for programmers: they can write complex transactions while ignoring concurrency It allows parallel execution of transactions on different records example transactions 1 2 3 4 5 6 7 8 9 10 11 12 x and y are bank balances -- records in database tables x and y are on different servers (maybe at different banks) x and y start out as $10 T1 and T2 are transactions T1: transfer $1 from x to y T2: audit, to check that no money is lost T1: T2: begin_xaction begin_xaction add(x, 1) tmp1 = get(x) add(y, -1) tmp2 = get(y) end_xaction print tmp1, tmp2 end_xaction execute concurrent transactions T1 and T2 1 2 3 T1; T2 : x=11 y=9 \"11,9\" T2; T1 : x=11 y=9 \"10,10\" the results for the two differ; either is OK","title":"Serializable"},{"location":"Notes/papers/Compute/distributedTransactions/#concurrency-control","text":"Distributed transactions have two big components: concurrency control (to provide isolation/serializability) atomic commit (to provide atomicity despite failure) Two classes of concurrency control for transactions: pessimistic: lock records before use conflicts cause delays (waiting for locks) optimistic: use records without locking commit checks if reads/writes were serializable conflict causes abort+retry called Optimistic Concurrency Control (OCC) pessimistic is faster if conflicts are frequent optimistic is faster if conflicts are rare","title":"Concurrency Control"},{"location":"Notes/papers/Compute/distributedTransactions/#two-phase-locking","text":"Two-phase locking is a pessimistic form of concurrency control and one way to achieve serializability: a transaction must acquire a record's lock before using it a transaction must hold its locks until after commit or abort can result in deadlock. Systems must be smart enough to detect and abort.","title":"Two-Phase locking"},{"location":"Notes/papers/Compute/distributedTransactions/#atomic-commit-two-phase-commit","text":"A bunch of computers are cooperating on some task, each computer has a different role. We want to ensure atomicity: all execute, or none execute. Challenges : failure and performance","title":"Atomic Commit - Two Phase Commit"},{"location":"Notes/papers/Compute/distributedTransactions/#process","text":"Data is sharded among multiple servers Transactions run on \"transaction coordinators\" (TCs) For each read/write, TC sends RPC to relevant shard server Each is a \"participant\" who manages locks for its shard of the data There may be many concurrent transactions, many TCs TC assigns unique transaction ID (TID) to each transaction Every message, every table entry tagged with TID to avoid confusion 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 TC A B |----put------>|(lock data) |-----------get---------->|(lock data) | | |---prepare--->| |----------prepare-------->| | |<---yes/no----| |<-----------yes/no--------| | | |-commit/abort->|(commit/abort and release lock) |---------commit/abort---->|(commit/abort and release lock) |<----ack-----| |<-------------ack---------| Why is this correct so far? Neither A or B can commit unless they both agreed.","title":"Process"},{"location":"Notes/papers/Compute/distributedTransactions/#failure-tolerance","text":"What if B crashes and restarts? If B sent YES before crash, B must remember (since it wrote to log despite crash)! Because A might have received a COMMIT and committed. So B must be able to commit (or not) even after a reboot. What if TC crashes and restarts? If TC might have sent COMMIT before crash, TC must remember! Since one worker may already have committed. write log to disk before sending COMMIT msgs. repeat COMMIT if it crashes and reboots, or if a participants asks. What if TC never gets a YES/NO from B? Perhaps B crashed and didn't recover; perhaps network is broken. TC can time out, and abort (since has not sent any COMMIT msgs). What if B times out or crashes while waiting for PREPARE from TC? B has not yet responded to PREPARE, so TC can't have decided commit B can unilaterally abort, and release locks respond NO to future PREPARE What if B replied YES to PREPARE, but doesn't receive COMMIT or ABORT? B cannot decide to abort it, because TC might have gotten YES from both, and sent out COMMIT to A, but crashed before sending to B. cannot do anything, just wait for TC came back","title":"Failure tolerance"},{"location":"Notes/papers/Compute/distributedTransactions/#two-phase-commit-perspective","text":"Used in sharded DBs when a transaction uses data on multiple shards Bad reputation - Thus usually used only in a single small domain slow: multiple rounds of messages slow: disk writes locks are held over the prepare/commit exchanges; blocks other xactions TC crash can cause indefinite blocking, with locks held TC crash can cause indefinite blocking, with locks held","title":"Two-phase commit perspective"},{"location":"Notes/papers/Compute/distributedTransactions/#raft-comparison","text":"Raft and two-phase commit solve different problems! Use Raft to get high availability by replicating i.e. to be able to operate when some servers are crashed the servers all do the same thing Use 2PC when each participant does something different And all of them must do their part 2PC does not help availability since all servers must be up to get anything done Raft does not ensure that all servers do something since only a majority have to be alive What if you want high availability and atomic commit? The TC and servers should each be replicated with Raft Run two-phase commit among the replicated services Then you can tolerate failures and still make progress Whats described is basically Spanner","title":"Raft Comparison"},{"location":"Notes/papers/Compute/mapreduce/","text":"MapReduce MapReduce: Simplified Data Processing on Large Clusters A programming model and an associated implementation for processing and generating large data sets. Introduction Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. 1 2 map (k1,v1) \u2192 list(k2,v2) reduce (k2,list(v2)) \u2192 list(v2) Implementation Execution A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk partitioned into R regions. The locations of these buffered pairs on disk are passed back to the master, who forwards these locations to the reduce workers. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks. Master Data Structures stores state(idle, in-progress, competed) and identity of the worker machine for each task (non idle). for each completed map task: locations and sizes of the R intermediate file regions. The information is pushed incrementally to in-progress reduce workers. Fault Tolerance For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed and in-progress map tasks are reset to idle and reduce workers executing are notified. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry. Miscellaneous Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers. MR takes 44% longer without backup tasks. Refinements Partitioning Function: Typically hash(key) mod R is used for hashing. However, it is useful to allow custom partitioning function so that for example users can have all URLs from the same host end up in the same output file. Combiner Function: In some cases, there is significant repetition in the intermediate keys produced by each map task. For example, word count map produces <word, 1> for each word. All of these counts will be sent over RPC. Better allow user to specify optional Combiner that does partial merging of data before sent to reducers.","title":"MapReduce"},{"location":"Notes/papers/Compute/mapreduce/#mapreduce","text":"MapReduce: Simplified Data Processing on Large Clusters A programming model and an associated implementation for processing and generating large data sets.","title":"MapReduce"},{"location":"Notes/papers/Compute/mapreduce/#introduction","text":"Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. 1 2 map (k1,v1) \u2192 list(k2,v2) reduce (k2,list(v2)) \u2192 list(v2)","title":"Introduction"},{"location":"Notes/papers/Compute/mapreduce/#implementation","text":"","title":"Implementation"},{"location":"Notes/papers/Compute/mapreduce/#execution","text":"A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk partitioned into R regions. The locations of these buffered pairs on disk are passed back to the master, who forwards these locations to the reduce workers. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks.","title":"Execution"},{"location":"Notes/papers/Compute/mapreduce/#master-data-structures","text":"stores state(idle, in-progress, competed) and identity of the worker machine for each task (non idle). for each completed map task: locations and sizes of the R intermediate file regions. The information is pushed incrementally to in-progress reduce workers.","title":"Master Data Structures"},{"location":"Notes/papers/Compute/mapreduce/#fault-tolerance","text":"For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed and in-progress map tasks are reset to idle and reduce workers executing are notified. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry.","title":"Fault Tolerance"},{"location":"Notes/papers/Compute/mapreduce/#miscellaneous","text":"Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers. MR takes 44% longer without backup tasks.","title":"Miscellaneous"},{"location":"Notes/papers/Compute/mapreduce/#refinements","text":"Partitioning Function: Typically hash(key) mod R is used for hashing. However, it is useful to allow custom partitioning function so that for example users can have all URLs from the same host end up in the same output file. Combiner Function: In some cases, there is significant repetition in the intermediate keys produced by each map task. For example, word count map produces <word, 1> for each word. All of these counts will be sent over RPC. Better allow user to specify optional Combiner that does partial merging of data before sent to reducers.","title":"Refinements"},{"location":"Notes/papers/Storage/aurora/","text":"Aurora Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases A cloud-native relational database service for OLTP workloads. MIT Notes FAQ Overview We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora uses a novel architecture with a fleet of database instances and storage service. Several database functions(redo logging, crash recovery, etc) are offloaded to the storage service, which is like a virtualized segmented redo log (shared-disk architecture). Aurora uses two big ideas : Quorum writes for better fault-tolerance without too much waiting Storage servers understand how to apply DB's log to data pages, so only need to send (small) log entries, not (big) dirty pages. Sending to many replicas, but not much data. The log is the database; any page that the storage system materializes are simply a cache of log application. Three advantages over traditional approaches to traditional distributed databases First, by building storage as an independent faulttolerant and self-healing service across multiple data-centers, we protect the database from performance variance and transient or permanent failures at either the networking or storage tiers. Second, by only writing redo log records to storage, we are able to reduce network IOPS by an order of magnitude Third, we move some of the most complex and critical functions (backup and redo recovery) from one-time expensive operations in the database engine to continuous asynchronous operations amortized across a large distributed fleet. This yields near-instant crash recovery without checkpointing as well as inexpensive backups that do not interfere with foreground processing. HLD of Architecture To tolerate AZ failure, Aurora replicates each data item 6 ways across 3AZs with 2 copies in each AZ. Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. The only writes that cross the network are redo log records, so network load is drastically reduced despite amplifying write for replication. Storage nodes gossips with peers to fill gaps in received log records. Durable log application happens at the storage nodes continuously and asynchronously. Each log record has a monotonically-increasing Log Sequence Number(LSN). Instead of 2PC protocol, Aurora maintains points of consistency and durability and advances them when receiving acknowledgements for storage requests. Durability: the highest LSN at which all prior log records are available. Consistency: each transaction is broken up to mini-transactions, and the final log record in a mini-transaction is a consistency point. Normally read quorum is not needed since the database feeds log records to storage nodes and tracks progress. : Durability V nodes, read quorum V_r , write quorum V_w To ensure each write is aware of the most recent write: V_w > V/2 Read = max_version(all nodes), so V_r + V_w > V , it can ensure the request must be accepted by a node with most update data. Because each log entry have an index/version, we only pick the date with the most updated log. AZ (availability zone) level failure tolerance Losing an entire AZ and one additional node (AZ+1) without losing data Losing an entire AZ without impacting the ability to write data AZ = 3, V = 6, V_w = 4, V_r = 3 Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. Advantages of storage server based on quorum mechanism : Smoother handling of server failures, slow execution or network partition problems, because each operation does not require a response from all replica servers (for example, as opposed to chain replication, chain replication needs to wait for write operations to be completed on all replicas) Under the premise of satisfying W+R>V , W and R can be adjusted. For different read and write load conditions, if the read load is relatively large, R can be reduced, and vice versa Raft also uses the quorum mechanism: leader will submit the log entry only after most copies are written to the log entry , but Raft is more powerful: it can handle more complex operations (due to its sequential operations); leader can be automatically re-elected when a split-brain problem occurs The Log Is The Database Database execution process Paper assumes you know how DB works, how it uses storage. Let's describe the execution process of the write operation of a single-machine general transaction database. The data is stored in the B-Tree of the hard disk, and there are cached data pages in the database. Take the transaction x=x+10 y=y-10 as an example: First lock x and y DB server modifies only cached data pages as transaction runs and appends update info to Write-Ahead Log (redo log) At this time log entry can be expressed as: LSID TID Key old new Notes 101 7 x 500 510 x=x+10 102 7 y 750 740 y=y-10 103 7 commit transaction finished Release the locks of x and y after WAL is written to the hard disk, and reply to client Log Applicator acts on the modification of the log entry on the before image of the cached data page, which will generate an after image. Delayed writing can optimize performance because the data page is large Crash Recovery Replay all committed transactions in log ( redo ) Roll back all uncommitted transactions in log ( undo ) Aurora Log Processing In Aurora , Log Procesing is pushed down to the storage layer to generate data pages in the background or on-demand. The write data transmitted through the network is only REDO logs, thus reducing the network load , And provides considerable performance and durability. Extra Notes In shared-disk architecture, all data is shared by all nodes. In shared-nothing architecture, each node manages a subset of data. It is hard to use change buffer in shared-disk architecture, so writes are penalized when there are secondary indexes.*","title":"Aurora"},{"location":"Notes/papers/Storage/aurora/#aurora","text":"Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases A cloud-native relational database service for OLTP workloads. MIT Notes FAQ","title":"Aurora"},{"location":"Notes/papers/Storage/aurora/#overview","text":"We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora uses a novel architecture with a fleet of database instances and storage service. Several database functions(redo logging, crash recovery, etc) are offloaded to the storage service, which is like a virtualized segmented redo log (shared-disk architecture). Aurora uses two big ideas : Quorum writes for better fault-tolerance without too much waiting Storage servers understand how to apply DB's log to data pages, so only need to send (small) log entries, not (big) dirty pages. Sending to many replicas, but not much data. The log is the database; any page that the storage system materializes are simply a cache of log application. Three advantages over traditional approaches to traditional distributed databases First, by building storage as an independent faulttolerant and self-healing service across multiple data-centers, we protect the database from performance variance and transient or permanent failures at either the networking or storage tiers. Second, by only writing redo log records to storage, we are able to reduce network IOPS by an order of magnitude Third, we move some of the most complex and critical functions (backup and redo recovery) from one-time expensive operations in the database engine to continuous asynchronous operations amortized across a large distributed fleet. This yields near-instant crash recovery without checkpointing as well as inexpensive backups that do not interfere with foreground processing.","title":"Overview"},{"location":"Notes/papers/Storage/aurora/#hld-of-architecture","text":"To tolerate AZ failure, Aurora replicates each data item 6 ways across 3AZs with 2 copies in each AZ. Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. The only writes that cross the network are redo log records, so network load is drastically reduced despite amplifying write for replication. Storage nodes gossips with peers to fill gaps in received log records. Durable log application happens at the storage nodes continuously and asynchronously. Each log record has a monotonically-increasing Log Sequence Number(LSN). Instead of 2PC protocol, Aurora maintains points of consistency and durability and advances them when receiving acknowledgements for storage requests. Durability: the highest LSN at which all prior log records are available. Consistency: each transaction is broken up to mini-transactions, and the final log record in a mini-transaction is a consistency point. Normally read quorum is not needed since the database feeds log records to storage nodes and tracks progress. :","title":"HLD of Architecture"},{"location":"Notes/papers/Storage/aurora/#durability","text":"V nodes, read quorum V_r , write quorum V_w To ensure each write is aware of the most recent write: V_w > V/2 Read = max_version(all nodes), so V_r + V_w > V , it can ensure the request must be accepted by a node with most update data. Because each log entry have an index/version, we only pick the date with the most updated log. AZ (availability zone) level failure tolerance Losing an entire AZ and one additional node (AZ+1) without losing data Losing an entire AZ without impacting the ability to write data AZ = 3, V = 6, V_w = 4, V_r = 3 Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. Advantages of storage server based on quorum mechanism : Smoother handling of server failures, slow execution or network partition problems, because each operation does not require a response from all replica servers (for example, as opposed to chain replication, chain replication needs to wait for write operations to be completed on all replicas) Under the premise of satisfying W+R>V , W and R can be adjusted. For different read and write load conditions, if the read load is relatively large, R can be reduced, and vice versa Raft also uses the quorum mechanism: leader will submit the log entry only after most copies are written to the log entry , but Raft is more powerful: it can handle more complex operations (due to its sequential operations); leader can be automatically re-elected when a split-brain problem occurs","title":"Durability"},{"location":"Notes/papers/Storage/aurora/#the-log-is-the-database","text":"","title":"The Log Is The Database"},{"location":"Notes/papers/Storage/aurora/#database-execution-process","text":"Paper assumes you know how DB works, how it uses storage. Let's describe the execution process of the write operation of a single-machine general transaction database. The data is stored in the B-Tree of the hard disk, and there are cached data pages in the database. Take the transaction x=x+10 y=y-10 as an example: First lock x and y DB server modifies only cached data pages as transaction runs and appends update info to Write-Ahead Log (redo log) At this time log entry can be expressed as: LSID TID Key old new Notes 101 7 x 500 510 x=x+10 102 7 y 750 740 y=y-10 103 7 commit transaction finished Release the locks of x and y after WAL is written to the hard disk, and reply to client Log Applicator acts on the modification of the log entry on the before image of the cached data page, which will generate an after image. Delayed writing can optimize performance because the data page is large Crash Recovery Replay all committed transactions in log ( redo ) Roll back all uncommitted transactions in log ( undo )","title":"Database execution process"},{"location":"Notes/papers/Storage/aurora/#aurora-log-processing","text":"In Aurora , Log Procesing is pushed down to the storage layer to generate data pages in the background or on-demand. The write data transmitted through the network is only REDO logs, thus reducing the network load , And provides considerable performance and durability.","title":"Aurora Log Processing"},{"location":"Notes/papers/Storage/aurora/#extra-notes","text":"In shared-disk architecture, all data is shared by all nodes. In shared-nothing architecture, each node manages a subset of data. It is hard to use change buffer in shared-disk architecture, so writes are penalized when there are secondary indexes.*","title":"Extra Notes"},{"location":"Notes/papers/Storage/craq/","text":"CRAQ Object Storage on CRAQ High-throughput chain replication for read-mostly workloads. MIT Notes , FAQ Introduction Chain Replication with Apportioned Queries (CRAQ) is an improvement to chain replication. It distributes the load on all object copies to greatly improve read throughput while maintaining strong consistency. This article mainly summarizes the chain replication, the principle of CRAQ , and the consistency model of CRAQ . Chain Replication Chain Replication (CR) is a method of replicating data across multiple nodes: The nodes form a chain of length C The head node of the chain handles all write operations from the client When a node receives a write operation, it will propagate to every node in the chain Once the write reaches the tail node, it is applied to all copies in the chain and is considered committed When the tail node submits a write operation, it will notify up the chain and head will respond to the client The tail node handles all read operations, so only the submitted value can be returned by the read operation Chain replication achieves strong consistency : Since all read operations are performed at the tail, and all write operations are committed at the tail, the chain tail can simply apply a total sequence to all operations. Tradeoffs vs Raft Both CRAQ and Raft/Paxos are replicated state machines. They can be used to replicate any service that can be fit into a state machine mold (basically, processes a stream of requests one at a time). One application for Raft/Paxos is object storage. CR and CRAQ are likely to be faster than protocols like Raft that provide strong consistency because the CR head does less work than the Raft leader: the CR head sends writes to just one replica, while the Raft leader must send all operations to all followers. CR has a performance advantage for reads as well, since it serves them from the tail (not the head), while the Raft leader must serve all client requests. However, Raft/Paxos and CR/CRAQ differ significantly in their failure properties. Raft (and Paxos and ZooKeeper) can continue operating (with no pauses at all) even if a minority of nodes are crashed, slow, unreliable, or partitioned. A CRAQ or CR chain must stop if something like that goes wrong, and wait for a configuration manager to decide how to proceed. On the other hand the post-failure situation is significantly simpler in CR/CRAQ; recall Figures 7 and 8 in the Raft paper. Failure recovery for chain replication: When the head node fails: the subsequent node replaces it as the head node, and there is no missing committed write operation When the tail node fails: the previous node replaces it as the tail node, and there is no lost write operation When the intermediate node fails: removed from the chain, the previous node needs to resend the most recent write operation Limitations : All reads of an object must go to the tail node, resulting in heavy load. CRAQ CRAQ Principles CRAQ is an improvement of chain replication which allows any node in the chain to perform read operations: CRAQ Each node can store multiple versions of an object, and each version contains a monotonically increasing version number and an additional attribute (identifying clean or dirty ) When a node receives a new version of an object (via a write operation that propagates down), the node appends this latest version to the list of the object If the node is not the tail node, mark the version as dirty and pass the write operation to subsequent nodes If the node is the tail node, the version is marked as clean , at this time the write operation is committed . Then, the tail node sends ACK back in the chain to notify other nodes to submit When the ACK of the object version arrives at the node, the node will mark the object version as clean . The node can then delete all previous versions of the object When the node receives a read request from the object: -If the latest known version of the requested object is clean, the node will return this value -Otherwise, the node will contact the tail node and ask for the last submitted version number of the object on the tail node, and then the node will return this version of the object Performance Improvement CRAQ 's throughput improvement over CR occurs in two different situations: Read-intensive workload : Read operations can be performed on all nodes, so throughput is linearly proportional to chain length Write-intensive workload : In workloads with a large number of write operations, it is easier to read dirty data, so there are more query requests for tail nodes. However, the workload of querying the tail node is much lower than that of all read requests performed by the tail node, so the throughput of CRAQ is higher than that of CR . Consistency Model For read operations, CRAQ supports three consistency models: Strong Consistency : The read operation described in 4.1 enables the latest written data to be read for each read, thus providing strong consistency Eventual consistency : Allow nodes to return uncommitted new data, that is, allow client to read inconsistent object versions from different nodes. But for a client , since it establishes a session with the node, its read operation is guaranteed to be monotonous and consistent. Eventual consistency with maximum inconsistency boundary : Nodes are allowed to return uncommitted new data, but there is a limit of inconsistency. This limit can be based on version or time. For example, it is allowed to return newly written but uncommitted data within a period of time. ZooKeeper Coordination Service If the network connection between two adjacent nodes is disconnected, the subsequent node will want to become the head node, which will result in two head nodes. CRAQ itself will not solve such a problem, so an external distributed coordination service is needed to solve this problem, such as using ZooKeeper . ZooKeeper determines the composition of the chain, determines which node is the head and tail, and monitors which node has failed. When a network failure occurs, ZooKeeper determines the new composition of the chain, not based on each node's own perception of the network situation. Through the use of Zookeper watch flags , CRAQ nodes are guaranteed to receive a notification when nodes are added to or removed from a group. Similarly, a node can be notified when metadata in which it has expressed interest changes. During initialization, a CRAQ node creates an ephemeral file in /nodes/dc_name/node_id . CRAQ nodes can query /nodes/dc_name to determine the membership list for its datacenter, but instead of having to periodically check the list for changes, ZooKeeper provides processes with the ability to create a watch on a file. A CRAQ node, after creating an ephemeral file to notify other nodes it has joined the system, creates a watch on the children list of /nodes/dc_name , thereby guaranteeing that it receives a notification when a node is added or removed. Alternate Approaches To Improving Chain Replication A data center will probably have lots of distinct CR chains, each serving a fraction (shard) of the objects. Suppose you have three servers (S1, S2, and S3) and three chains (C1, C2, C3). Then you can have the three chains be: 1 2 3 C1: S1 S2 S3 C2: S2 S3 S1 C3: S3 S1 S2 Now, assuming activity on the three chains is roughly equal, the load on the three servers will also be roughly equal. In particular the load of serving client requests (head and tail) will be roughly equally divided among the three servers. This is a pretty reasonable arrangement; CRAQ is only better if it turns out that some chains see more load than others . Summary In Chain Replication, only the head and tail directly serve client requests; the other replicas help fault tolerance but not performance. Since the load on the head and tail is thus likely to be higher than the load on intermediate nodes, you could get into a situation where performance is bottlenecked by head/tail, yet there is plenty of idle CPU available in the intermediate nodes. CRAQ exploits that idle CPU by moving the read work to them.","title":"CRAQ"},{"location":"Notes/papers/Storage/craq/#craq","text":"Object Storage on CRAQ High-throughput chain replication for read-mostly workloads. MIT Notes , FAQ","title":"CRAQ"},{"location":"Notes/papers/Storage/craq/#introduction","text":"Chain Replication with Apportioned Queries (CRAQ) is an improvement to chain replication. It distributes the load on all object copies to greatly improve read throughput while maintaining strong consistency. This article mainly summarizes the chain replication, the principle of CRAQ , and the consistency model of CRAQ .","title":"Introduction"},{"location":"Notes/papers/Storage/craq/#chain-replication","text":"Chain Replication (CR) is a method of replicating data across multiple nodes: The nodes form a chain of length C The head node of the chain handles all write operations from the client When a node receives a write operation, it will propagate to every node in the chain Once the write reaches the tail node, it is applied to all copies in the chain and is considered committed When the tail node submits a write operation, it will notify up the chain and head will respond to the client The tail node handles all read operations, so only the submitted value can be returned by the read operation Chain replication achieves strong consistency : Since all read operations are performed at the tail, and all write operations are committed at the tail, the chain tail can simply apply a total sequence to all operations. Tradeoffs vs Raft Both CRAQ and Raft/Paxos are replicated state machines. They can be used to replicate any service that can be fit into a state machine mold (basically, processes a stream of requests one at a time). One application for Raft/Paxos is object storage. CR and CRAQ are likely to be faster than protocols like Raft that provide strong consistency because the CR head does less work than the Raft leader: the CR head sends writes to just one replica, while the Raft leader must send all operations to all followers. CR has a performance advantage for reads as well, since it serves them from the tail (not the head), while the Raft leader must serve all client requests. However, Raft/Paxos and CR/CRAQ differ significantly in their failure properties. Raft (and Paxos and ZooKeeper) can continue operating (with no pauses at all) even if a minority of nodes are crashed, slow, unreliable, or partitioned. A CRAQ or CR chain must stop if something like that goes wrong, and wait for a configuration manager to decide how to proceed. On the other hand the post-failure situation is significantly simpler in CR/CRAQ; recall Figures 7 and 8 in the Raft paper. Failure recovery for chain replication: When the head node fails: the subsequent node replaces it as the head node, and there is no missing committed write operation When the tail node fails: the previous node replaces it as the tail node, and there is no lost write operation When the intermediate node fails: removed from the chain, the previous node needs to resend the most recent write operation Limitations : All reads of an object must go to the tail node, resulting in heavy load.","title":"Chain Replication"},{"location":"Notes/papers/Storage/craq/#craq_1","text":"","title":"CRAQ"},{"location":"Notes/papers/Storage/craq/#craq-principles","text":"CRAQ is an improvement of chain replication which allows any node in the chain to perform read operations: CRAQ Each node can store multiple versions of an object, and each version contains a monotonically increasing version number and an additional attribute (identifying clean or dirty ) When a node receives a new version of an object (via a write operation that propagates down), the node appends this latest version to the list of the object If the node is not the tail node, mark the version as dirty and pass the write operation to subsequent nodes If the node is the tail node, the version is marked as clean , at this time the write operation is committed . Then, the tail node sends ACK back in the chain to notify other nodes to submit When the ACK of the object version arrives at the node, the node will mark the object version as clean . The node can then delete all previous versions of the object When the node receives a read request from the object: -If the latest known version of the requested object is clean, the node will return this value -Otherwise, the node will contact the tail node and ask for the last submitted version number of the object on the tail node, and then the node will return this version of the object","title":"CRAQ Principles"},{"location":"Notes/papers/Storage/craq/#performance-improvement","text":"CRAQ 's throughput improvement over CR occurs in two different situations: Read-intensive workload : Read operations can be performed on all nodes, so throughput is linearly proportional to chain length Write-intensive workload : In workloads with a large number of write operations, it is easier to read dirty data, so there are more query requests for tail nodes. However, the workload of querying the tail node is much lower than that of all read requests performed by the tail node, so the throughput of CRAQ is higher than that of CR .","title":"Performance Improvement"},{"location":"Notes/papers/Storage/craq/#consistency-model","text":"For read operations, CRAQ supports three consistency models: Strong Consistency : The read operation described in 4.1 enables the latest written data to be read for each read, thus providing strong consistency Eventual consistency : Allow nodes to return uncommitted new data, that is, allow client to read inconsistent object versions from different nodes. But for a client , since it establishes a session with the node, its read operation is guaranteed to be monotonous and consistent. Eventual consistency with maximum inconsistency boundary : Nodes are allowed to return uncommitted new data, but there is a limit of inconsistency. This limit can be based on version or time. For example, it is allowed to return newly written but uncommitted data within a period of time.","title":"Consistency Model"},{"location":"Notes/papers/Storage/craq/#zookeeper-coordination-service","text":"If the network connection between two adjacent nodes is disconnected, the subsequent node will want to become the head node, which will result in two head nodes. CRAQ itself will not solve such a problem, so an external distributed coordination service is needed to solve this problem, such as using ZooKeeper . ZooKeeper determines the composition of the chain, determines which node is the head and tail, and monitors which node has failed. When a network failure occurs, ZooKeeper determines the new composition of the chain, not based on each node's own perception of the network situation. Through the use of Zookeper watch flags , CRAQ nodes are guaranteed to receive a notification when nodes are added to or removed from a group. Similarly, a node can be notified when metadata in which it has expressed interest changes. During initialization, a CRAQ node creates an ephemeral file in /nodes/dc_name/node_id . CRAQ nodes can query /nodes/dc_name to determine the membership list for its datacenter, but instead of having to periodically check the list for changes, ZooKeeper provides processes with the ability to create a watch on a file. A CRAQ node, after creating an ephemeral file to notify other nodes it has joined the system, creates a watch on the children list of /nodes/dc_name , thereby guaranteeing that it receives a notification when a node is added or removed.","title":"ZooKeeper Coordination Service"},{"location":"Notes/papers/Storage/craq/#alternate-approaches-to-improving-chain-replication","text":"A data center will probably have lots of distinct CR chains, each serving a fraction (shard) of the objects. Suppose you have three servers (S1, S2, and S3) and three chains (C1, C2, C3). Then you can have the three chains be: 1 2 3 C1: S1 S2 S3 C2: S2 S3 S1 C3: S3 S1 S2 Now, assuming activity on the three chains is roughly equal, the load on the three servers will also be roughly equal. In particular the load of serving client requests (head and tail) will be roughly equally divided among the three servers. This is a pretty reasonable arrangement; CRAQ is only better if it turns out that some chains see more load than others .","title":"Alternate Approaches To Improving Chain Replication"},{"location":"Notes/papers/Storage/craq/#summary","text":"In Chain Replication, only the head and tail directly serve client requests; the other replicas help fault tolerance but not performance. Since the load on the head and tail is thus likely to be higher than the load on intermediate nodes, you could get into a situation where performance is bottlenecked by head/tail, yet there is plenty of idle CPU available in the intermediate nodes. CRAQ exploits that idle CPU by moving the read work to them.","title":"Summary"},{"location":"Notes/papers/Storage/dynamo/","text":"Dynamo Dynamo: Amazon\u2019s Highly Available Key-value Store A highly available key-value storage system for \"always-on\" experience. Background The Amazon platform is built on top of tens of thousands of server and network components, where there are always a small but significant number of components failing at any given time. Some applications like shopping cart need always-available storage technologies for customer experience. Introduction Dynamo provides only simple key-value interface; no operations span multiple data items. Unlike traditional commercial systems putting importance on consistency, Dynamo sacrifices consistency under certain failure scenarios for availability. Dynamo uses a synthesis of well known techniques to achieve scalability and availability: System Architecture Consistent hashing: To scale incrementally, Dynamo uses consistent hashing to partition data. The principle advantage is that arrival/departure of a node only affects immediate neighbors. Virtual node : Dynamo introduces the concept of virtual nodes to balance the load when membership changes and account for heterogeneity in the physical infrastructure (virtual nodes on same physical node are skipped in replication). Data Versioning : Dynamo uses vector clocks(list of <node, counter> pairs) to capture the causality between different versions of the same object. If Dynamo can't resolve divergent versions, it will return all objects and let applications resolve the conflicts. Sloppy quorum and hinted handoff: Dynamo uses quorum-based consistency protocol(R+W>N), but does not ensure strict quorum membership. When a node A is temporarily unavailable, another node B will help maintain the replica and deliver the replica to A when detecting A has recovered. Replica synchronization: Dynamo anti-entropy protocol uses hash trees to reduce the amount of data need to be transferred to detect replica inconsistencies. Gossip protocol: Each node contacts a random peer every second and two nodes reconcile their persisted membership change histories and views of failure state. To prevent logical partitions, some seed nodes are known to all nodes. Consistent Hashing Dynamo\u2019s partitioning scheme relies on consistent hashing to distribute the load across multiple storage hosts. In consistent hashing, the output range of a hash function is treated as a fixed circular space or \u201cring\u201d Each node in the system is assigned a random value within this space which represents its \u201cposition\u201d on the ring. Each data item identified by a key is assigned to a node by hashing the data item\u2019s key to yield its position on the ring, and then walking the ring clockwise to find the first node with a position larger than the item\u2019s position. Thus, each node becomes responsible for the region in the ring between it and its predecessor node on the ring. The principle advantage is that departure or arrival of a node only affects its immediate neighbors and other nodes remain unaffected. The basic consistent hashing algorithm presents some challenges. First, the random position assignment of each node on the ring leads to non-uniform data and load distribution. Second, the basic algorithm is oblivious to the heterogeneity in the performance of nodes. To address these issues, Dynamo uses a variant of consistent hashing: instead of mapping a node to a single point in the circle, each node gets assigned to multiple points in the ring. To this end, Dynamo uses the concept of \u201cvirtual nodes\u201d. A virtual node looks like a single node in the system, but each node can be responsible for more than one virtual node Effectively, when a new node is added to the system, it is assigned multiple positions (henceforth, \u201ctokens\u201d) in the ring. Using virtual nodes has the following advantages: If a node becomes unavailable (due to failures or routine maintenance), the load handled by this node is evenly dispersed across the remaining available nodes. When a node becomes available again, or a new node is added to the system, the newly available node accepts a roughly equivalent amount of load from each of the other available nodes. The number of virtual nodes that a node is responsible can decided based on its capacity, accounting for heterogeneity in the physical infrastructure.","title":"Dynamo"},{"location":"Notes/papers/Storage/dynamo/#dynamo","text":"Dynamo: Amazon\u2019s Highly Available Key-value Store A highly available key-value storage system for \"always-on\" experience.","title":"Dynamo"},{"location":"Notes/papers/Storage/dynamo/#background","text":"The Amazon platform is built on top of tens of thousands of server and network components, where there are always a small but significant number of components failing at any given time. Some applications like shopping cart need always-available storage technologies for customer experience.","title":"Background"},{"location":"Notes/papers/Storage/dynamo/#introduction","text":"Dynamo provides only simple key-value interface; no operations span multiple data items. Unlike traditional commercial systems putting importance on consistency, Dynamo sacrifices consistency under certain failure scenarios for availability. Dynamo uses a synthesis of well known techniques to achieve scalability and availability:","title":"Introduction"},{"location":"Notes/papers/Storage/dynamo/#system-architecture","text":"Consistent hashing: To scale incrementally, Dynamo uses consistent hashing to partition data. The principle advantage is that arrival/departure of a node only affects immediate neighbors. Virtual node : Dynamo introduces the concept of virtual nodes to balance the load when membership changes and account for heterogeneity in the physical infrastructure (virtual nodes on same physical node are skipped in replication). Data Versioning : Dynamo uses vector clocks(list of <node, counter> pairs) to capture the causality between different versions of the same object. If Dynamo can't resolve divergent versions, it will return all objects and let applications resolve the conflicts. Sloppy quorum and hinted handoff: Dynamo uses quorum-based consistency protocol(R+W>N), but does not ensure strict quorum membership. When a node A is temporarily unavailable, another node B will help maintain the replica and deliver the replica to A when detecting A has recovered. Replica synchronization: Dynamo anti-entropy protocol uses hash trees to reduce the amount of data need to be transferred to detect replica inconsistencies. Gossip protocol: Each node contacts a random peer every second and two nodes reconcile their persisted membership change histories and views of failure state. To prevent logical partitions, some seed nodes are known to all nodes.","title":"System Architecture"},{"location":"Notes/papers/Storage/dynamo/#consistent-hashing","text":"Dynamo\u2019s partitioning scheme relies on consistent hashing to distribute the load across multiple storage hosts. In consistent hashing, the output range of a hash function is treated as a fixed circular space or \u201cring\u201d Each node in the system is assigned a random value within this space which represents its \u201cposition\u201d on the ring. Each data item identified by a key is assigned to a node by hashing the data item\u2019s key to yield its position on the ring, and then walking the ring clockwise to find the first node with a position larger than the item\u2019s position. Thus, each node becomes responsible for the region in the ring between it and its predecessor node on the ring. The principle advantage is that departure or arrival of a node only affects its immediate neighbors and other nodes remain unaffected. The basic consistent hashing algorithm presents some challenges. First, the random position assignment of each node on the ring leads to non-uniform data and load distribution. Second, the basic algorithm is oblivious to the heterogeneity in the performance of nodes. To address these issues, Dynamo uses a variant of consistent hashing: instead of mapping a node to a single point in the circle, each node gets assigned to multiple points in the ring. To this end, Dynamo uses the concept of \u201cvirtual nodes\u201d. A virtual node looks like a single node in the system, but each node can be responsible for more than one virtual node Effectively, when a new node is added to the system, it is assigned multiple positions (henceforth, \u201ctokens\u201d) in the ring. Using virtual nodes has the following advantages: If a node becomes unavailable (due to failures or routine maintenance), the load handled by this node is evenly dispersed across the remaining available nodes. When a node becomes available again, or a new node is added to the system, the newly available node accepts a roughly equivalent amount of load from each of the other available nodes. The number of virtual nodes that a node is responsible can decided based on its capacity, accounting for heterogeneity in the physical infrastructure.","title":"Consistent Hashing"},{"location":"Notes/papers/Storage/farm/","text":"FaRM FaRM: Distributed Transactions With Consistency, Availability, and Performance Design of new transaction, replication, and recovery protocols from first principles to leverage commodity networks with RDMA and a new, inexpensive approach to providing non-volatile DRAM. MIT Notes FAQ Overview FaRM provides distributed ACID transactions with strict serializability, high availability, high throughput and low latency FaRM uses optimistic concurrency control with a four phase commit protocol (lock, validation, commit backup, and commit primary) NVRAM FaRM writes go to RAM, not disk -- eliminates a huge bottleneck RAM write takes 200 ns, hard drive write takes 10 ms, SSD write 100 us, ns = nanosecond, ms = millisecond, us = microsecond RAM is normally volatile so NV achieved by attaching batteries to power supply units and writing the contents of DRAM to SSD when the power fails. What if crash prevents s/w from writing SSD? FaRM copes with single-machine crashes by copying data from RAM of machines' replicas to other machines to ensure always f+1 copies FaRM uses two networking ideas: Kernel bypass RDMA Kernel bypass [diagram: FaRM user program, CPU cores, DMA queues, NIC] application directly interacts with NIC -- no system calls, no kernel NIC DMAs into/out of user RAM FaRM s/w polls DMA areas to check for new messages CPU operations is what limits RPC (100,000 bits/), not wire between machines (10 gb/s) RDMA [src host, NIC, switch, NIC, target memory, target CPU] remote NIC directly reads/writes memory Sender provides memory address Remote CPU is not involved! This is \"one-sided RDMA\" Reads an entire cache line, atomically Distributed transactions and replication FaRM uses fewer messages than traditional protocols, and exploits one-sided RDMA reads and writes for CPU efficiency and low latency. FaRM uses primary-backup replication in non-volatile DRAM for both data and transaction logs, and uses unreplicated transaction coordinators that communicate directly with primaries and backups. FaRM uses optimistic concurrency control with read validation. Transactions use one-sided RDMA to read objects and they buffer writes locally. The coordinator also records the addresses and versions of all objects accessed At the end of the execution, FaRM attempts to commit the transaction by executing the following steps: Lock TC writes a LOCK record to the log on each machine that is a primary for any written object, containing versions and new values of all written objects on that primary as well as the list of all regions with written objects. Primaries attempt to lock the objects at the specified versions using compare-and-swap. Locking can fail if : 1. Any object version changed since it was read by the transaction. 2. If the object is currently locked by another transaction. 3. In this case, the coordinator aborts the transaction. It writes an abort record to all primaries and returns an error to the application. Validate TC performs read validation by reading, from their primaries, the versions of all objects that were read but not written by the transaction. If any object has changed, validation fails and the transaction is aborted. Validation uses one-sided RDMA reads by default. For primaries that hold more than t_r objects, validation is done over RPC. The threshold t_r (currently 4) reflects the CPU cost of an RPC relative to an RDMA read. Commit backups TC writes a COMMITBACKUP record to the non-volatile logs at each backup and waits for an ack from the NIC hardware. Commit primaries TC writes a COMMITPRIMARY record to the logs at each primary. Primaries process these records by updating the objects in place, incrementing their versions, and unlocking them, which exposes the writes committed by the transaction. How does FaRM differ from Spanner? both replicate and use two-phase commit (2pc) for transactions Spanner: a deployed system focuses on geographic replication e.g. copies on East and West coasts, in case data centers fail is most innovative for read-only transactions -- TrueTime performance: r/w xaction takes 10 to 100 ms (Tables 3 and 6) FaRM a research prototype, to explore potential of RDMA all replicas are in same data center (wouldn't make sense otherwise) RDMA restricts design options: thus Optimistic Concurrency Control (OCC) performance: 58 microseconds for simple transactions (6.3, Figure 7) i.e. 100 times faster than Spanner performance: throughput of 100 million/second on 90 machines (Figure 7) extremely impressive, particularly for transactions+replication They target different bottlenecks: Spanner: speed of light and network delays FaRM: CPU time on servers","title":"FaRM"},{"location":"Notes/papers/Storage/farm/#farm","text":"FaRM: Distributed Transactions With Consistency, Availability, and Performance Design of new transaction, replication, and recovery protocols from first principles to leverage commodity networks with RDMA and a new, inexpensive approach to providing non-volatile DRAM. MIT Notes FAQ","title":"FaRM"},{"location":"Notes/papers/Storage/farm/#overview","text":"FaRM provides distributed ACID transactions with strict serializability, high availability, high throughput and low latency FaRM uses optimistic concurrency control with a four phase commit protocol (lock, validation, commit backup, and commit primary)","title":"Overview"},{"location":"Notes/papers/Storage/farm/#nvram","text":"FaRM writes go to RAM, not disk -- eliminates a huge bottleneck RAM write takes 200 ns, hard drive write takes 10 ms, SSD write 100 us, ns = nanosecond, ms = millisecond, us = microsecond RAM is normally volatile so NV achieved by attaching batteries to power supply units and writing the contents of DRAM to SSD when the power fails. What if crash prevents s/w from writing SSD? FaRM copes with single-machine crashes by copying data from RAM of machines' replicas to other machines to ensure always f+1 copies FaRM uses two networking ideas: Kernel bypass RDMA","title":"NVRAM"},{"location":"Notes/papers/Storage/farm/#kernel-bypass","text":"[diagram: FaRM user program, CPU cores, DMA queues, NIC] application directly interacts with NIC -- no system calls, no kernel NIC DMAs into/out of user RAM FaRM s/w polls DMA areas to check for new messages CPU operations is what limits RPC (100,000 bits/), not wire between machines (10 gb/s)","title":"Kernel bypass"},{"location":"Notes/papers/Storage/farm/#rdma","text":"[src host, NIC, switch, NIC, target memory, target CPU] remote NIC directly reads/writes memory Sender provides memory address Remote CPU is not involved! This is \"one-sided RDMA\" Reads an entire cache line, atomically","title":"RDMA"},{"location":"Notes/papers/Storage/farm/#distributed-transactions-and-replication","text":"FaRM uses fewer messages than traditional protocols, and exploits one-sided RDMA reads and writes for CPU efficiency and low latency. FaRM uses primary-backup replication in non-volatile DRAM for both data and transaction logs, and uses unreplicated transaction coordinators that communicate directly with primaries and backups. FaRM uses optimistic concurrency control with read validation. Transactions use one-sided RDMA to read objects and they buffer writes locally. The coordinator also records the addresses and versions of all objects accessed At the end of the execution, FaRM attempts to commit the transaction by executing the following steps: Lock TC writes a LOCK record to the log on each machine that is a primary for any written object, containing versions and new values of all written objects on that primary as well as the list of all regions with written objects. Primaries attempt to lock the objects at the specified versions using compare-and-swap. Locking can fail if : 1. Any object version changed since it was read by the transaction. 2. If the object is currently locked by another transaction. 3. In this case, the coordinator aborts the transaction. It writes an abort record to all primaries and returns an error to the application. Validate TC performs read validation by reading, from their primaries, the versions of all objects that were read but not written by the transaction. If any object has changed, validation fails and the transaction is aborted. Validation uses one-sided RDMA reads by default. For primaries that hold more than t_r objects, validation is done over RPC. The threshold t_r (currently 4) reflects the CPU cost of an RPC relative to an RDMA read. Commit backups TC writes a COMMITBACKUP record to the non-volatile logs at each backup and waits for an ack from the NIC hardware. Commit primaries TC writes a COMMITPRIMARY record to the logs at each primary. Primaries process these records by updating the objects in place, incrementing their versions, and unlocking them, which exposes the writes committed by the transaction.","title":"Distributed transactions and replication"},{"location":"Notes/papers/Storage/farm/#how-does-farm-differ-from-spanner","text":"both replicate and use two-phase commit (2pc) for transactions Spanner: a deployed system focuses on geographic replication e.g. copies on East and West coasts, in case data centers fail is most innovative for read-only transactions -- TrueTime performance: r/w xaction takes 10 to 100 ms (Tables 3 and 6) FaRM a research prototype, to explore potential of RDMA all replicas are in same data center (wouldn't make sense otherwise) RDMA restricts design options: thus Optimistic Concurrency Control (OCC) performance: 58 microseconds for simple transactions (6.3, Figure 7) i.e. 100 times faster than Spanner performance: throughput of 100 million/second on 90 machines (Figure 7) extremely impressive, particularly for transactions+replication They target different bottlenecks: Spanner: speed of light and network delays FaRM: CPU time on servers","title":"How does FaRM differ from Spanner?"},{"location":"Notes/papers/Storage/frangipani/","text":"Frangipani Frangipani: A Scalable Distributed File System i 1990s scalable distributed file system that manages a collection of disks on multiple machines as a single shared pool of storage. MIT Notes FAQ Background What to digest: strong consistency cache coherence distributed transactions distributed crash recovery Overall design: A decentralized file system , cache for performance Petal: block storage service; what is stored: just like an ordinary hard disk file system directores i-node file content blocks free bitmaps Scenario 1: WS2(work station) run ls / or cat /grades while WS1 is modifying the same inode Scenario 2: WS1 and WS2 concurrently try to create /a , /b under same directory Scenario 3: WS1 crashes while creating a file operation: allocate i-node, initialize i-node, update directory Components Petal : block storage service; replicated Lock Server (LS) , with one lock per file/directory , the locks are named by files/directories (really i-numbers) 1 2 3 4 file owner ----------- x WS1 y WS1 Workstation (WS) cache(store the lock status): 1 2 3 4 file/dir lock content ----------------------- x busy ... //using data right now y idle ... //holds lock but not using the cached data right now Solution for Scenario 1: Cache Coherence (revealing writes) It is to guarantee linearizability AND caching Example: WS1 changes file z , then WS2 reads z WS1 changes file z 1 2 3 4 5 6 request lock (WS1 -> LS) LS put: owner(z) = WS1 grant lock (LS -> WS1) (WS1: read+cache z data from Petal modify z locally when done, cached lock in state) WS2 try read file z Revoke Lock: Coherence Protocol Msg 1 2 3 4 5 6 request lock (WS2 -> LS) grant (LS -> WS2) revoke (LS -> WS1) (write update log of metadata to Petal) (write modified z to Petal) release lock (WS1 -> LS) WS2 get the lock, and read z from Petal notes: locks and rules force reads to see last write one optimization: Frangipani has shared read locks, as well as exclusive write locks Solution for Scenario 2: Atomic Transactions (concealing writes) There is two operation for create/rename/delete a file, 1. create/rename initializes i-node, adds to directory entry. 2. rename. The challenge is to guarantee the atomicity. Transactional file-system operations: operation corresponds to a system call (create file, remove file, rename, &c) WS acquires locks on all file system data that it will modify performs operation with all locks held only releases when finished no other WS can see partially-completed operations Solution for Scenario 3: Crash Recovery (write-ahead logging) What if a Frangipani workstation dies while holding locks? eg: dead WS had modified data in its cache eg: dead WS had started to write back modified data to Petal Solution: Before writing any of op's cached blocks to Petal, first write log to Petal Log entry stored in Petal for recovery Before writing any of op's cached blocks to Petal, first write log to Petal if a crashed workstation has done some Petal writes for an operation, not not all. the writes can be completed from the log in Petal What an log entry include: note : it is just for file metadata, not for file data log sequence number array of updates: block #, new version #, addr, new bytes When WS receive lock revoke: 1 2 3 Play log in the Petal (Petal already store it) (P) WS send the cached updated blocks to Petal (WS1-P) Release the lock (WS1-LS) Why version number is necessary: for linearizability of the metadata update 1 2 3 4 WS1: delete(d/f)(v1) crash WS2: create(d/f)(v2) WS3: recover WS1 WS3 is recovering WS1's log -- but it doesn't look at WS2's log When WS1 delete the file, it add a log entry with v1 when WS2 create a file with same name, the system will give it a version number v2 so when replay the log of ws1, the v1<v2. so ignore it.","title":"Frangipani"},{"location":"Notes/papers/Storage/frangipani/#frangipani","text":"Frangipani: A Scalable Distributed File System i 1990s scalable distributed file system that manages a collection of disks on multiple machines as a single shared pool of storage. MIT Notes FAQ","title":"Frangipani"},{"location":"Notes/papers/Storage/frangipani/#background","text":"What to digest: strong consistency cache coherence distributed transactions distributed crash recovery Overall design: A decentralized file system , cache for performance Petal: block storage service; what is stored: just like an ordinary hard disk file system directores i-node file content blocks free bitmaps Scenario 1: WS2(work station) run ls / or cat /grades while WS1 is modifying the same inode Scenario 2: WS1 and WS2 concurrently try to create /a , /b under same directory Scenario 3: WS1 crashes while creating a file operation: allocate i-node, initialize i-node, update directory","title":"Background"},{"location":"Notes/papers/Storage/frangipani/#components","text":"Petal : block storage service; replicated Lock Server (LS) , with one lock per file/directory , the locks are named by files/directories (really i-numbers) 1 2 3 4 file owner ----------- x WS1 y WS1 Workstation (WS) cache(store the lock status): 1 2 3 4 file/dir lock content ----------------------- x busy ... //using data right now y idle ... //holds lock but not using the cached data right now","title":"Components"},{"location":"Notes/papers/Storage/frangipani/#solution-for-scenario-1-cache-coherence-revealing-writes","text":"It is to guarantee linearizability AND caching Example: WS1 changes file z , then WS2 reads z WS1 changes file z 1 2 3 4 5 6 request lock (WS1 -> LS) LS put: owner(z) = WS1 grant lock (LS -> WS1) (WS1: read+cache z data from Petal modify z locally when done, cached lock in state) WS2 try read file z Revoke Lock: Coherence Protocol Msg 1 2 3 4 5 6 request lock (WS2 -> LS) grant (LS -> WS2) revoke (LS -> WS1) (write update log of metadata to Petal) (write modified z to Petal) release lock (WS1 -> LS) WS2 get the lock, and read z from Petal notes: locks and rules force reads to see last write one optimization: Frangipani has shared read locks, as well as exclusive write locks","title":"Solution for Scenario 1: Cache Coherence (revealing writes)"},{"location":"Notes/papers/Storage/frangipani/#solution-for-scenario-2-atomic-transactions-concealing-writes","text":"There is two operation for create/rename/delete a file, 1. create/rename initializes i-node, adds to directory entry. 2. rename. The challenge is to guarantee the atomicity. Transactional file-system operations: operation corresponds to a system call (create file, remove file, rename, &c) WS acquires locks on all file system data that it will modify performs operation with all locks held only releases when finished no other WS can see partially-completed operations","title":"Solution for Scenario 2: Atomic Transactions (concealing writes)"},{"location":"Notes/papers/Storage/frangipani/#solution-for-scenario-3-crash-recovery-write-ahead-logging","text":"What if a Frangipani workstation dies while holding locks? eg: dead WS had modified data in its cache eg: dead WS had started to write back modified data to Petal Solution: Before writing any of op's cached blocks to Petal, first write log to Petal Log entry stored in Petal for recovery Before writing any of op's cached blocks to Petal, first write log to Petal if a crashed workstation has done some Petal writes for an operation, not not all. the writes can be completed from the log in Petal What an log entry include: note : it is just for file metadata, not for file data log sequence number array of updates: block #, new version #, addr, new bytes When WS receive lock revoke: 1 2 3 Play log in the Petal (Petal already store it) (P) WS send the cached updated blocks to Petal (WS1-P) Release the lock (WS1-LS) Why version number is necessary: for linearizability of the metadata update 1 2 3 4 WS1: delete(d/f)(v1) crash WS2: create(d/f)(v2) WS3: recover WS1 WS3 is recovering WS1's log -- but it doesn't look at WS2's log When WS1 delete the file, it add a log entry with v1 when WS2 create a file with same name, the system will give it a version number v2 so when replay the log of ws1, the v1<v2. so ignore it.","title":"Solution for Scenario 3: Crash Recovery (write-ahead logging)"},{"location":"Notes/papers/Storage/gfs/","text":"GFS The Google File System A scalable distributed file system for large distributed data-intensive applications. Introduction The design is driven by key observations different from earlier file system assumptions: frequent component failures, huge files (GB+), most files are mutated by appending rather than overwriting. GFS provides a POSIX-like file system interface. It supports snapshot and record append operations(useful for multi-way merge and consumer/producer). Architecture Figure 1 A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients. Files are divided into fixed-sized chunks(64 MB, larger than typical) and each chunk has an immutable and globally unique id. Chunkservers store chunks as local Linux files and the master maintains all file system metadata. The master stores in memory three majors types of metadata: the file and chunk namespaces, the mapping from files to chunks and the locations of each chunk's replicas. The first two are also persisted in a replicated operation log(with checkpoints); the last one is polled from chunkservers at start time and kept up-to-date via heartbeat messages. Reads Using the fixed chunk size, the client translates the file name and byte offset into a chunk index within the file. Sends the master a request containing the file name and chunk index. The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the file name and chunk index as the key. The client then sends a request to one of the replicas,most likely the closest one. The request specifies the chunk handle and a byte range within that chunk. Further reads of the same chunk require no more client-master interaction until the cached information expires. Chunk Size Key design parameters, Large chunk chosen : 64MB Advantages Reduces clients' need to interact with the master Operations are more likely to target at the same chunk, can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time Reduces metadata size, easier for master to store in memory Disadvantages Small files -> small # of chunks -> the chunkserver becomes a hot spot Operation Log Contains a historical record of critical metadata changes. Replicated on multiple remote machines. Respond to a client operation only after flushing the corresponding log record to disk both locally and remotely. Recovers file system by replaying the log. Master checkpoints its state in a compact B-tree like form whenever the log grows beyond a certain size. Only keeps latest complete checkpoint and subsequent log files. Figure 2 System Interactions Leases and Mutation Order For mutations to a chunk, the master grants an extendable chunk lease to one of the replicas. The primary will pick a serial order for all mutations to the chunk and all replicas will follow this order. Control and data flows are decoupled to use the network efficiently. Control flows from the client to the primary and then to all secodaries. Data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. GFS provides at-least-once semantics. A client retries when mutation fails, so replicas of the same chunk may contain different data including duplicates or paddings(not bytewise identical). Writes Client asks the master which chunkserver holds the current lease for the chunk and replica locations. Master replies with primary and secondaries locations. Caches this until primary unavailable/no longer has lease. Client pushes the data to all the replicas. Each chunkserver will store the data in an internal LRU buffer cache. After all the replicas ack, client sends a write request to the primary. Primary assigns write order to all mutations it receives. Primary forwards the write request to all secondary replicas with mutation order. Secondaries ack to primary on completion. Primary replies to client. Failure -> client retry. Master Operations GFS namespace is represented as a lookup table(with prefix compression) from full pathnames to metadata. Each file or directory has a read-write lock to ensure proper searialization. The master create chunk replicas for initial creation, re-replication and rebalancing. It considers various factors when placing the replica, to maximizing data reliability/availability/network bandwidth utilization. When a file is deleted, the master logs the deletion immediately but deletes metadata after some delay(3 days, etc). Each chunkserver reports a subset of chunks in heartbeat messages, and the master replies whether these chunks can be deleted. This lazy approach is simple and reliable in a large-scale distributed system. The master maintains a version number for each chunk. Whenever the master grants a new lease on the chunk, it increases the version number and informs all the up-to-date replicas. For high availability, we have (1) monitoring infrastructure outside GFS to start a master and (2) \"shadow\" masters for read-only operations. Fault Tolerance High Availability Fast Recovery - Both the master and the chunkserver are designed to restore their state and start in seconds. Chunk Replication - default 3 replications per chunk. Master Replication - Operation log and checkpoints replicated on multiple machines. \"Shadow\" masters above. Data Integrity Breaks each 64 MB chunk into blocks of 64 KB, each with its own 32-bit checksum stored in memory and written to the log. For reads, the chunkserver verifies the checksum of datablocks that overlap the read range before returning any data to the requester If a block does not match the recorded checksum, the chunkserver returns an error and and reports the mismatch to the master. In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica. Summary Good Ideas High Availability through fast recovery of master and chunkserver. Separation of naming (master) from storage (chunkserver). Sharding (chunk replication) for parallel throughput. Primary to sequence writes. Leases to prevent split-brain chunkserver primaries. Not So Good Single master performance : Ran out of RAM and CPU as file count increased 1000x Chunkservers not very efficient for small files Lack of automatic fail-over to master replica","title":"GFS"},{"location":"Notes/papers/Storage/gfs/#gfs","text":"The Google File System A scalable distributed file system for large distributed data-intensive applications.","title":"GFS"},{"location":"Notes/papers/Storage/gfs/#introduction","text":"The design is driven by key observations different from earlier file system assumptions: frequent component failures, huge files (GB+), most files are mutated by appending rather than overwriting. GFS provides a POSIX-like file system interface. It supports snapshot and record append operations(useful for multi-way merge and consumer/producer).","title":"Introduction"},{"location":"Notes/papers/Storage/gfs/#architecture","text":"Figure 1 A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients. Files are divided into fixed-sized chunks(64 MB, larger than typical) and each chunk has an immutable and globally unique id. Chunkservers store chunks as local Linux files and the master maintains all file system metadata. The master stores in memory three majors types of metadata: the file and chunk namespaces, the mapping from files to chunks and the locations of each chunk's replicas. The first two are also persisted in a replicated operation log(with checkpoints); the last one is polled from chunkservers at start time and kept up-to-date via heartbeat messages. Reads Using the fixed chunk size, the client translates the file name and byte offset into a chunk index within the file. Sends the master a request containing the file name and chunk index. The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the file name and chunk index as the key. The client then sends a request to one of the replicas,most likely the closest one. The request specifies the chunk handle and a byte range within that chunk. Further reads of the same chunk require no more client-master interaction until the cached information expires.","title":"Architecture"},{"location":"Notes/papers/Storage/gfs/#chunk-size","text":"Key design parameters, Large chunk chosen : 64MB Advantages Reduces clients' need to interact with the master Operations are more likely to target at the same chunk, can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time Reduces metadata size, easier for master to store in memory Disadvantages Small files -> small # of chunks -> the chunkserver becomes a hot spot","title":"Chunk Size"},{"location":"Notes/papers/Storage/gfs/#operation-log","text":"Contains a historical record of critical metadata changes. Replicated on multiple remote machines. Respond to a client operation only after flushing the corresponding log record to disk both locally and remotely. Recovers file system by replaying the log. Master checkpoints its state in a compact B-tree like form whenever the log grows beyond a certain size. Only keeps latest complete checkpoint and subsequent log files. Figure 2","title":"Operation Log"},{"location":"Notes/papers/Storage/gfs/#system-interactions","text":"","title":"System Interactions"},{"location":"Notes/papers/Storage/gfs/#leases-and-mutation-order","text":"For mutations to a chunk, the master grants an extendable chunk lease to one of the replicas. The primary will pick a serial order for all mutations to the chunk and all replicas will follow this order. Control and data flows are decoupled to use the network efficiently. Control flows from the client to the primary and then to all secodaries. Data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. GFS provides at-least-once semantics. A client retries when mutation fails, so replicas of the same chunk may contain different data including duplicates or paddings(not bytewise identical). Writes Client asks the master which chunkserver holds the current lease for the chunk and replica locations. Master replies with primary and secondaries locations. Caches this until primary unavailable/no longer has lease. Client pushes the data to all the replicas. Each chunkserver will store the data in an internal LRU buffer cache. After all the replicas ack, client sends a write request to the primary. Primary assigns write order to all mutations it receives. Primary forwards the write request to all secondary replicas with mutation order. Secondaries ack to primary on completion. Primary replies to client. Failure -> client retry.","title":"Leases and Mutation Order"},{"location":"Notes/papers/Storage/gfs/#master-operations","text":"GFS namespace is represented as a lookup table(with prefix compression) from full pathnames to metadata. Each file or directory has a read-write lock to ensure proper searialization. The master create chunk replicas for initial creation, re-replication and rebalancing. It considers various factors when placing the replica, to maximizing data reliability/availability/network bandwidth utilization. When a file is deleted, the master logs the deletion immediately but deletes metadata after some delay(3 days, etc). Each chunkserver reports a subset of chunks in heartbeat messages, and the master replies whether these chunks can be deleted. This lazy approach is simple and reliable in a large-scale distributed system. The master maintains a version number for each chunk. Whenever the master grants a new lease on the chunk, it increases the version number and informs all the up-to-date replicas. For high availability, we have (1) monitoring infrastructure outside GFS to start a master and (2) \"shadow\" masters for read-only operations.","title":"Master Operations"},{"location":"Notes/papers/Storage/gfs/#fault-tolerance","text":"High Availability Fast Recovery - Both the master and the chunkserver are designed to restore their state and start in seconds. Chunk Replication - default 3 replications per chunk. Master Replication - Operation log and checkpoints replicated on multiple machines. \"Shadow\" masters above. Data Integrity Breaks each 64 MB chunk into blocks of 64 KB, each with its own 32-bit checksum stored in memory and written to the log. For reads, the chunkserver verifies the checksum of datablocks that overlap the read range before returning any data to the requester If a block does not match the recorded checksum, the chunkserver returns an error and and reports the mismatch to the master. In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica.","title":"Fault Tolerance"},{"location":"Notes/papers/Storage/gfs/#summary","text":"Good Ideas High Availability through fast recovery of master and chunkserver. Separation of naming (master) from storage (chunkserver). Sharding (chunk replication) for parallel throughput. Primary to sequence writes. Leases to prevent split-brain chunkserver primaries. Not So Good Single master performance : Ran out of RAM and CPU as file count increased 1000x Chunkservers not very efficient for small files Lack of automatic fail-over to master replica","title":"Summary"},{"location":"Notes/papers/Storage/memcache/","text":"Memcache Scaling Memcache at Facebook How Facebook scaled a distributed key-value store that supports the world\u2019s largest social network History of Application Scaling Typical story of evolution over time: Single machine w/ web server + application + DB DB provides persistent storage, crash recovery, transactions, SQL application queries DB, formats HTML, &c but: as load grows, application takes too much CPU time Many web FEs, one shared DB an easy change, since web server + app already separate from storage FEs are stateless, all sharing (and concurrency control) via DB stateless -> any FE can serve any request, no harm from FE crash but: as load grows, need more FEs, soon single DB server is bottleneck Many web FEs, data sharded over cluster of DBs partition data by key over the DBs; app looks at key (e.g. user), chooses the right DB good DB parallelism if no data is super-popular painful -- cross-shard transactions and queries probably don't work hard to partition too finely but: DBs are slow, even for reads, why not cache read requests? Many web FEs, many caches for reads, many DBs for writes cost-effective b/c read-heavy and memcached 10x faster than a DB; memcached just an in-memory hash table, very simple complex b/c DB and memcacheds can get out of sync fragile b/c cache misses can easily overload the DB (next bottleneck will be DB writes -- hard to solve) Will partition or replication yield most mc throughput? partition: divide keys over mc servers replicate: divide clients over mc servers partition: (+) more memory-efficient (one copy of each k/v) (+) works well if no key is very popular (-) each web server must talk to many mc servers (overhead) replication: (+) good if a few keys are very popular (+) fewer TCP connections (-) less total data can be cached Overview FB uses mc as a \"look-aside\" cache application determines relationship of mc to DB mc doesn't know anything about DB important to read your own writes Q: what is the point of regions -- multiple complete replicas? lower RTT to users (east coast, west coast) quick local reads, from local mc and DB (though writes are expensive: must be sent to primary) maybe hot replica for main site failure? Q: why not partition users over regions? i.e. why not east-coast users' data in east-coast region, &c then no need to replicate: might cut hardware costs in half! but: social net -> not much locality; might work well for e.g. e-mail Q: why OK performance despite all writes forced to go to the primary region? writes are much rarer than reads perhaps 100ms to send write to primary, not so bad for human users users do not wait for all effects of writes to finish; i.e. for all stale cached values to be deleted In a Cluster: Latency and Load Reducing Latency Items are distributed in mc through consistent hashing. Thus web servers have to communicate with many mc servers. N^2 (each web to each mc) (all to all pattern) Important practical networking problems: N^2 TCP connections is too much state; thus UDP for client get()s UDP is not reliable or orderedl; thus TCP for client set()s and mcrouter to reduce n in n^2 single request per packet is not efficient (for TCP or UDP) per-packet overhead (interrupt &c) is too high thus mcrouter batches many requests into each packet Reducing Load Leases to address two problems: stale sets and thundering herds MC gives a lease bound to a specific k to a client to set data back into the cache when the client experiences a cache miss Client provides the lease token when setting k in mc MC invalidates leases if it receives a delete request Thundering herds: one client updates DB and delete()s a key lots of clients get() but miss they all fetch from DB they all set() not good: needless DB load mc gives just the first missing client a \"lease\" lease = permission to refresh from DB mc tells others \"try get() again in a few milliseconds\" effect: only one client reads the DB and does set() others re-try get() later and hopefully hit In a Region: Replication Why multiple clusters per region? why not add more and more mc servers to a single cluster? (1) adding mc servers to cluster doesn't help single popular keys replicating (one copy per cluster) does help (2) more mcs in cluster -> each client req talks to more servers and more in-cast congestion at requesting web servers client requests fetch 20 to 500 keys! over many mc servers MUST request them in parallel (otherwise total latency too large) so all replies come back at the same time network switches, NIC run out of buffers (3) hard to build network for single big cluster uniform client/server access so cross-section b/w must be large -- expensive two clusters -> 1/2 the cross-section b/w But replicating is a waste of RAM for less-popular items! \"regional pool\" shared by all clusters unpopular objects (no need for many copies) New Clusters Bringing up new mc cluster is a performance problem as new cluster has 0% hit rate. Clients that use it will generate a big spike in DB load. If originally 1% miss rate, adding new cluster will cause misses for 50% -> 50x spike in DB load. thus the clients of new cluster first get() from existing cluster and set() into new cluster basically lazy copy of existing cluster to new cluster Across Regions: Consistency Writes go direct to primary DB, with transactions, so writes are consistent Reads eventually consistent but Read-your-own-writes How do they keep mc content consistent w/ DB content? DBs send invalidates (delete()s) to all mc servers that might cache; this is McSqueal in Figure 6 writing client also invalidates mc in local cluster for read-your-own-writes Writes from a master region To avoid a race condition in which an invalidation arrives to a replica mc before the data has been replicated from the master region, mcsqueal implemented so that each DB will notify mcs in its region to invalidate instead of master sending invalidates to all regions Writes from a non-master region Consider a user who updates his data from a non-master region when replication lag is excessively large. When this user tries to fetch data, it hits regional replica DB and may see stale data Employs a remote marker mechanism to minimalize stale data reads. When a web server wishes to update data that affects a key k, that server (1) sets a remote marker r_k in the region (2) performs the write to the master embedding k and r_k to be invalidated in the SQL statement (3) deletes k in the local cluster. On a subsequent request for k, a web server will be unable to find the cached data, check whether r_k exists, and direct its query to the master or local region depending on the presence of r_k .","title":"Memcache"},{"location":"Notes/papers/Storage/memcache/#memcache","text":"Scaling Memcache at Facebook How Facebook scaled a distributed key-value store that supports the world\u2019s largest social network","title":"Memcache"},{"location":"Notes/papers/Storage/memcache/#history-of-application-scaling","text":"Typical story of evolution over time: Single machine w/ web server + application + DB DB provides persistent storage, crash recovery, transactions, SQL application queries DB, formats HTML, &c but: as load grows, application takes too much CPU time Many web FEs, one shared DB an easy change, since web server + app already separate from storage FEs are stateless, all sharing (and concurrency control) via DB stateless -> any FE can serve any request, no harm from FE crash but: as load grows, need more FEs, soon single DB server is bottleneck Many web FEs, data sharded over cluster of DBs partition data by key over the DBs; app looks at key (e.g. user), chooses the right DB good DB parallelism if no data is super-popular painful -- cross-shard transactions and queries probably don't work hard to partition too finely but: DBs are slow, even for reads, why not cache read requests? Many web FEs, many caches for reads, many DBs for writes cost-effective b/c read-heavy and memcached 10x faster than a DB; memcached just an in-memory hash table, very simple complex b/c DB and memcacheds can get out of sync fragile b/c cache misses can easily overload the DB (next bottleneck will be DB writes -- hard to solve) Will partition or replication yield most mc throughput? partition: divide keys over mc servers replicate: divide clients over mc servers partition: (+) more memory-efficient (one copy of each k/v) (+) works well if no key is very popular (-) each web server must talk to many mc servers (overhead) replication: (+) good if a few keys are very popular (+) fewer TCP connections (-) less total data can be cached","title":"History of Application Scaling"},{"location":"Notes/papers/Storage/memcache/#overview","text":"FB uses mc as a \"look-aside\" cache application determines relationship of mc to DB mc doesn't know anything about DB important to read your own writes Q: what is the point of regions -- multiple complete replicas? lower RTT to users (east coast, west coast) quick local reads, from local mc and DB (though writes are expensive: must be sent to primary) maybe hot replica for main site failure? Q: why not partition users over regions? i.e. why not east-coast users' data in east-coast region, &c then no need to replicate: might cut hardware costs in half! but: social net -> not much locality; might work well for e.g. e-mail Q: why OK performance despite all writes forced to go to the primary region? writes are much rarer than reads perhaps 100ms to send write to primary, not so bad for human users users do not wait for all effects of writes to finish; i.e. for all stale cached values to be deleted","title":"Overview"},{"location":"Notes/papers/Storage/memcache/#in-a-cluster-latency-and-load","text":"","title":"In a Cluster: Latency and Load"},{"location":"Notes/papers/Storage/memcache/#reducing-latency","text":"Items are distributed in mc through consistent hashing. Thus web servers have to communicate with many mc servers. N^2 (each web to each mc) (all to all pattern) Important practical networking problems: N^2 TCP connections is too much state; thus UDP for client get()s UDP is not reliable or orderedl; thus TCP for client set()s and mcrouter to reduce n in n^2 single request per packet is not efficient (for TCP or UDP) per-packet overhead (interrupt &c) is too high thus mcrouter batches many requests into each packet","title":"Reducing Latency"},{"location":"Notes/papers/Storage/memcache/#reducing-load","text":"Leases to address two problems: stale sets and thundering herds MC gives a lease bound to a specific k to a client to set data back into the cache when the client experiences a cache miss Client provides the lease token when setting k in mc MC invalidates leases if it receives a delete request Thundering herds: one client updates DB and delete()s a key lots of clients get() but miss they all fetch from DB they all set() not good: needless DB load mc gives just the first missing client a \"lease\" lease = permission to refresh from DB mc tells others \"try get() again in a few milliseconds\" effect: only one client reads the DB and does set() others re-try get() later and hopefully hit","title":"Reducing Load"},{"location":"Notes/papers/Storage/memcache/#in-a-region-replication","text":"Why multiple clusters per region? why not add more and more mc servers to a single cluster? (1) adding mc servers to cluster doesn't help single popular keys replicating (one copy per cluster) does help (2) more mcs in cluster -> each client req talks to more servers and more in-cast congestion at requesting web servers client requests fetch 20 to 500 keys! over many mc servers MUST request them in parallel (otherwise total latency too large) so all replies come back at the same time network switches, NIC run out of buffers (3) hard to build network for single big cluster uniform client/server access so cross-section b/w must be large -- expensive two clusters -> 1/2 the cross-section b/w But replicating is a waste of RAM for less-popular items! \"regional pool\" shared by all clusters unpopular objects (no need for many copies)","title":"In a Region: Replication"},{"location":"Notes/papers/Storage/memcache/#new-clusters","text":"Bringing up new mc cluster is a performance problem as new cluster has 0% hit rate. Clients that use it will generate a big spike in DB load. If originally 1% miss rate, adding new cluster will cause misses for 50% -> 50x spike in DB load. thus the clients of new cluster first get() from existing cluster and set() into new cluster basically lazy copy of existing cluster to new cluster","title":"New Clusters"},{"location":"Notes/papers/Storage/memcache/#across-regions-consistency","text":"Writes go direct to primary DB, with transactions, so writes are consistent Reads eventually consistent but Read-your-own-writes How do they keep mc content consistent w/ DB content? DBs send invalidates (delete()s) to all mc servers that might cache; this is McSqueal in Figure 6 writing client also invalidates mc in local cluster for read-your-own-writes Writes from a master region To avoid a race condition in which an invalidation arrives to a replica mc before the data has been replicated from the master region, mcsqueal implemented so that each DB will notify mcs in its region to invalidate instead of master sending invalidates to all regions Writes from a non-master region Consider a user who updates his data from a non-master region when replication lag is excessively large. When this user tries to fetch data, it hits regional replica DB and may see stale data Employs a remote marker mechanism to minimalize stale data reads. When a web server wishes to update data that affects a key k, that server (1) sets a remote marker r_k in the region (2) performs the write to the master embedding k and r_k to be invalidated in the SQL statement (3) deletes k in the local cluster. On a subsequent request for k, a web server will be unable to find the cached data, check whether r_k exists, and direct its query to the master or local region depending on the presence of r_k .","title":"Across Regions: Consistency"},{"location":"Notes/papers/Storage/spanner/","text":"Spanner Spanner: Google\u2019s Globally-Distributed Database Google\u2019s scalable, multi-version, globally-distributed and synchronously-replicated database. MIT Notes FAQ Introduction Bigtable is hard to use for applications need complex, evolving schemas or strong consistency in wide-area replication, so Spanner evolved into a temporal, multi-version, semi-relational database, providing distributed transactions over geographically distributed data. With a new TrueTime API that exposes clock uncertainty, Spanner assigns globally-meaningful commit timestamp to even distributed transactions. Spanner shards data across many sets of Paxos state machines in data centers spread all over the world. Workload is dominated by read-only transactions (Table 6). Strong consistency (External consistency / linearizability / serializability) Basic Organization 1 2 3 4 5 6 7 8 9 10 11 12 Datacenter A: \"clients\" are web servers e.g. for gmail data is sharded over multiple servers: a-m n-z Datacenter B: has its own local clients and its own copy of the data shards a-m n-z Datacenter C: same setup Replication Replication managed by Paxos; one Paxos group per shard. Replicas are in different data centers. The benefit of shard across data center Sharding allows huge total throughput via parallelism. Datacenters fail independently -- different cities. Clients can read local replica -- fast! Can place replicas near relevant customers. Paxos requires only a majority -- tolerate slow/distant replicas. Challenges Read of local replica must yield fresh data. So it have the same overhead as Write request, vote via majority, It eliminate the benefit of locality. A transaction may involve multiple shards -> multiple Paxos groups. Transactions that read multiple records must be serializable. It means it need the assistance from Xaction Coordinator. System Architecture A Spanner zone has one zonemaster and many spanservers. The former assigns data to spanservers; the latter serve data to clients. A spanserver is responsible for 100~1000 tablets. Each tablet contains a bag of mappings (key:string, timestamp:int64)\\rightarrow string from multiple partitions of the row space(data colocation). A tablet's state is stored in a set of B-tree like files and a write-ahead log on Colossus. A spanserver implements a Paxos state machine on each tablet; each Paxos state machine stores metadata and log in corresponding tablet. At every replica that is a leader, each spanserver implements a lock table and a transaction table to support distributed transactions(two-phase commit on mutliple Paxos groups). TrueTime Main method TT.now() returns an interval [earliest, latest] that is guaranteed to contain the absolute time when TT.now() is invoked. TT.after(t) and TT.before(t) are convenience wrappers. TrueTime uses two forms of time references, GPS and atomic clocks, as they have different failure modes. TrueTime is implemented by a set of time master machines per data center and a time slave daemon per machine. Each daemon polls a variety of masters, apply Marzullo's algorithm to detect liars and synchronize the local machine clock to non-liars. Between synchronizations, a daemon advertises a slowly increasing time uncertainty \\epsilon (average 4ms in production) derived from worst-case local clock drift, time master uncertainty and communication delay to the time masters. Concurrency Control Spanner supports read-write transactions, read-only transactions and snapshot reads. Read-only transactions and snapshot reads execute at a specific timestamp without locking. Spanner maintains following invariants: disjointness: * within a Paxos group, each Paxos leader's lease interval is disjoint from every other leader's. monotonicity: within a Paxos group, Paxos writes have monotonically increasing timestamp. external consistency: if the start of a transaction T_2 occurs after the commit of a transaction T_1 , then the commit timestamp of T_2 must be greater than the commit timestamp of T_1 . For RW transactions, the coordinator assigns a commit timestamp S_i no later than TT.now().latest computed after receiving the commit request, and waits until TT.after(S_i) is true to apply the commit(\"commit wait\"). Sets the commit time to TT.now().latest since TT.now() returns a range time is between, exclusive so latest is guaranteed to not have occured. Keeps calling TT.now() until TT.now().earliest() is greater than above commit time. For RO transactions and snapshot reads, every replica tracks t_{safe} for the maximum at which the replica is up-to-date, which depends on the Paxos state machine and if there are prepared but not committed transactions. R/W Transactions 1 2 3 4 BEGIN x = x + 1 y = y - 1 END We don't want any read or write of x or y sneaking between our two ops. After commit, all reads should see our updates. Two-phase commit (2pc) with Paxos-replicated participants 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Client picks a unique transaction id (TID). Client sends each read to Paxos leader of relevant shard (2.1). Each shard first acquires a lock on the relevant record. May have to wait. Separate lock table per shard, in shard leader. Read locks are not replicated via Paxos, so leader failure -> abort. Client keeps writes private until commit. When client commits (4.2.1): Chooses a Paxos group to act as 2pc Transaction Coordinator (TC). Sends writes to relevant shard leaders. Each written shard leader: Acquires lock(s) on the written record(s). Log a \"prepare\" record via Paxos, to replicate lock and new value. Tell TC it is prepared. Or tell TC \"no\" if crashed and thus lost lock table. Transaction Coordinator: Decides commit or abort. Logs the decision to its group via Paxos. Tell participant leaders and client the result. Each participant leader: Log the TC's decision via Paxos. Release the transaction's locks. Locking (two-phase locking) ensures serializability. 2pc widely hated b/c it blocks with locks held if TC fails. Replicating the TC with Paxos solves this problem! R/O Transactions Eliminates two big costs from R/O transactions Read from local replicas, to avoid Paxos and cross-datacenter msgs. But note local replica may not be up to date! No locks, no two-phase commit, no transaction manager. Again to avoid cross-data center msg to Paxos leader. And to avoid slowing down r/w transactions. Tables 3 and 6 show a 10x latency improvement as a result! Achieves this through Snapshot Isolation Synchronize all computers' clocks (to real wall-clock time). Assign every transaction a time-stamp. r/w: commit time. r/o: start time. Execute as if one-at-a-time in time-stamp order. Even if actual reads occur in different order. Each replica stores multiple time-stamped versions of each record. All of a r/w transactions's writes get the same time-stamp. An r/o transaction's reads see version as of xaction's time-stamp. The record version with the highest time-stamp less than the xaction's. Obviously we can't synchronize all computer clocks, so TrueTime is used to give bounds of certainty. Perspective Snapshot Isolation gives you serializable r/o transactions. Timestamps set an order. Snapshot versions (and safe time) implement consistent reads at a timestamp. Xaction sees all writes from lower-TS xactions, none from higher. Any number will do for TS if you don't care about external consistency. Synchronized timestamps yield external consistency . Even among transactions at different data centers. Even though reading from local replicas that might lag. Why is all this useful? Fast r/o transactions: Read from replica in client's datacenter. No locking, no two-phase commit. Thus the 10x latency improvement in Tables 3 and 6. Although: r/o transaction reads may block due to safe time, to catch up. r/w transaction commits may block in Commit Wait. Accurate (small interval) time minimizes these delays.","title":"Spanner"},{"location":"Notes/papers/Storage/spanner/#spanner","text":"Spanner: Google\u2019s Globally-Distributed Database Google\u2019s scalable, multi-version, globally-distributed and synchronously-replicated database. MIT Notes FAQ","title":"Spanner"},{"location":"Notes/papers/Storage/spanner/#introduction","text":"Bigtable is hard to use for applications need complex, evolving schemas or strong consistency in wide-area replication, so Spanner evolved into a temporal, multi-version, semi-relational database, providing distributed transactions over geographically distributed data. With a new TrueTime API that exposes clock uncertainty, Spanner assigns globally-meaningful commit timestamp to even distributed transactions. Spanner shards data across many sets of Paxos state machines in data centers spread all over the world. Workload is dominated by read-only transactions (Table 6). Strong consistency (External consistency / linearizability / serializability)","title":"Introduction"},{"location":"Notes/papers/Storage/spanner/#basic-organization","text":"1 2 3 4 5 6 7 8 9 10 11 12 Datacenter A: \"clients\" are web servers e.g. for gmail data is sharded over multiple servers: a-m n-z Datacenter B: has its own local clients and its own copy of the data shards a-m n-z Datacenter C: same setup","title":"Basic Organization"},{"location":"Notes/papers/Storage/spanner/#replication","text":"Replication managed by Paxos; one Paxos group per shard. Replicas are in different data centers.","title":"Replication"},{"location":"Notes/papers/Storage/spanner/#the-benefit-of-shard-across-data-center","text":"Sharding allows huge total throughput via parallelism. Datacenters fail independently -- different cities. Clients can read local replica -- fast! Can place replicas near relevant customers. Paxos requires only a majority -- tolerate slow/distant replicas.","title":"The benefit of shard across data center"},{"location":"Notes/papers/Storage/spanner/#challenges","text":"Read of local replica must yield fresh data. So it have the same overhead as Write request, vote via majority, It eliminate the benefit of locality. A transaction may involve multiple shards -> multiple Paxos groups. Transactions that read multiple records must be serializable. It means it need the assistance from Xaction Coordinator.","title":"Challenges"},{"location":"Notes/papers/Storage/spanner/#system-architecture","text":"A Spanner zone has one zonemaster and many spanservers. The former assigns data to spanservers; the latter serve data to clients. A spanserver is responsible for 100~1000 tablets. Each tablet contains a bag of mappings (key:string, timestamp:int64)\\rightarrow string from multiple partitions of the row space(data colocation). A tablet's state is stored in a set of B-tree like files and a write-ahead log on Colossus. A spanserver implements a Paxos state machine on each tablet; each Paxos state machine stores metadata and log in corresponding tablet. At every replica that is a leader, each spanserver implements a lock table and a transaction table to support distributed transactions(two-phase commit on mutliple Paxos groups).","title":"System Architecture"},{"location":"Notes/papers/Storage/spanner/#truetime","text":"Main method TT.now() returns an interval [earliest, latest] that is guaranteed to contain the absolute time when TT.now() is invoked. TT.after(t) and TT.before(t) are convenience wrappers. TrueTime uses two forms of time references, GPS and atomic clocks, as they have different failure modes. TrueTime is implemented by a set of time master machines per data center and a time slave daemon per machine. Each daemon polls a variety of masters, apply Marzullo's algorithm to detect liars and synchronize the local machine clock to non-liars. Between synchronizations, a daemon advertises a slowly increasing time uncertainty \\epsilon (average 4ms in production) derived from worst-case local clock drift, time master uncertainty and communication delay to the time masters.","title":"TrueTime"},{"location":"Notes/papers/Storage/spanner/#concurrency-control","text":"Spanner supports read-write transactions, read-only transactions and snapshot reads. Read-only transactions and snapshot reads execute at a specific timestamp without locking. Spanner maintains following invariants: disjointness: * within a Paxos group, each Paxos leader's lease interval is disjoint from every other leader's. monotonicity: within a Paxos group, Paxos writes have monotonically increasing timestamp. external consistency: if the start of a transaction T_2 occurs after the commit of a transaction T_1 , then the commit timestamp of T_2 must be greater than the commit timestamp of T_1 . For RW transactions, the coordinator assigns a commit timestamp S_i no later than TT.now().latest computed after receiving the commit request, and waits until TT.after(S_i) is true to apply the commit(\"commit wait\"). Sets the commit time to TT.now().latest since TT.now() returns a range time is between, exclusive so latest is guaranteed to not have occured. Keeps calling TT.now() until TT.now().earliest() is greater than above commit time. For RO transactions and snapshot reads, every replica tracks t_{safe} for the maximum at which the replica is up-to-date, which depends on the Paxos state machine and if there are prepared but not committed transactions.","title":"Concurrency Control"},{"location":"Notes/papers/Storage/spanner/#rw-transactions","text":"1 2 3 4 BEGIN x = x + 1 y = y - 1 END We don't want any read or write of x or y sneaking between our two ops. After commit, all reads should see our updates.","title":"R/W Transactions"},{"location":"Notes/papers/Storage/spanner/#two-phase-commit-2pc-with-paxos-replicated-participants","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Client picks a unique transaction id (TID). Client sends each read to Paxos leader of relevant shard (2.1). Each shard first acquires a lock on the relevant record. May have to wait. Separate lock table per shard, in shard leader. Read locks are not replicated via Paxos, so leader failure -> abort. Client keeps writes private until commit. When client commits (4.2.1): Chooses a Paxos group to act as 2pc Transaction Coordinator (TC). Sends writes to relevant shard leaders. Each written shard leader: Acquires lock(s) on the written record(s). Log a \"prepare\" record via Paxos, to replicate lock and new value. Tell TC it is prepared. Or tell TC \"no\" if crashed and thus lost lock table. Transaction Coordinator: Decides commit or abort. Logs the decision to its group via Paxos. Tell participant leaders and client the result. Each participant leader: Log the TC's decision via Paxos. Release the transaction's locks. Locking (two-phase locking) ensures serializability. 2pc widely hated b/c it blocks with locks held if TC fails. Replicating the TC with Paxos solves this problem!","title":"Two-phase commit (2pc) with Paxos-replicated participants"},{"location":"Notes/papers/Storage/spanner/#ro-transactions","text":"Eliminates two big costs from R/O transactions Read from local replicas, to avoid Paxos and cross-datacenter msgs. But note local replica may not be up to date! No locks, no two-phase commit, no transaction manager. Again to avoid cross-data center msg to Paxos leader. And to avoid slowing down r/w transactions. Tables 3 and 6 show a 10x latency improvement as a result! Achieves this through Snapshot Isolation Synchronize all computers' clocks (to real wall-clock time). Assign every transaction a time-stamp. r/w: commit time. r/o: start time. Execute as if one-at-a-time in time-stamp order. Even if actual reads occur in different order. Each replica stores multiple time-stamped versions of each record. All of a r/w transactions's writes get the same time-stamp. An r/o transaction's reads see version as of xaction's time-stamp. The record version with the highest time-stamp less than the xaction's. Obviously we can't synchronize all computer clocks, so TrueTime is used to give bounds of certainty.","title":"R/O Transactions"},{"location":"Notes/papers/Storage/spanner/#perspective","text":"Snapshot Isolation gives you serializable r/o transactions. Timestamps set an order. Snapshot versions (and safe time) implement consistent reads at a timestamp. Xaction sees all writes from lower-TS xactions, none from higher. Any number will do for TS if you don't care about external consistency. Synchronized timestamps yield external consistency . Even among transactions at different data centers. Even though reading from local replicas that might lag. Why is all this useful? Fast r/o transactions: Read from replica in client's datacenter. No locking, no two-phase commit. Thus the 10x latency improvement in Tables 3 and 6. Although: r/o transaction reads may block due to safe time, to catch up. r/w transaction commits may block in Commit Wait. Accurate (small interval) time minimizes these delays.","title":"Perspective"},{"location":"Notes/papers/Storage/tao/","text":"TAO TAO: Facebook\u2019s Distributed Data Store for the Social Graph A geographically distributed data store that provides efficient and timely access to the social graph. Background The content Facebook presents to each user is highly tailored, so we have to perform filtering and aggregation when the content is viewed instead of when the content is created. Facebook was originally built with MySQL/PHP/memcache. This look-aside key-value cache architecture has problems like inefficient edge lists, distributed control logic and expensive read-after-write consistency. TAO is designed to handle the workload of read-mostly access to a constantly changing graph. Efficiency and availability are explicitly favored over consistency. Data Model and API 1 2 3 Object:(id) -> (otype, (key -> value)) Association: (id1, atype, id2) -> (time, (key -> value)*) Association List: (id1, atype) -> [a1, a2, ..., an] Objects are typed nodes. Each object has a globally unique id. Associations are typed directed edges between objects; at most one association can exist between two objects. Both objects and associations may contain data as key->value pairs. Bidirectional edges are modeled as two associations. TAO keeps associations in sync with their inverses. TAO's association queries are organized around association lists(associations in descending order by the \"time\" field). A per-atype upper bound for each query is enforced( assoc_ranage(id1, atype, pos, limit) ), so the client must issue multiple queries to enumerate a longer association list. Architecture Basics TAO uses MySQL as persistent storage. Data is divided into logical shards and each database server is responsible for one or more shards. Each object id has an embedded shard_id(fixed for entire lifetime). An association is stored on the shard of its id1. Multiple caching servers together form a tier. Shards are mapped onto cache servers with consistent hashing. The TAO in-memory cache contains objects, association lists and association counts. Cache servers understand the semantics of the contents and can use them to answer queries not processed before(example: cache count zero for range query). Client issues requests directly to the appropriate cache server, which will complete the read/write request(contact other cache servers/databases if necessary). For write operation on an association with inverse, the cache server for id1 will contact the cache server for id2 and issue write to database after the inverse write is complete. TAO does not provide atomicity between two updates; hanging associations are repaired by an asynchronous job. Scaling Leader and Follower Tiers Larger tiers are problematic because they are prone to hot spots and have O(N^2) growth in all-to-all connections. We split the cache into two levels: a leader tier and multiple follow tiers. Leaders behave as the basic case, while followers will instead forward read misses and writes to leaders. To keep cache consistent, leaders asynchronously send cache maintenance messages to followers. A version number in message allows it to be ignored when arriving late. Geographically Cluster data center locations into a few regions where the intra-region latency is small. Store one copy of the social graph per region. Followers behave identically in all regions, forwarding read misses and writes to the local region's tier leader. Leaders query the local region's database regardless of whether master or slave. Writes are forwarded by the local leader to the leader in the master region. Master region is controlled separately for each shard and is automatically switched to recover from database failure. We prefer to locate all of the master databases in a single region, otherwise inverse write may introduce inter-region latency. Consistency After a write, TAO guarantees the eventual delivery of an invalidation or refill to all tiers. TAO reads marked as critical will be proxied to the master region for stronger consistency. Fault Tolerance Database failures: When a master database is down, one of its slave is promoted to be the new master. When a slave database is down, cache misses are redirected to leaders in the master region. Leader failures: Followers reroute read misses directly to the database and writes to a random replacement leader. Invalidation and refill failures: If a follower is unreachable, leader persists messages and redeliver them later. If these messages are lost due to permanent leader failure, a bulk invalidation operation is used to invalidate all data of specific shard in followers. Follower failures: Client requests are failed over to followers in other tiers.","title":"TAO"},{"location":"Notes/papers/Storage/tao/#tao","text":"TAO: Facebook\u2019s Distributed Data Store for the Social Graph A geographically distributed data store that provides efficient and timely access to the social graph.","title":"TAO"},{"location":"Notes/papers/Storage/tao/#background","text":"The content Facebook presents to each user is highly tailored, so we have to perform filtering and aggregation when the content is viewed instead of when the content is created. Facebook was originally built with MySQL/PHP/memcache. This look-aside key-value cache architecture has problems like inefficient edge lists, distributed control logic and expensive read-after-write consistency. TAO is designed to handle the workload of read-mostly access to a constantly changing graph. Efficiency and availability are explicitly favored over consistency.","title":"Background"},{"location":"Notes/papers/Storage/tao/#data-model-and-api","text":"1 2 3 Object:(id) -> (otype, (key -> value)) Association: (id1, atype, id2) -> (time, (key -> value)*) Association List: (id1, atype) -> [a1, a2, ..., an] Objects are typed nodes. Each object has a globally unique id. Associations are typed directed edges between objects; at most one association can exist between two objects. Both objects and associations may contain data as key->value pairs. Bidirectional edges are modeled as two associations. TAO keeps associations in sync with their inverses. TAO's association queries are organized around association lists(associations in descending order by the \"time\" field). A per-atype upper bound for each query is enforced( assoc_ranage(id1, atype, pos, limit) ), so the client must issue multiple queries to enumerate a longer association list.","title":"Data Model and API"},{"location":"Notes/papers/Storage/tao/#architecture","text":"","title":"Architecture"},{"location":"Notes/papers/Storage/tao/#basics","text":"TAO uses MySQL as persistent storage. Data is divided into logical shards and each database server is responsible for one or more shards. Each object id has an embedded shard_id(fixed for entire lifetime). An association is stored on the shard of its id1. Multiple caching servers together form a tier. Shards are mapped onto cache servers with consistent hashing. The TAO in-memory cache contains objects, association lists and association counts. Cache servers understand the semantics of the contents and can use them to answer queries not processed before(example: cache count zero for range query). Client issues requests directly to the appropriate cache server, which will complete the read/write request(contact other cache servers/databases if necessary). For write operation on an association with inverse, the cache server for id1 will contact the cache server for id2 and issue write to database after the inverse write is complete. TAO does not provide atomicity between two updates; hanging associations are repaired by an asynchronous job.","title":"Basics"},{"location":"Notes/papers/Storage/tao/#scaling","text":"Leader and Follower Tiers Larger tiers are problematic because they are prone to hot spots and have O(N^2) growth in all-to-all connections. We split the cache into two levels: a leader tier and multiple follow tiers. Leaders behave as the basic case, while followers will instead forward read misses and writes to leaders. To keep cache consistent, leaders asynchronously send cache maintenance messages to followers. A version number in message allows it to be ignored when arriving late. Geographically Cluster data center locations into a few regions where the intra-region latency is small. Store one copy of the social graph per region. Followers behave identically in all regions, forwarding read misses and writes to the local region's tier leader. Leaders query the local region's database regardless of whether master or slave. Writes are forwarded by the local leader to the leader in the master region. Master region is controlled separately for each shard and is automatically switched to recover from database failure. We prefer to locate all of the master databases in a single region, otherwise inverse write may introduce inter-region latency.","title":"Scaling"},{"location":"Notes/papers/Storage/tao/#consistency","text":"After a write, TAO guarantees the eventual delivery of an invalidation or refill to all tiers. TAO reads marked as critical will be proxied to the master region for stronger consistency.","title":"Consistency"},{"location":"Notes/papers/Storage/tao/#fault-tolerance","text":"Database failures: When a master database is down, one of its slave is promoted to be the new master. When a slave database is down, cache misses are redirected to leaders in the master region. Leader failures: Followers reroute read misses directly to the database and writes to a random replacement leader. Invalidation and refill failures: If a follower is unreachable, leader persists messages and redeliver them later. If these messages are lost due to permanent leader failure, a bulk invalidation operation is used to invalidate all data of specific shard in followers. Follower failures: Client requests are failed over to followers in other tiers.","title":"Fault Tolerance"},{"location":"Notes/todo/articles/","text":"TODO https://github.com/jlevy/the-art-of-command-line https://docs.microsoft.com/en-us/sysinternals/resources/windows-internals https://www.google.com/search?q=hnsw&pws=0&gl=us&gws_rd=cr https://arxiv.org/abs/1603.09320 https://arxiv.org/abs/2006.11632","title":"TODO"},{"location":"Notes/todo/articles/#todo","text":"https://github.com/jlevy/the-art-of-command-line https://docs.microsoft.com/en-us/sysinternals/resources/windows-internals https://www.google.com/search?q=hnsw&pws=0&gl=us&gws_rd=cr https://arxiv.org/abs/1603.09320 https://arxiv.org/abs/2006.11632","title":"TODO"},{"location":"Rust/1.1%20Cargo/","text":"Cargo cargo new <name> : creates a new project with the specified name. cargo build compiles into target/debug . cargo build --release compiles with optimizations. Creates an executable in target/release instead of in target/debug . Optimizations make your code run faster but takes longer to compile. cargo update updates versions in Cargo.toml . cargo check quickly checks your code to make sure it compiles but doesn't produce an executable. cargo run builds and runs. cargo doc -- open builds documentation provided by all your dependencies locally.","title":"Cargo"},{"location":"Rust/1.1%20Cargo/#cargo","text":"cargo new <name> : creates a new project with the specified name. cargo build compiles into target/debug . cargo build --release compiles with optimizations. Creates an executable in target/release instead of in target/debug . Optimizations make your code run faster but takes longer to compile. cargo update updates versions in Cargo.toml . cargo check quickly checks your code to make sure it compiles but doesn't produce an executable. cargo run builds and runs. cargo doc -- open builds documentation provided by all your dependencies locally.","title":"Cargo"},{"location":"Rust/2.1%20Variables%20and%20Mutability/","text":"Variables and Mutability let declares a variable. Variables are immutable unless declared mut mutable. 1 2 3 4 5 let x = 5 ; let mut y = 6 ; x = 7 ; //Compile Error y = 7 ; //No error const creates a constant that is always immutable . This must be set to a constant expression, not something that has to be determined at runtime. 1 const THREE_HOURS_IN_SECONDS : u32 = 60 * 60 * 3 ; Shadowing is when you declare a new variable with the same name as a previous one. (The first is shadowed by the second variable.) You shadow by using let repeatedly: 1 2 3 4 5 6 7 8 9 fn main () { let x = 5 ; let x = x + 1 ; { let x = x * 2 ; println! ( \"The value of x in the inner scope is: {}\" , x ); } println! ( \"The value of x is: {}\" , x ); } 1 2 3 4 5 6 $ cargo run Compiling variables v0.1.0 (file:///projects/variables) Finished dev [unoptimized + debuginfo] target(s) in 0.31s Running `target/debug/variables` The value of x in the inner scope is: 12 The value of x is: 6 Shadowing is different from marking a variable as mut. We get a compile-time error if we reassign to this variable without using let. By using let, we can perform a few transformations on a value but have the variable be immutable after those transformations have been completed. The other difference is that because we're creating a new variable when we use let , we can change the type of the value but reuse the same name. 1 2 let spaces = \" \" ; let spaces = spaces . len (); However, the following gets a compile time error. 1 2 let mut spaces = \" \" ; spaces = spaces . len ();","title":"Variables and Mutability"},{"location":"Rust/2.1%20Variables%20and%20Mutability/#variables-and-mutability","text":"let declares a variable. Variables are immutable unless declared mut mutable. 1 2 3 4 5 let x = 5 ; let mut y = 6 ; x = 7 ; //Compile Error y = 7 ; //No error const creates a constant that is always immutable . This must be set to a constant expression, not something that has to be determined at runtime. 1 const THREE_HOURS_IN_SECONDS : u32 = 60 * 60 * 3 ; Shadowing is when you declare a new variable with the same name as a previous one. (The first is shadowed by the second variable.) You shadow by using let repeatedly: 1 2 3 4 5 6 7 8 9 fn main () { let x = 5 ; let x = x + 1 ; { let x = x * 2 ; println! ( \"The value of x in the inner scope is: {}\" , x ); } println! ( \"The value of x is: {}\" , x ); } 1 2 3 4 5 6 $ cargo run Compiling variables v0.1.0 (file:///projects/variables) Finished dev [unoptimized + debuginfo] target(s) in 0.31s Running `target/debug/variables` The value of x in the inner scope is: 12 The value of x is: 6 Shadowing is different from marking a variable as mut. We get a compile-time error if we reassign to this variable without using let. By using let, we can perform a few transformations on a value but have the variable be immutable after those transformations have been completed. The other difference is that because we're creating a new variable when we use let , we can change the type of the value but reuse the same name. 1 2 let spaces = \" \" ; let spaces = spaces . len (); However, the following gets a compile time error. 1 2 let mut spaces = \" \" ; spaces = spaces . len ();","title":"Variables and Mutability"},{"location":"Rust/2.2%20Functions/","text":"Functions Statement - instructions that perform some action and do not return a value. Expression - evaluate to a resulting value. Statement: 1 let x = 5 ; Expression: 1 5 Statements do not return values. Therefore, you can\u2019t assign a let statement to another variable. 1 let x = ( let y = 6 ); //Error Calling a function is an expression ( hello() ), calling a macro is an expression ( println!(\"Guess the number!\") ), and the block used to create new scopes {} is an expression: 1 2 3 4 { let x = 5 ; x - 2 } The above expression evaluates to 3 . Expressions do not include semi-colons. If you add a semi-colon, it becomes a statement and will not return a value.","title":"Functions"},{"location":"Rust/2.2%20Functions/#functions","text":"Statement - instructions that perform some action and do not return a value. Expression - evaluate to a resulting value. Statement: 1 let x = 5 ; Expression: 1 5 Statements do not return values. Therefore, you can\u2019t assign a let statement to another variable. 1 let x = ( let y = 6 ); //Error Calling a function is an expression ( hello() ), calling a macro is an expression ( println!(\"Guess the number!\") ), and the block used to create new scopes {} is an expression: 1 2 3 4 { let x = 5 ; x - 2 } The above expression evaluates to 3 . Expressions do not include semi-colons. If you add a semi-colon, it becomes a statement and will not return a value.","title":"Functions"},{"location":"System%20Design/Template/","text":"Template (1) Gather Requirements [1-2 min] Ask questions to help define what parts of the system we will focus on. Users/Customers Who will use? How will system be used? Do we need very low latency? (Usually serve from Memory then) Usage patterns What data do we store? Does the data change alot? Real time or delayed data ok? Stale data okay? Read your own writes usually a must otherwise confusing Do we need to push notifications? Scale (use answers for capacity estimation) How many users? How many data points? Open source technologies ok? Cost of development? (2) Functional + Non Functional Requirements [5 min] Functional requirements Actions the systems will make. Expand to make more extensible. countViewEvent -> countEvent -> processEvent -> processEvents How to prevent abuse? API Key for greater limits Non-Functional requirements Availability vs Consistency (discuss) Scalability Performance (Low latency) Durable (if required) What features should we ensure (payment always processed, data not lost, etc) (3) Capacity Estimations [3 min] Num users per day & Num of events per user = Num events per day QPS = Num events per day / 1E5 R&W Ratio applied to QPS to figure out reads/writes per second. Storage size per 1 day and 5 yr = Num events per day * 5Y Bandwidth read / write = R/W per second * size of each (Optional) Bandwidth estimate (#of bytes/sec system should handle for incoming and outgoing traffic) (Optional) Cache estimate = 20% * Traffic 80-20 rule - 20% of objects are used 80% of the time. If we are using a cache, what is the kind of data we want to store in cache Redis. Do we need persistence? (4) High Level Design [5-10 min] \"Hey I'm going to solve problem for small scale. 1 server is going to handle it. Then once I've fixed the solution I will work on scaling it up.\" Explain high level data flow for each operation in functional requirements . Stateful vs Stateless web tier: stateful has to deal with sticky session and other compleixty, stateless is more robust and scalable too. (5) Data Model [5 min] Think about the nature of our data How many records do we store? (Capacity Estimations) Size of each object? (Capacity Estimations) . Raw data or aggregated data? Relationship between records? Read vs Write heavy? Defining the data model in the early part of the interview will clarify how data will flow between different system components. Later, it will guide for data partitioning and management. The candidate should identify various system entities, how they will interact with each other, and different aspects of data management like storage, transportation, encryption, etc. Which database system should we use? Will Non Relational like Cassandra best fit our needs, or should we use a relational-like solution? What kind of block storage should we use to store photos and videos? SQL consider if we have lots of tables, joins, need for consistency and transactions. Relational is harder to scale since it offers more guarantees. Does not mean its not as scalable, but more work and may lose guarantees. Availability, Consistency, Txn, Schema, Scalability, joins, ACID properties Writes scaled by sharding / Reads scaled by replication MySQL auto-incrementing columns for primary keys (unique IDs) Non Relational Easily scaled for writes / reads by simply adding more servers and using Consistent hashing. See SQL or NoSQL (6) Deep Dive [15-20 min] Scaling individual components: Use Non Functional Requirements to drive this. Availability, Consistency and Scale story for each component Consistency and availability patterns Database scaling Data access patterns. Do we need seperate hot/cold storage? DB cleanup processor? Vertical vs Horizontal. Usually Vertical not as cost efficient, reliable, or scalable. When sharding in H, most important factor is sharding key. Resharding, Celebrity, Join/Denormalization. Consider Proximity Server. We store data in DB but create Quad Tree in memory. Maybe we cannot fit entire Quad Tree on server. Shard so each quad tree contains 1/10th of original quad tree. How to shard? If we shard on regionid, we may have hot regions. NY machines will have alot more queries/data than SA machines Although not applicable to Quad Tree, one approach to deal with hot partitions is to include event time into the partition key. All events for one minute go to one host and over time the load is distributed. If we shard on locationId, we have to fan out to all hosts containing each quad tree to aggregate results. Each quad tree will contain random elemsensts Tradeoff Consistent Hashing MySQL Cluster proxy that routes traffic to correct shard. To know the shards, can use a configuration service which maintains a connection to all shards using ZooKeeper ephemeral nodes. Shard proxy can sit infront of a database and cache queries, monitor health, and publish metrics. Replication with async, semisync, sync Complex, FB uses many optimizations and extensive cache layer. Also moved to LSM which has less write amplification w/ RocksDB. NoSQL Utilizes gossip protocol instead of a cluster proxy w/ configuration service. Uses consistent hashing to pick the node to write the data. Data is replicated to next 2-3 nodes in the ring. Can increase read/write quorum. Mention utilizing multiple data centers per shard. Message Queues Asynchronous communication between web servers. Servse as a buffer and distributes requests. If consumer is unavailable, message will not be lost. Fire and forget. There are many benefits of batching: it increases throughput, it helps to save on cost, request compression is more effective. But there are drawbacks as well. It introduces some complexity both on the client and the server side. For example think of a scenario when partitioner service processes a batch request and several events from the batch fail, while other succeed. Should we re-send the whole batch? Caches For data that is read heavy (data access patterns!) Seperate cache tier? Yes, better performance, ability to scale independently Persistence? Eviction policies: LRU LFU FIFO Cache Loading Policies Cache aside Write through Write behind Refresh ahead Consistency? ReadYourOwnWrites: Cache in your own region always updated on writes. Tie user to a specific cache cluster. SQL Binlog used to replicate cache invalidations to other servers. Client caching, CDN caching, Web server caching, Database caching, Application caching, Cache @Query level, Cache @Object level Load Balancer Between clients and application servers, application servers and database/cache servers. Layer 4 vs Layer 7 - transport vs application. Layer 7 has more info at cost of time and computing resources. Pros Prevents requests going to unhealthy servers. Eliminates single point of failure SSL termination - decrypt incoming requests and encrypt server responses Sticky sessions Cons Can become performance bottleneck, increased complexity, need for multiple to prevent SPoF Primary/Seconday load balancer to address SpoF Communication TCP: ordered, more overhead, has seq numbers and checksum fields for each packet UDP: unordered, less latency, less overhead Short Poll vs Long Poll vs Websocket JSON vs Thrift. Saving space on the wire. 50% reductions Field tags vs field names. Error cases Write Ahead Log for services that keep in memory data structures Timeouts on service calls set to speed of 1% slowest requests. Maybe hit a bad server. Retry mechanism. Exponential backoff with jitter. CDN Lightens DB load by serving static content (images, videso, CSS, JS) Push vs Pull? Push better for small traffic. Pull allows user demanded data to be populated with TTL. Generally costly. Charged for data transfer in/out so should not cache infrequently used assets. Logging/Metrics Host level metrics: CPU/Memory/Disk I/O Aggregated level metrics: performance of entire database/cache tier Business metrics: DAU, retention, revenue","title":"Template"},{"location":"System%20Design/Template/#template","text":"","title":"Template"},{"location":"System%20Design/Template/#1-gather-requirements-1-2-min","text":"Ask questions to help define what parts of the system we will focus on. Users/Customers Who will use? How will system be used? Do we need very low latency? (Usually serve from Memory then) Usage patterns What data do we store? Does the data change alot? Real time or delayed data ok? Stale data okay? Read your own writes usually a must otherwise confusing Do we need to push notifications? Scale (use answers for capacity estimation) How many users? How many data points? Open source technologies ok? Cost of development?","title":"(1) Gather Requirements [1-2 min]"},{"location":"System%20Design/Template/#2-functional-non-functional-requirements-5-min","text":"Functional requirements Actions the systems will make. Expand to make more extensible. countViewEvent -> countEvent -> processEvent -> processEvents How to prevent abuse? API Key for greater limits Non-Functional requirements Availability vs Consistency (discuss) Scalability Performance (Low latency) Durable (if required) What features should we ensure (payment always processed, data not lost, etc)","title":"(2) Functional + Non Functional Requirements [5 min]"},{"location":"System%20Design/Template/#3-capacity-estimations-3-min","text":"Num users per day & Num of events per user = Num events per day QPS = Num events per day / 1E5 R&W Ratio applied to QPS to figure out reads/writes per second. Storage size per 1 day and 5 yr = Num events per day * 5Y Bandwidth read / write = R/W per second * size of each (Optional) Bandwidth estimate (#of bytes/sec system should handle for incoming and outgoing traffic) (Optional) Cache estimate = 20% * Traffic 80-20 rule - 20% of objects are used 80% of the time. If we are using a cache, what is the kind of data we want to store in cache Redis. Do we need persistence?","title":"(3) Capacity Estimations [3 min]"},{"location":"System%20Design/Template/#4-high-level-design-5-10-min","text":"\"Hey I'm going to solve problem for small scale. 1 server is going to handle it. Then once I've fixed the solution I will work on scaling it up.\" Explain high level data flow for each operation in functional requirements . Stateful vs Stateless web tier: stateful has to deal with sticky session and other compleixty, stateless is more robust and scalable too.","title":"(4) High Level Design [5-10 min]"},{"location":"System%20Design/Template/#5-data-model-5-min","text":"Think about the nature of our data How many records do we store? (Capacity Estimations) Size of each object? (Capacity Estimations) . Raw data or aggregated data? Relationship between records? Read vs Write heavy? Defining the data model in the early part of the interview will clarify how data will flow between different system components. Later, it will guide for data partitioning and management. The candidate should identify various system entities, how they will interact with each other, and different aspects of data management like storage, transportation, encryption, etc. Which database system should we use? Will Non Relational like Cassandra best fit our needs, or should we use a relational-like solution? What kind of block storage should we use to store photos and videos? SQL consider if we have lots of tables, joins, need for consistency and transactions. Relational is harder to scale since it offers more guarantees. Does not mean its not as scalable, but more work and may lose guarantees. Availability, Consistency, Txn, Schema, Scalability, joins, ACID properties Writes scaled by sharding / Reads scaled by replication MySQL auto-incrementing columns for primary keys (unique IDs) Non Relational Easily scaled for writes / reads by simply adding more servers and using Consistent hashing. See SQL or NoSQL","title":"(5) Data Model [5 min]"},{"location":"System%20Design/Template/#6-deep-dive-15-20-min","text":"Scaling individual components: Use Non Functional Requirements to drive this. Availability, Consistency and Scale story for each component Consistency and availability patterns","title":"(6) Deep Dive [15-20 min]"},{"location":"System%20Design/Template/#database-scaling","text":"Data access patterns. Do we need seperate hot/cold storage? DB cleanup processor? Vertical vs Horizontal. Usually Vertical not as cost efficient, reliable, or scalable. When sharding in H, most important factor is sharding key. Resharding, Celebrity, Join/Denormalization. Consider Proximity Server. We store data in DB but create Quad Tree in memory. Maybe we cannot fit entire Quad Tree on server. Shard so each quad tree contains 1/10th of original quad tree. How to shard? If we shard on regionid, we may have hot regions. NY machines will have alot more queries/data than SA machines Although not applicable to Quad Tree, one approach to deal with hot partitions is to include event time into the partition key. All events for one minute go to one host and over time the load is distributed. If we shard on locationId, we have to fan out to all hosts containing each quad tree to aggregate results. Each quad tree will contain random elemsensts Tradeoff Consistent Hashing MySQL Cluster proxy that routes traffic to correct shard. To know the shards, can use a configuration service which maintains a connection to all shards using ZooKeeper ephemeral nodes. Shard proxy can sit infront of a database and cache queries, monitor health, and publish metrics. Replication with async, semisync, sync Complex, FB uses many optimizations and extensive cache layer. Also moved to LSM which has less write amplification w/ RocksDB. NoSQL Utilizes gossip protocol instead of a cluster proxy w/ configuration service. Uses consistent hashing to pick the node to write the data. Data is replicated to next 2-3 nodes in the ring. Can increase read/write quorum. Mention utilizing multiple data centers per shard.","title":"Database scaling"},{"location":"System%20Design/Template/#message-queues","text":"Asynchronous communication between web servers. Servse as a buffer and distributes requests. If consumer is unavailable, message will not be lost. Fire and forget. There are many benefits of batching: it increases throughput, it helps to save on cost, request compression is more effective. But there are drawbacks as well. It introduces some complexity both on the client and the server side. For example think of a scenario when partitioner service processes a batch request and several events from the batch fail, while other succeed. Should we re-send the whole batch?","title":"Message Queues"},{"location":"System%20Design/Template/#caches","text":"For data that is read heavy (data access patterns!) Seperate cache tier? Yes, better performance, ability to scale independently Persistence? Eviction policies: LRU LFU FIFO Cache Loading Policies Cache aside Write through Write behind Refresh ahead Consistency? ReadYourOwnWrites: Cache in your own region always updated on writes. Tie user to a specific cache cluster. SQL Binlog used to replicate cache invalidations to other servers. Client caching, CDN caching, Web server caching, Database caching, Application caching, Cache @Query level, Cache @Object level","title":"Caches"},{"location":"System%20Design/Template/#load-balancer","text":"Between clients and application servers, application servers and database/cache servers. Layer 4 vs Layer 7 - transport vs application. Layer 7 has more info at cost of time and computing resources. Pros Prevents requests going to unhealthy servers. Eliminates single point of failure SSL termination - decrypt incoming requests and encrypt server responses Sticky sessions Cons Can become performance bottleneck, increased complexity, need for multiple to prevent SPoF Primary/Seconday load balancer to address SpoF","title":"Load Balancer"},{"location":"System%20Design/Template/#communication","text":"TCP: ordered, more overhead, has seq numbers and checksum fields for each packet UDP: unordered, less latency, less overhead Short Poll vs Long Poll vs Websocket JSON vs Thrift. Saving space on the wire. 50% reductions Field tags vs field names.","title":"Communication"},{"location":"System%20Design/Template/#error-cases","text":"Write Ahead Log for services that keep in memory data structures Timeouts on service calls set to speed of 1% slowest requests. Maybe hit a bad server. Retry mechanism. Exponential backoff with jitter.","title":"Error cases"},{"location":"System%20Design/Template/#cdn","text":"Lightens DB load by serving static content (images, videso, CSS, JS) Push vs Pull? Push better for small traffic. Pull allows user demanded data to be populated with TTL. Generally costly. Charged for data transfer in/out so should not cache infrequently used assets.","title":"CDN"},{"location":"System%20Design/Template/#loggingmetrics","text":"Host level metrics: CPU/Memory/Disk I/O Aggregated level metrics: performance of entire database/cache tier Business metrics: DAU, retention, revenue","title":"Logging/Metrics"},{"location":"System%20Design/Components/KeyGenerationService/","text":"Key Generation Service A lot of times we have a problem that needs to create a unique id for a given input. Doing this at run time may be costly. Lets discuss both approaches and the tradeoffs. MySQL Auto Incrementing Columns MySQL auto-incrementing columns for primary keys, howver MySQL can\u2019t guarantee uniqueness across physical and logical databases. Two servers, one generating odd keys, one generating even keys. Prevents single point of failure. https://code.flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/ Bottleneck as requests to DB can become overloaded if we have enough TPS. Can't generate keys fast enough. 1 2 3 4 5 6 7 TicketServer1: auto-increment-increment = 2 auto-increment-offset = 1 TicketServer2: auto-increment-increment = 2 auto-increment-offset = 2 Snowflake IDs Have distributed service of machines that generate IDs with following schema Time - 41 bits (millisecond precision) (41 bits lets you have 1 Trillion seconds which is 1M years) Machine Id - 11 bits Sequence number - 12 bits - 2000 events per this millisecond Reset sequence number every second Performance still degrated by the roundtrips to the ID server. However this delay is considerably less than flushing objects in MySQL. If we make our TweetID 64bits (8 bytes) long, we can easily store tweets for the next 100 years and also store them for mili-seconds granularity. In the above approach, we still have to query all the servers for timeline generation, but our reads (and writes) will be substantially quicker. Since we don't have any secondary index (on creation time) this will reduce our write latency. While reading, we don't need to filter on creation-time as our primary key has epoch time included in it. Encoding actual URL We can compute a unique hash (e.g., MD5 or SHA256 , etc.) of the given URL. The hash can then be encoded for display. This encoding could be base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]) and if we add '+' and '/' we can use Base64 encoding. A reasonable question would be, what should be the length of the short key? 6, 8, or 10 characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings.\\ Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings. With 68.7B unique strings, let's assume six letter keys would suffice for our system. If we use the MD5 algorithm as our hash function, it will produce a 128-bit hash value. After base64 encoding, we'll get a string having more than 21 characters (since each base64 character encodes 6 bits of the hash value). Now we only have space for 6 (or 8) characters per short key; how will we choose our key then? We can take the first 6 (or 8) letters for the key. This could result in key duplication; to resolve that, we can choose some other characters out of the encoding string or swap some characters. What are the different issues with our solution? We have the following couple of problems with our encoding scheme: If multiple users enter the same URL, they can get the same shortened URL, which is not acceptable. What if parts of the URL are URL-encoded? e.g., http://www.educative.io/distributed.php?id=design , and http://www.educative.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Workaround for the issues : We can append an increasing sequence number to each input URL to make it unique and then generate its hash. We don't need to store this sequence number in the databases, though. Possible problems with this approach could be an ever-increasing sequence number. Can it overflow? Appending an increasing sequence number will also impact the performance of the service. Another solution could be to append the user id (which should be unique) to the input URL. However, if the user has not signed in, we would have to ask the user to choose a uniqueness key. Even after this, if we have a conflict, we have to keep generating a key until we get a unique one. Generating keys offline We can have a standalone Key Generation Service (KGS) that generates random six-letter strings beforehand and stores them in a database (let's call it key-DB). Whenever we want to shorten a URL, we will take one of the already-generated keys and use it. This approach will make things quite simple and fast. Not only are we not encoding the URL, but we won't have to worry about duplications or collisions. KGS will make sure all the keys inserted into key-DB are unique Can concurrency cause problems? As soon as a key is used, it should be marked in the database to ensure that it is not used again. If there are multiple servers reading keys concurrently, we might get a scenario where two or more servers try to read the same key from the database. How can we solve this concurrency problem? Servers can use KGS to read/mark keys in the database. KGS can use two tables to store keys: one for keys that are not used yet, and one for all the used keys. As soon as KGS gives keys to one of the servers, it can move them to the used keys table. KGS can always keep some keys in memory to quickly provide them whenever a server needs them. For simplicity, as soon as KGS loads some keys in memory, it can move them to the used keys table. This ensures each server gets unique keys. If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys--which could be acceptable, given the huge number of keys we have. KGS also has to make sure not to give the same key to multiple servers. For that, it must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server. What would be the key-DB size? With base64 encoding, we can generate 68.7B unique six letters keys. If we need one byte to store one alpha-numeric character, we can store all these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Isn't KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS. Whenever the primary server dies, the standby server can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although, in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This can be acceptable since we have 68B unique six-letter keys. How would we perform a key lookup? We can look up the key in our database to get the full URL. If it's present in the DB, issue an \"HTTP 302 Redirect\" status back to the browser, passing the stored URL in the \"Location\" field of the request. If that key is not present in our system, issue an \"HTTP 404 Not Found\" status or redirect the user back to the homepage. Should we impose size limits on custom aliases? Our service supports custom aliases. Users can pick any 'key' they like, but providing a custom alias is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on a custom alias to ensure we have a consistent URL database. Let's assume users can specify a maximum of 16 characters per customer key (as reflected in the above database schema). Existing Solutions UUID UUIDs are 128-bit hexadecimal numbers that are globally unique. The chances of the same UUID getting generated twice is negligible. The problem with UUIDs is that they are very big in size and don't index well. When your dataset increases, the index size increases as well and the query performance takes a hit. Twitter Snowflake Twitter snowflake is a dedicated network service for generating 64-bit unique IDs at high scale. The IDs generated by this service are roughly time sortable. The IDs are made up of the following components: Epoch timestamp in millisecond precision - 41 bits (gives us 69 years with a custom epoch) Configured machine id - 10 bits (gives us up to 1024 machines) Sequence number - 12 bits (A local counter per machine that rolls over every 4096) The extra 1 bit is reserved for future purposes. Since the IDs use timestamp as the first component, they are time sortable. The IDs generated by twitter snowflake fits in 64-bits and are time sortable, which is great. That's what we want. But If we use Twitter snowflake, we'll again be introducing another component in our infrastructure that we need to maintain.","title":"Key Generation Service"},{"location":"System%20Design/Components/KeyGenerationService/#key-generation-service","text":"A lot of times we have a problem that needs to create a unique id for a given input. Doing this at run time may be costly. Lets discuss both approaches and the tradeoffs.","title":"Key Generation Service"},{"location":"System%20Design/Components/KeyGenerationService/#mysql-auto-incrementing-columns","text":"MySQL auto-incrementing columns for primary keys, howver MySQL can\u2019t guarantee uniqueness across physical and logical databases. Two servers, one generating odd keys, one generating even keys. Prevents single point of failure. https://code.flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/ Bottleneck as requests to DB can become overloaded if we have enough TPS. Can't generate keys fast enough. 1 2 3 4 5 6 7 TicketServer1: auto-increment-increment = 2 auto-increment-offset = 1 TicketServer2: auto-increment-increment = 2 auto-increment-offset = 2","title":"MySQL Auto Incrementing Columns"},{"location":"System%20Design/Components/KeyGenerationService/#snowflake-ids","text":"Have distributed service of machines that generate IDs with following schema Time - 41 bits (millisecond precision) (41 bits lets you have 1 Trillion seconds which is 1M years) Machine Id - 11 bits Sequence number - 12 bits - 2000 events per this millisecond Reset sequence number every second Performance still degrated by the roundtrips to the ID server. However this delay is considerably less than flushing objects in MySQL. If we make our TweetID 64bits (8 bytes) long, we can easily store tweets for the next 100 years and also store them for mili-seconds granularity. In the above approach, we still have to query all the servers for timeline generation, but our reads (and writes) will be substantially quicker. Since we don't have any secondary index (on creation time) this will reduce our write latency. While reading, we don't need to filter on creation-time as our primary key has epoch time included in it.","title":"Snowflake IDs"},{"location":"System%20Design/Components/KeyGenerationService/#encoding-actual-url","text":"We can compute a unique hash (e.g., MD5 or SHA256 , etc.) of the given URL. The hash can then be encoded for display. This encoding could be base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]) and if we add '+' and '/' we can use Base64 encoding. A reasonable question would be, what should be the length of the short key? 6, 8, or 10 characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings.\\ Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings. With 68.7B unique strings, let's assume six letter keys would suffice for our system. If we use the MD5 algorithm as our hash function, it will produce a 128-bit hash value. After base64 encoding, we'll get a string having more than 21 characters (since each base64 character encodes 6 bits of the hash value). Now we only have space for 6 (or 8) characters per short key; how will we choose our key then? We can take the first 6 (or 8) letters for the key. This could result in key duplication; to resolve that, we can choose some other characters out of the encoding string or swap some characters. What are the different issues with our solution? We have the following couple of problems with our encoding scheme: If multiple users enter the same URL, they can get the same shortened URL, which is not acceptable. What if parts of the URL are URL-encoded? e.g., http://www.educative.io/distributed.php?id=design , and http://www.educative.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Workaround for the issues : We can append an increasing sequence number to each input URL to make it unique and then generate its hash. We don't need to store this sequence number in the databases, though. Possible problems with this approach could be an ever-increasing sequence number. Can it overflow? Appending an increasing sequence number will also impact the performance of the service. Another solution could be to append the user id (which should be unique) to the input URL. However, if the user has not signed in, we would have to ask the user to choose a uniqueness key. Even after this, if we have a conflict, we have to keep generating a key until we get a unique one.","title":"Encoding actual URL"},{"location":"System%20Design/Components/KeyGenerationService/#generating-keys-offline","text":"We can have a standalone Key Generation Service (KGS) that generates random six-letter strings beforehand and stores them in a database (let's call it key-DB). Whenever we want to shorten a URL, we will take one of the already-generated keys and use it. This approach will make things quite simple and fast. Not only are we not encoding the URL, but we won't have to worry about duplications or collisions. KGS will make sure all the keys inserted into key-DB are unique Can concurrency cause problems? As soon as a key is used, it should be marked in the database to ensure that it is not used again. If there are multiple servers reading keys concurrently, we might get a scenario where two or more servers try to read the same key from the database. How can we solve this concurrency problem? Servers can use KGS to read/mark keys in the database. KGS can use two tables to store keys: one for keys that are not used yet, and one for all the used keys. As soon as KGS gives keys to one of the servers, it can move them to the used keys table. KGS can always keep some keys in memory to quickly provide them whenever a server needs them. For simplicity, as soon as KGS loads some keys in memory, it can move them to the used keys table. This ensures each server gets unique keys. If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys--which could be acceptable, given the huge number of keys we have. KGS also has to make sure not to give the same key to multiple servers. For that, it must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server. What would be the key-DB size? With base64 encoding, we can generate 68.7B unique six letters keys. If we need one byte to store one alpha-numeric character, we can store all these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Isn't KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS. Whenever the primary server dies, the standby server can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although, in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This can be acceptable since we have 68B unique six-letter keys. How would we perform a key lookup? We can look up the key in our database to get the full URL. If it's present in the DB, issue an \"HTTP 302 Redirect\" status back to the browser, passing the stored URL in the \"Location\" field of the request. If that key is not present in our system, issue an \"HTTP 404 Not Found\" status or redirect the user back to the homepage. Should we impose size limits on custom aliases? Our service supports custom aliases. Users can pick any 'key' they like, but providing a custom alias is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on a custom alias to ensure we have a consistent URL database. Let's assume users can specify a maximum of 16 characters per customer key (as reflected in the above database schema).","title":"Generating keys offline"},{"location":"System%20Design/Components/KeyGenerationService/#existing-solutions","text":"","title":"Existing Solutions"},{"location":"System%20Design/Components/KeyGenerationService/#uuid","text":"UUIDs are 128-bit hexadecimal numbers that are globally unique. The chances of the same UUID getting generated twice is negligible. The problem with UUIDs is that they are very big in size and don't index well. When your dataset increases, the index size increases as well and the query performance takes a hit.","title":"UUID"},{"location":"System%20Design/Components/KeyGenerationService/#twitter-snowflake","text":"Twitter snowflake is a dedicated network service for generating 64-bit unique IDs at high scale. The IDs generated by this service are roughly time sortable. The IDs are made up of the following components: Epoch timestamp in millisecond precision - 41 bits (gives us 69 years with a custom epoch) Configured machine id - 10 bits (gives us up to 1024 machines) Sequence number - 12 bits (A local counter per machine that rolls over every 4096) The extra 1 bit is reserved for future purposes. Since the IDs use timestamp as the first component, they are time sortable. The IDs generated by twitter snowflake fits in 64-bits and are time sortable, which is great. That's what we want. But If we use Twitter snowflake, we'll again be introducing another component in our infrastructure that we need to maintain.","title":"Twitter Snowflake"},{"location":"System%20Design/Concepts/CDN/","text":"CDN A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content. The site's DNS resolution will tell clients which server to contact. Serving content from CDNs can significantly improve performance in two ways: Users receive content from data centers close to them Reduce latency and round-trip time Ensure sufficient bandwidth for high traffic periods They reduce the workload on origin servers Since it takes a lot of computing power for a single server to respond to requests, and even more so for video live streaming, CDNs essentially protect the origin servers from overload and keep it operational. Push CDNs Push CDNs receive new content whenever changes occur on your server. You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN. You can configure when content expires and when it is updated. Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage. Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs. Content is placed on the CDNs once, instead of being re-pulled at regular intervals. Pull CDNs Pull CDNs grab new content from your server when the first user requests the content. You leave the content on your server and rewrite URLs to point to the CDN. This results in a slower request until the content is cached on the CDN. A time-to-live (TTL) determines how long content is cached. Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed. Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN. Disadvantages CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN. Content might be stale if it is updated before the TTL expires it. CDNs require changing URLs for static content to point to the CDN. Live Streaming CDN How does a live streaming CDN work? Step 1: Video capture Content creator captures the raw data or visual information using a camera. The data is represented in binary 1s and 0s in the device. Step 2: Segmentation Video file is broken down into smaller parts of a few seconds in length. Breaking them down into segments helps in streaming the entire video bit by bit. Step 3: Compression and encoding Each of the segments are compressed and encoded. Compressing removes redundant visual information such as a background that does not change in the video. This makes it easy to render just the moving frames in the video before streaming. Encoding is a process that is necessary to convert the data into a format that is compatible with the variety of devices that the end user consumes the content on. For example, H.264, HEVC, VP9 and AV1 are some of the popular formats that videos are encoding into. Step 4: Content Distribution and CDN Caching Next, the segmented, compressed and encoded video is distributed to end users. When the end user accesses a website or plays a video, their device (client) sends a request to the origin server to retrieve these files. Now if the users are located in close proximity to the server, or within a nearby region, this should not be a problem and the video files are streamed without much of an issue. In fact, if your viewership is small and they are not widely distributed, the single server can stream to all your users. There is no need to introduce more elements into your streaming workflow. But when the users are dispersed across a larger geographical area, in some cases across different countries, the round-trip-time for the server to deliver the content can be longer, resulting in delays or latency. This results in a below par user experience and one that is inconsistent across all of the video's consumers. Using a CDN solves this problem by caching the content in its distributed network of streaming servers. The CDN server closest to a particular end user will take care of delivering the content to that user. Step 5: Decoding and playback Once the video data reaches the users, their devices will decode and decompress the video segment by segment into the binary raw data. And with a video player, the user is able to see the visual information and play the video. Advantages Ensure sufficient bandwidth for high traffic periods Reduce latency and round-trip time Help in live streaming to a global audience Reduce the workload on origin servers","title":"CDN"},{"location":"System%20Design/Concepts/CDN/#cdn","text":"A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content. The site's DNS resolution will tell clients which server to contact. Serving content from CDNs can significantly improve performance in two ways: Users receive content from data centers close to them Reduce latency and round-trip time Ensure sufficient bandwidth for high traffic periods They reduce the workload on origin servers Since it takes a lot of computing power for a single server to respond to requests, and even more so for video live streaming, CDNs essentially protect the origin servers from overload and keep it operational.","title":"CDN"},{"location":"System%20Design/Concepts/CDN/#push-cdns","text":"Push CDNs receive new content whenever changes occur on your server. You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN. You can configure when content expires and when it is updated. Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage. Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs. Content is placed on the CDNs once, instead of being re-pulled at regular intervals.","title":"Push CDNs"},{"location":"System%20Design/Concepts/CDN/#pull-cdns","text":"Pull CDNs grab new content from your server when the first user requests the content. You leave the content on your server and rewrite URLs to point to the CDN. This results in a slower request until the content is cached on the CDN. A time-to-live (TTL) determines how long content is cached. Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed. Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.","title":"Pull CDNs"},{"location":"System%20Design/Concepts/CDN/#disadvantages","text":"CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN. Content might be stale if it is updated before the TTL expires it. CDNs require changing URLs for static content to point to the CDN.","title":"Disadvantages"},{"location":"System%20Design/Concepts/CDN/#live-streaming-cdn","text":"How does a live streaming CDN work? Step 1: Video capture Content creator captures the raw data or visual information using a camera. The data is represented in binary 1s and 0s in the device. Step 2: Segmentation Video file is broken down into smaller parts of a few seconds in length. Breaking them down into segments helps in streaming the entire video bit by bit. Step 3: Compression and encoding Each of the segments are compressed and encoded. Compressing removes redundant visual information such as a background that does not change in the video. This makes it easy to render just the moving frames in the video before streaming. Encoding is a process that is necessary to convert the data into a format that is compatible with the variety of devices that the end user consumes the content on. For example, H.264, HEVC, VP9 and AV1 are some of the popular formats that videos are encoding into. Step 4: Content Distribution and CDN Caching Next, the segmented, compressed and encoded video is distributed to end users. When the end user accesses a website or plays a video, their device (client) sends a request to the origin server to retrieve these files. Now if the users are located in close proximity to the server, or within a nearby region, this should not be a problem and the video files are streamed without much of an issue. In fact, if your viewership is small and they are not widely distributed, the single server can stream to all your users. There is no need to introduce more elements into your streaming workflow. But when the users are dispersed across a larger geographical area, in some cases across different countries, the round-trip-time for the server to deliver the content can be longer, resulting in delays or latency. This results in a below par user experience and one that is inconsistent across all of the video's consumers. Using a CDN solves this problem by caching the content in its distributed network of streaming servers. The CDN server closest to a particular end user will take care of delivering the content to that user. Step 5: Decoding and playback Once the video data reaches the users, their devices will decode and decompress the video segment by segment into the binary raw data. And with a video player, the user is able to see the visual information and play the video. Advantages Ensure sufficient bandwidth for high traffic periods Reduce latency and round-trip time Help in live streaming to a global audience Reduce the workload on origin servers","title":"Live Streaming CDN"},{"location":"System%20Design/Concepts/Cache/","text":"Cache Caching improves page load times and can reduce the load on your servers and databases. In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution. Databases often benefit from a uniform distribution of reads and writes across its partitions. Popular items can skew the distribution, causing bottlenecks. Putting a cache in front of a database can help absorb uneven loads and spikes in traffic. When to update the cache Since you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case. Cache-aside The application is responsible for reading and writing from storage. The cache does not interact with storage directly. The application does the following: Look for entry in cache, resulting in a cache miss Load entry from the database Add entry to cache Return entry Memcached is generally used in this manner. Subsequent reads of data added to cache are fast. Cache-aside is also referred to as lazy loading. Only requested data is cached, which avoids filling up the cache with data that isn't requested. Disadvantage(s): cache-aside Each cache miss results in three trips, which can cause a noticeable delay. Data can become stale if it is updated in the database. Need TTL or cache invalidation. When a node fails, it is replaced by a new, empty node, increasing latency. Write-through The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database: Application adds/updates entry in cache Cache synchronously writes entry to data store Return Application code: 1 set_user(12345, {\"foo\":\"bar\"}) Cache code: 1 2 3 def set_user(user_id, values): user = db.query(\"UPDATE Users WHERE id = {0}\", user_id, values) cache.set(user_id, user) Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast. Users are generally more tolerant of latency when updating data than reading data. Data in the cache is not stale. Disadvantage(s): write through When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database. Cache-aside in conjunction with write through can mitigate this issue. Most data written might never be read, which can be minimized with a TTL. Write-behind (write-back) In write-behind, the application does the following: Add/update entry in cache Asynchronously write entry to the data store, improving write performance Disadvantage(s): write-behind There could be data loss if the cache goes down prior to its contents hitting the data store. It is more complex to implement write-behind than it is to implement cache-aside or write-through. Disadvantage(s): cache Need to maintain consistency between caches and the source of truth such as the database through cache invalidation . Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache. Need to make application changes such as adding Redis or memcached.","title":"Cache"},{"location":"System%20Design/Concepts/Cache/#cache","text":"Caching improves page load times and can reduce the load on your servers and databases. In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution. Databases often benefit from a uniform distribution of reads and writes across its partitions. Popular items can skew the distribution, causing bottlenecks. Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.","title":"Cache"},{"location":"System%20Design/Concepts/Cache/#when-to-update-the-cache","text":"Since you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.","title":"When to update the cache"},{"location":"System%20Design/Concepts/Cache/#cache-aside","text":"The application is responsible for reading and writing from storage. The cache does not interact with storage directly. The application does the following: Look for entry in cache, resulting in a cache miss Load entry from the database Add entry to cache Return entry Memcached is generally used in this manner. Subsequent reads of data added to cache are fast. Cache-aside is also referred to as lazy loading. Only requested data is cached, which avoids filling up the cache with data that isn't requested.","title":"Cache-aside"},{"location":"System%20Design/Concepts/Cache/#write-through","text":"The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database: Application adds/updates entry in cache Cache synchronously writes entry to data store Return Application code: 1 set_user(12345, {\"foo\":\"bar\"}) Cache code: 1 2 3 def set_user(user_id, values): user = db.query(\"UPDATE Users WHERE id = {0}\", user_id, values) cache.set(user_id, user) Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast. Users are generally more tolerant of latency when updating data than reading data. Data in the cache is not stale.","title":"Write-through"},{"location":"System%20Design/Concepts/Cache/#write-behind-write-back","text":"In write-behind, the application does the following: Add/update entry in cache Asynchronously write entry to the data store, improving write performance","title":"Write-behind (write-back)"},{"location":"System%20Design/Concepts/Cache/#disadvantages-cache","text":"Need to maintain consistency between caches and the source of truth such as the database through cache invalidation . Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache. Need to make application changes such as adding Redis or memcached.","title":"Disadvantage(s): cache"},{"location":"System%20Design/Concepts/Communication/","text":"Communication HTTP HTTP is a method for encoding and transporting data between a client and a server. It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request. HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression. A basic HTTP request consists of a verb (method) and a resource (endpoint). Below are common HTTP verbs: Verb Description Idempotent* Safe Cacheable GET Reads a resource Yes Yes Yes POST Creates a resource or trigger a process that handles data No No Yes if response contains freshness info PUT Creates or replace a resource Yes No No PATCH Partially updates a resource No No Yes if response contains freshness info DELETE Deletes a resource Yes No No *Can be called many times without different outcomes. HTTP is an application layer protocol relying on lower-level protocols such as TCP and UDP . TCP TCP is a connection-oriented protocol over an IP network . Connection is established and terminated using a handshake . All packets sent are guaranteed to reach the destination in the original order and without corruption through: Sequence numbers and checksum fields for each packet Acknowledgement packets and automatic retransmission If the sender does not receive a correct response, it will resend the packets. If there are multiple timeouts, the connection is dropped. TCP also implements flow control and congestion control . These guarantees cause delays and generally result in less efficient transmission than UDP. To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage. It can be expensive to have a large number of open connections between web server threads and say, a memcached server. Connection pooling can help in addition to switching to UDP where applicable. TCP is useful for applications that require high reliability but are less time critical. Some examples include web servers, database info, SMTP, FTP, and SSH. Use TCP over UDP when: You need all of the data to arrive intact You want to automatically make a best estimate use of the network throughput UDP UDP is connectionless. Datagrams (analogous to packets) are guaranteed only at the datagram level. Datagrams might reach their destination out of order or not at all. UDP does not support congestion control. Without the guarantees that TCP support, UDP is generally more efficient. UDP can broadcast, sending datagrams to all devices on the subnet. This is useful with DHCP because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address. UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games. Use UDP over TCP when: You need the lowest latency Late data is worse than loss of data You want to implement your own error correction RPC In an RPC, a client causes a procedure to execute on a different address space, usually a remote server. The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program. Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls. Popular RPC frameworks include Protobuf , Thrift , and Avro . RPC is a request-response protocol: Client program - Calls the client stub procedure. The parameters are pushed onto the stack like a local procedure call. Client stub procedure - Marshals (packs) procedure id and arguments into a request message. Client communication module - OS sends the message from the client to the server. Server communication module - OS passes the incoming packets to the server stub procedure. Server stub procedure - Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments. The server response repeats the steps above in reverse order. HTTP APIs following REST tend to be used more often for public APIs. REST vs RPC API's Don't confuse RPC API with RPC connection. Consider the following example of HTTP APIs that model orders being placed in a restaurant. The RPC API thinks in terms of \"verbs\", exposing the restaurant functionality as function calls that accept parameters, and invokes these functions via the HTTP verb that seems most appropriate - a 'get' for a query, and so on, but the name of the verb is purely incidental and has no real bearing on the actual functionality, since you're calling a different URL each time . Return codes are hand-coded, and part of the service contract. The REST API , in contrast, models the various entities within the problem domain as resources, and uses HTTP verbs to represent transactions against these resources - POST to create, PUT to update, and GET to read. All of these verbs, invoked on the same URL , provide different functionality. Common HTTP return codes are used to convey status of the requests. Placing an Order: RPC: http://myrestaurant:8080/Orders/PlaceOrder (POST: {Tacos object}) REST: http://MyRestaurant:8080/Orders/Order?OrderNumber=asdf (POST: {Tacos object}) Retrieving an Order: RPC: http://MyRestaurant:8080/Orders/GetOrder?OrderNumber=asdf (GET) REST: http://myrestaurant:8080/Orders/Order?OrderNumber=asdf (GET) Updating an Order: RPC: http://MyRestaurant:8080/Orders/UpdateOrder (PUT: {Pineapple Tacos object}) REST: http://MyRestaurant:8080/Orders/Order?OrderNumber=asdf (PUT: {Pineapple Tacos object})","title":"Communication"},{"location":"System%20Design/Concepts/Communication/#communication","text":"","title":"Communication"},{"location":"System%20Design/Concepts/Communication/#http","text":"HTTP is a method for encoding and transporting data between a client and a server. It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request. HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression. A basic HTTP request consists of a verb (method) and a resource (endpoint). Below are common HTTP verbs: Verb Description Idempotent* Safe Cacheable GET Reads a resource Yes Yes Yes POST Creates a resource or trigger a process that handles data No No Yes if response contains freshness info PUT Creates or replace a resource Yes No No PATCH Partially updates a resource No No Yes if response contains freshness info DELETE Deletes a resource Yes No No *Can be called many times without different outcomes. HTTP is an application layer protocol relying on lower-level protocols such as TCP and UDP .","title":"HTTP"},{"location":"System%20Design/Concepts/Communication/#tcp","text":"TCP is a connection-oriented protocol over an IP network . Connection is established and terminated using a handshake . All packets sent are guaranteed to reach the destination in the original order and without corruption through: Sequence numbers and checksum fields for each packet Acknowledgement packets and automatic retransmission If the sender does not receive a correct response, it will resend the packets. If there are multiple timeouts, the connection is dropped. TCP also implements flow control and congestion control . These guarantees cause delays and generally result in less efficient transmission than UDP. To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage. It can be expensive to have a large number of open connections between web server threads and say, a memcached server. Connection pooling can help in addition to switching to UDP where applicable. TCP is useful for applications that require high reliability but are less time critical. Some examples include web servers, database info, SMTP, FTP, and SSH. Use TCP over UDP when: You need all of the data to arrive intact You want to automatically make a best estimate use of the network throughput","title":"TCP"},{"location":"System%20Design/Concepts/Communication/#udp","text":"UDP is connectionless. Datagrams (analogous to packets) are guaranteed only at the datagram level. Datagrams might reach their destination out of order or not at all. UDP does not support congestion control. Without the guarantees that TCP support, UDP is generally more efficient. UDP can broadcast, sending datagrams to all devices on the subnet. This is useful with DHCP because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address. UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games. Use UDP over TCP when: You need the lowest latency Late data is worse than loss of data You want to implement your own error correction","title":"UDP"},{"location":"System%20Design/Concepts/Communication/#rpc","text":"In an RPC, a client causes a procedure to execute on a different address space, usually a remote server. The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program. Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls. Popular RPC frameworks include Protobuf , Thrift , and Avro . RPC is a request-response protocol: Client program - Calls the client stub procedure. The parameters are pushed onto the stack like a local procedure call. Client stub procedure - Marshals (packs) procedure id and arguments into a request message. Client communication module - OS sends the message from the client to the server. Server communication module - OS passes the incoming packets to the server stub procedure. Server stub procedure - Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments. The server response repeats the steps above in reverse order. HTTP APIs following REST tend to be used more often for public APIs.","title":"RPC"},{"location":"System%20Design/Concepts/Communication/#rest-vs-rpc-apis","text":"Don't confuse RPC API with RPC connection. Consider the following example of HTTP APIs that model orders being placed in a restaurant. The RPC API thinks in terms of \"verbs\", exposing the restaurant functionality as function calls that accept parameters, and invokes these functions via the HTTP verb that seems most appropriate - a 'get' for a query, and so on, but the name of the verb is purely incidental and has no real bearing on the actual functionality, since you're calling a different URL each time . Return codes are hand-coded, and part of the service contract. The REST API , in contrast, models the various entities within the problem domain as resources, and uses HTTP verbs to represent transactions against these resources - POST to create, PUT to update, and GET to read. All of these verbs, invoked on the same URL , provide different functionality. Common HTTP return codes are used to convey status of the requests. Placing an Order: RPC: http://myrestaurant:8080/Orders/PlaceOrder (POST: {Tacos object}) REST: http://MyRestaurant:8080/Orders/Order?OrderNumber=asdf (POST: {Tacos object}) Retrieving an Order: RPC: http://MyRestaurant:8080/Orders/GetOrder?OrderNumber=asdf (GET) REST: http://myrestaurant:8080/Orders/Order?OrderNumber=asdf (GET) Updating an Order: RPC: http://MyRestaurant:8080/Orders/UpdateOrder (PUT: {Pineapple Tacos object}) REST: http://MyRestaurant:8080/Orders/Order?OrderNumber=asdf (PUT: {Pineapple Tacos object})","title":"REST vs RPC API's"},{"location":"System%20Design/Concepts/ConnectionTypes/","text":"Connection Types Push vs Pull Push has a consistent connection and server pushes messages to client. Pull is wasteful and not great latency wise. Using pull, you have an inherent tradeoff between UI freshness and server load. If you increase UI freshness, server will suffer. Try to use efficient data transfer type usch as Thrift instead of JSON. JSON repeats every field name for every record. Very inefficient for large amounts of data JSON more geared towards human readability at cost of making it more CPU intesnive. Thrift has better serialization performance. Thrift supports versioning. Web Sockets Provides full-duplex communication channels over a single TCP connection. Located at layer 7 and depends on TCP at layer 4. Pros: WebSockets offer bi-directional communication in realtime. Headers only sent once. Reduces data load sent to server. Binary and UTF-8. Cons: When connections are terminated WebSockets don\u2019t automatically recover. Can force client to close. Less overhead as connection only has to be established once. Enables two way data flow. MQTT over Websockets PubSub system over WebSockets. The broker is the MQTT server, and the connected devices are the clients. Neither the publisher nor the clients handle the legwork. Instead, the processing power and communications are mainly handled by the broker. MQTT protocol is incredibly lightweight and designed to connect even the most resource-constrained of devices. Low power, low bandwidth QualityOfService built in. Atleast once delivery. MQTT is bi-directional. Server Sent Events Like Websockets but only one-way communication from server to client. Connection stays active Pros: Transported over simple HTTP instead of a custom protocol Useful for apps that enable one-way communication of data, eg live stock prices Cons: SSE is subject to limitation with regards to the maximum number of open connections per browser. UTF-8 only, no binary data. Long Polling Client makes request to server and server holds connection open until new data is available to give to client. Usually closes after a tiemout. Once client receives a response, client sends another request. Less requests made from client Short Polling Client makes requests to server repeatedly at regualr interval to check for new data. Should rarely use this. Making repeated requests to server wastes resources each new incoming connection must be established HTTP headers must be passed a query for new data must be performed and a response (usually with no new data to offer) must be generated and delivered. The connection must be closed and any resources cleaned up.","title":"Connection Types"},{"location":"System%20Design/Concepts/ConnectionTypes/#connection-types","text":"","title":"Connection Types"},{"location":"System%20Design/Concepts/ConnectionTypes/#push-vs-pull","text":"Push has a consistent connection and server pushes messages to client. Pull is wasteful and not great latency wise. Using pull, you have an inherent tradeoff between UI freshness and server load. If you increase UI freshness, server will suffer. Try to use efficient data transfer type usch as Thrift instead of JSON. JSON repeats every field name for every record. Very inefficient for large amounts of data JSON more geared towards human readability at cost of making it more CPU intesnive. Thrift has better serialization performance. Thrift supports versioning.","title":"Push vs Pull"},{"location":"System%20Design/Concepts/ConnectionTypes/#web-sockets","text":"Provides full-duplex communication channels over a single TCP connection. Located at layer 7 and depends on TCP at layer 4. Pros: WebSockets offer bi-directional communication in realtime. Headers only sent once. Reduces data load sent to server. Binary and UTF-8. Cons: When connections are terminated WebSockets don\u2019t automatically recover. Can force client to close. Less overhead as connection only has to be established once. Enables two way data flow.","title":"Web Sockets"},{"location":"System%20Design/Concepts/ConnectionTypes/#mqtt-over-websockets","text":"PubSub system over WebSockets. The broker is the MQTT server, and the connected devices are the clients. Neither the publisher nor the clients handle the legwork. Instead, the processing power and communications are mainly handled by the broker. MQTT protocol is incredibly lightweight and designed to connect even the most resource-constrained of devices. Low power, low bandwidth QualityOfService built in. Atleast once delivery. MQTT is bi-directional.","title":"MQTT over Websockets"},{"location":"System%20Design/Concepts/ConnectionTypes/#server-sent-events","text":"Like Websockets but only one-way communication from server to client. Connection stays active Pros: Transported over simple HTTP instead of a custom protocol Useful for apps that enable one-way communication of data, eg live stock prices Cons: SSE is subject to limitation with regards to the maximum number of open connections per browser. UTF-8 only, no binary data.","title":"Server Sent Events"},{"location":"System%20Design/Concepts/ConnectionTypes/#long-polling","text":"Client makes request to server and server holds connection open until new data is available to give to client. Usually closes after a tiemout. Once client receives a response, client sends another request. Less requests made from client","title":"Long Polling"},{"location":"System%20Design/Concepts/ConnectionTypes/#short-polling","text":"Client makes requests to server repeatedly at regualr interval to check for new data. Should rarely use this. Making repeated requests to server wastes resources each new incoming connection must be established HTTP headers must be passed a query for new data must be performed and a response (usually with no new data to offer) must be generated and delivered. The connection must be closed and any resources cleaned up.","title":"Short Polling"},{"location":"System%20Design/Concepts/DNS/","text":"DNS A Domain Name System (DNS) translates a domain name such as www.example.com to an IP address. DNS is hierarchical, with a few authoritative servers at the top level. Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL) . NS record (name server) - Specifies the DNS servers for your domain/subdomain. MX record (mail exchange) - Specifies the mail servers for accepting messages. A record (address) - Points a name to an IP address. CNAME (canonical) - Points a name to another name or CNAME (example.com to www.example.com ) or to an A record. Services such as CloudFlare and Route 53 provide managed DNS services. Some DNS services can route traffic through various methods: Weighted round robin Prevent traffic from going to servers under maintenance Balance between varying cluster sizes A/B testing Latency-based Geolocation-based Disadvantage(s): DNS Accessing a DNS server introduces a slight delay, although mitigated by caching described above. DNS server management could be complex and is generally managed by governments, ISPs, and large companies . Deep Dive : CloudFlare There are 4 DNS servers involved in loading a webpage: DNS recursor - The recursor can be thought of as a librarian who is asked to go find a particular book somewhere in a library. The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client's DNS query. Root nameserver - The root server is the first step in translating (resolving) human readable host names into IP addresses. It can be thought of like an index in a library that points to different racks of books - typically it serves as a reference to other more specific locations. TLD nameserver - The top level domain server ( TLD ) can be thought of as a specific rack of books in a library. This nameserver is the next step in the search for a specific IP address, and it hosts the last portion of a hostname (In example.com, the TLD server is \"com\"). Authoritative nameserver - This final nameserver can be thought of as a dictionary on a rack of books, in which a specific name can be translated into its definition. The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor (the librarian) that made the initial request.","title":"DNS"},{"location":"System%20Design/Concepts/DNS/#dns","text":"A Domain Name System (DNS) translates a domain name such as www.example.com to an IP address. DNS is hierarchical, with a few authoritative servers at the top level. Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL) . NS record (name server) - Specifies the DNS servers for your domain/subdomain. MX record (mail exchange) - Specifies the mail servers for accepting messages. A record (address) - Points a name to an IP address. CNAME (canonical) - Points a name to another name or CNAME (example.com to www.example.com ) or to an A record. Services such as CloudFlare and Route 53 provide managed DNS services. Some DNS services can route traffic through various methods: Weighted round robin Prevent traffic from going to servers under maintenance Balance between varying cluster sizes A/B testing Latency-based Geolocation-based","title":"DNS"},{"location":"System%20Design/Concepts/DNS/#disadvantages-dns","text":"Accessing a DNS server introduces a slight delay, although mitigated by caching described above. DNS server management could be complex and is generally managed by governments, ISPs, and large companies .","title":"Disadvantage(s): DNS"},{"location":"System%20Design/Concepts/DNS/#deep-dive-cloudflare","text":"There are 4 DNS servers involved in loading a webpage: DNS recursor - The recursor can be thought of as a librarian who is asked to go find a particular book somewhere in a library. The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client's DNS query. Root nameserver - The root server is the first step in translating (resolving) human readable host names into IP addresses. It can be thought of like an index in a library that points to different racks of books - typically it serves as a reference to other more specific locations. TLD nameserver - The top level domain server ( TLD ) can be thought of as a specific rack of books in a library. This nameserver is the next step in the search for a specific IP address, and it hosts the last portion of a hostname (In example.com, the TLD server is \"com\"). Authoritative nameserver - This final nameserver can be thought of as a dictionary on a rack of books, in which a specific name can be translated into its definition. The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor (the librarian) that made the initial request.","title":"Deep Dive : CloudFlare"},{"location":"System%20Design/Concepts/Load%20Balancer/","text":"Load Balancer Load balancers distribute incoming client requests to computing resources such as application servers and databases. In each case, the load balancer returns the response from the computing resource to the appropriate client. Load balancers are effective at: Advantages Preventing requests from going to unhealthy servers Preventing overloading resources Helping to eliminate a single point of failure SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Session persistence - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions Disadvantages The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly. Introducing a load balancer to help eliminate a single point of failure results in increased complexity. A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity. Load balancers can be implemented with hardware (expensive) or with software such as HAProxy. To protect against failures, it's common to set up multiple load balancers, either in active-passive or active-active mode. Load balancers can route traffic based on various metrics, including: Random Least loaded Session/cookies Round robin or weighted round robin Layer 4 Layer 7 Layer 4 load balancing Layer 4 load balancers look at info at the transport layer to decide how to distribute requests. Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet. Layer 4 load balancers forward network packets to and from the upstream server, performing Network Address Translation (NAT) . Layer 7 load balancing Layer 7 load balancers look at the application layer to decide how to distribute requests. This can involve contents of the header, message, and cookies. Layer 7 load balancers terminate network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server. For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers. At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware. Horizontal scaling Load balancers can also help with horizontal scaling, improving performance and availability. Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called Vertical Scaling. It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems. Disadvantage(s): horizontal scaling Scaling horizontally introduces complexity and involves cloning servers Servers should be stateless: they should not contain any user-related data like sessions or profile pictures Sessions can be stored in a centralized data store such as a database (SQL, NoSQL) or a persistent cache (Redis, Memcached) Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out","title":"Load Balancer"},{"location":"System%20Design/Concepts/Load%20Balancer/#load-balancer","text":"Load balancers distribute incoming client requests to computing resources such as application servers and databases. In each case, the load balancer returns the response from the computing resource to the appropriate client. Load balancers are effective at:","title":"Load Balancer"},{"location":"System%20Design/Concepts/Load%20Balancer/#advantages","text":"Preventing requests from going to unhealthy servers Preventing overloading resources Helping to eliminate a single point of failure SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Session persistence - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions","title":"Advantages"},{"location":"System%20Design/Concepts/Load%20Balancer/#disadvantages","text":"The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly. Introducing a load balancer to help eliminate a single point of failure results in increased complexity. A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity. Load balancers can be implemented with hardware (expensive) or with software such as HAProxy. To protect against failures, it's common to set up multiple load balancers, either in active-passive or active-active mode. Load balancers can route traffic based on various metrics, including: Random Least loaded Session/cookies Round robin or weighted round robin Layer 4 Layer 7","title":"Disadvantages"},{"location":"System%20Design/Concepts/Load%20Balancer/#layer-4-load-balancing","text":"Layer 4 load balancers look at info at the transport layer to decide how to distribute requests. Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet. Layer 4 load balancers forward network packets to and from the upstream server, performing Network Address Translation (NAT) .","title":"Layer 4 load balancing"},{"location":"System%20Design/Concepts/Load%20Balancer/#layer-7-load-balancing","text":"Layer 7 load balancers look at the application layer to decide how to distribute requests. This can involve contents of the header, message, and cookies. Layer 7 load balancers terminate network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server. For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers. At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.","title":"Layer 7 load balancing"},{"location":"System%20Design/Concepts/Load%20Balancer/#horizontal-scaling","text":"Load balancers can also help with horizontal scaling, improving performance and availability. Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called Vertical Scaling. It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.","title":"Horizontal scaling"},{"location":"System%20Design/Concepts/Load%20Balancer/#disadvantages-horizontal-scaling","text":"Scaling horizontally introduces complexity and involves cloning servers Servers should be stateless: they should not contain any user-related data like sessions or profile pictures Sessions can be stored in a centralized data store such as a database (SQL, NoSQL) or a persistent cache (Redis, Memcached) Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out","title":"Disadvantage(s): horizontal scaling"},{"location":"System%20Design/Concepts/Reverse%20Proxy/","text":"Reverse Proxy A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public. Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client. Additional benefits include: Increased security - Hide information about backend servers, blacklist IPs, limit number of connections per client Increased scalability and flexibility - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Removes the need to install X.509 certificates on each server Compression - Compress server responses Caching - Return the response for cached requests Static content - Serve static content directly HTML/CSS/JS Photos Videos Etc Load balancer vs reverse proxy Deploying a load balancer is useful when you have multiple servers. Often, load balancers route traffic to a set of servers serving the same function. Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section. Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing. Disadvantage(s): reverse proxy Introducing a reverse proxy results in increased complexity. A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a failover ) further increases complexity.","title":"Reverse Proxy"},{"location":"System%20Design/Concepts/Reverse%20Proxy/#reverse-proxy","text":"A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public. Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client. Additional benefits include: Increased security - Hide information about backend servers, blacklist IPs, limit number of connections per client Increased scalability and flexibility - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Removes the need to install X.509 certificates on each server Compression - Compress server responses Caching - Return the response for cached requests Static content - Serve static content directly HTML/CSS/JS Photos Videos Etc","title":"Reverse Proxy"},{"location":"System%20Design/Concepts/Reverse%20Proxy/#load-balancer-vs-reverse-proxy","text":"Deploying a load balancer is useful when you have multiple servers. Often, load balancers route traffic to a set of servers serving the same function. Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section. Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing.","title":"Load balancer vs reverse proxy"},{"location":"System%20Design/Concepts/Reverse%20Proxy/#disadvantages-reverse-proxy","text":"Introducing a reverse proxy results in increased complexity. A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a failover ) further increases complexity.","title":"Disadvantage(s): reverse proxy"},{"location":"System%20Design/Concepts/Databases/HBase/","text":"HBase Facebook created Cassandra and it was purpose built for an inbox type application, but they found Cassandra's eventual consistency model wasn't a good match for their new real-time Messages product. Facebook also has an extensive MySQL infrastructure , but they found performance suffered as data set and indexes grew larger. And they could have built their own, but they chose HBase. HBase is a scaleout table store supporting very high rates of row-level updates over massive amounts of data . Exactly what is needed for a Messaging system. HBase is also a column based key-value store built on the BigTable model. It's good at fetching rows by key or scanning ranges of rows and filtering. Also what is needed for a Messaging system. Complex queries are not supported however. Facebook chose HBase because they monitored their usage and figured out what the really needed . What they needed was a system that could handle two types of data patterns: A short set of temporal data that tends to be volatile An ever-growing set of data that rarely gets accessed Has a simpler consistency model than Cassandra. Very good scalability and performance for their data patterns. Most feature rich for their requirements: auto load balancing and failover, compression support, multiple shards per server, etc. HDFS, the filesystem used by HBase, supports replication, end-to-end checksums, and automatic rebalancing. That's what HBase has achieved. Given how HBase covers a nice spot in the persistence spectrum--real-time, distributed, linearly scalable, robust, BigData, open-source, key-value, column-oriented--we should see it become even more popular, especially with its anointment by Facebook. Facebook move to MyRocks on MySQL To help improve Messenger even more, we now have overhauled and modernized the storage service to make it faster, more efficient, more reliable, and easier to upgrade with new features. This evolution involved several major changes: We redesigned and simplified the data schema, created a new source-of-truth index from existing data, and made consistent invariants to ensure that all data is formatted correctly. We moved from HBase, an open source distributed key-value store based on HDFS, to MyRocks , Facebook's open source database project that integrates RocksDB as a MySQL storage engine. We moved from storing the database on spinning disks to flash on our new Lightning Server SKU .","title":"HBase"},{"location":"System%20Design/Concepts/Databases/HBase/#hbase","text":"Facebook created Cassandra and it was purpose built for an inbox type application, but they found Cassandra's eventual consistency model wasn't a good match for their new real-time Messages product. Facebook also has an extensive MySQL infrastructure , but they found performance suffered as data set and indexes grew larger. And they could have built their own, but they chose HBase. HBase is a scaleout table store supporting very high rates of row-level updates over massive amounts of data . Exactly what is needed for a Messaging system. HBase is also a column based key-value store built on the BigTable model. It's good at fetching rows by key or scanning ranges of rows and filtering. Also what is needed for a Messaging system. Complex queries are not supported however. Facebook chose HBase because they monitored their usage and figured out what the really needed . What they needed was a system that could handle two types of data patterns: A short set of temporal data that tends to be volatile An ever-growing set of data that rarely gets accessed Has a simpler consistency model than Cassandra. Very good scalability and performance for their data patterns. Most feature rich for their requirements: auto load balancing and failover, compression support, multiple shards per server, etc. HDFS, the filesystem used by HBase, supports replication, end-to-end checksums, and automatic rebalancing. That's what HBase has achieved. Given how HBase covers a nice spot in the persistence spectrum--real-time, distributed, linearly scalable, robust, BigData, open-source, key-value, column-oriented--we should see it become even more popular, especially with its anointment by Facebook.","title":"HBase"},{"location":"System%20Design/Concepts/Databases/HBase/#facebook-move-to-myrocks-on-mysql","text":"To help improve Messenger even more, we now have overhauled and modernized the storage service to make it faster, more efficient, more reliable, and easier to upgrade with new features. This evolution involved several major changes: We redesigned and simplified the data schema, created a new source-of-truth index from existing data, and made consistent invariants to ensure that all data is formatted correctly. We moved from HBase, an open source distributed key-value store based on HDFS, to MyRocks , Facebook's open source database project that integrates RocksDB as a MySQL storage engine. We moved from storing the database on spinning disks to flash on our new Lightning Server SKU .","title":"Facebook move to MyRocks on MySQL"},{"location":"System%20Design/Concepts/Databases/MySQL%20Binlog-and-Replication/","text":"MySQL BinLog The binary log is a set of log files that contain information about data modifications made to a MySQL server instance. The log is enabled by starting the server with the --log-bin option. Facebook Example We used traditional asynchronous MySQL replication for cross region MySQL replication. However, for in-region fault tolerance, we created a middleware called Binlog Server (Log Backup Unit) which can retrieve and serve the MySQL replication logs known as Binary Logs. Binlog Servers only retain a short period of recent transaction logs and do not maintain a full copy of the database. Each MySQL instance replicates its log to two Binlog Servers using MySQL Semi-Synchronous protocol. All three servers are spread across different failure domains within the region. This architecture made it possible to achieve both short (in-region) commit latency and one database copy per region. We use MySQL's Binary Logs not only for MySQL Replication, but also for notifying updates to external applications. We created a pub-sub service called Wormhole [6] for this. One of the use cases of Wormhole is invalidating the TAO cache in remote regions by reading the Binary Log of the region's MySQL instance.","title":"MySQL BinLog"},{"location":"System%20Design/Concepts/Databases/MySQL%20Binlog-and-Replication/#mysql-binlog","text":"The binary log is a set of log files that contain information about data modifications made to a MySQL server instance. The log is enabled by starting the server with the --log-bin option.","title":"MySQL BinLog"},{"location":"System%20Design/Concepts/Databases/MySQL%20Binlog-and-Replication/#facebook-example","text":"We used traditional asynchronous MySQL replication for cross region MySQL replication. However, for in-region fault tolerance, we created a middleware called Binlog Server (Log Backup Unit) which can retrieve and serve the MySQL replication logs known as Binary Logs. Binlog Servers only retain a short period of recent transaction logs and do not maintain a full copy of the database. Each MySQL instance replicates its log to two Binlog Servers using MySQL Semi-Synchronous protocol. All three servers are spread across different failure domains within the region. This architecture made it possible to achieve both short (in-region) commit latency and one database copy per region. We use MySQL's Binary Logs not only for MySQL Replication, but also for notifying updates to external applications. We created a pub-sub service called Wormhole [6] for this. One of the use cases of Wormhole is invalidating the TAO cache in remote regions by reading the Binary Log of the region's MySQL instance.","title":"Facebook Example"},{"location":"System%20Design/Concepts/Databases/SQL%20or%20NoSQL/","text":"SQL vs NoSQL Reasons for SQL Structured/Relational data ACID properties Require multiple indexes Need for complex dynamic queries + analytics Writes scaled by sharding / Reads scaled by replication Automation - backup, restore, failover, replication (semi-sync, async, sync) MySQL auto-incrementing columns for primary keys (unique IDs) but MySQL can\u2019t guarantee uniqueness across physical and logical databases. Facebook There are many reasons why we use MySQL at Facebook. MySQL is amenable to automation, making it easy for a small team to manage thousands of MySQL servers while providing high-quality service. The automation includes backup, restore, failover, schema changes, and replacing failed hardware. MySQL also has flexible and extensible replication features, including asynchronous replication and lossless semi-synchronous replication, which makes it possible to do failover without losing data. Semisynchronous replication falls between asynchronous and fully synchronous replication. The source waits until at least one replica has received and logged the events (the required number of replicas is configurable), and then commits the transaction. The source does not wait for all replicas to acknowledge receipt, and it requires only an acknowledgement from the replicas, not that the events have been fully executed and committed on the replica side. Semisynchronous replication therefore guarantees that if the source crashes, all the transactions that it has committed have been transmitted to at least one replica. Facebook move to MyRocks on MySQL In the past, Facebook used InnoDB, a B+Tree based storage engine as the backend. The challenge was to find an index structure using less space and write amplification [1]. LSM-tree [2] has the potential to greatly improve these two bottlenecks. https://vldb.org/pvldb/vol13/p3217-matsunobu.pdf LSM vs B-Tree LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads. Reads are typically slower on LSM-tress as they have to check several different data structures and SSTables at different stages of compaction. The persistent data structure in InnoDB is the B+Tree on the primary key. In a few words, it means that the data is physically stored as B+Tree. It's well-known when we face a very high volume of inserts or random updates on a B+Tree (a heavy-write workload; for example, an event/tracking recording system), then there might be a trend to system degradation; because there is an overhead on the index maintenance while inserting rows in order to keep the tree as balanced as possible. Log-structured merge tree (or LSM-tree) is a data structure with performance characteristics very attractive on high-insertion scenarios. It's based on the principle that the most efficient operation on persistent storage (SSD, hard disks) is sequential access, so the strategy to improve the performance on write-heavy workloads is to append data similar to traditional logs, thus eliminating any type of slow random access. B Tree vs LSM Reasons for NoSQL Common types : Key-Value, Document, Wide-Column, Graph Dynamic schemas /Non-relational data No need for complex joins Easily scaled for writes / reads by simply adding more servers. Consistent hashing Example If you need to store a list, you can store it in both k/v and mysql. In k/v, you can have a key and the value is the list. It's great if the list doesn't change often and you often have to read the whole list. If you store as multiple rows in mysql, getting the whole list is more expensive but getting 1 row is easier, adding a row also easier, etc. There are many things to discuss here.","title":"SQL vs NoSQL"},{"location":"System%20Design/Concepts/Databases/SQL%20or%20NoSQL/#sql-vs-nosql","text":"","title":"SQL vs NoSQL"},{"location":"System%20Design/Concepts/Databases/SQL%20or%20NoSQL/#reasons-for-sql","text":"Structured/Relational data ACID properties Require multiple indexes Need for complex dynamic queries + analytics Writes scaled by sharding / Reads scaled by replication Automation - backup, restore, failover, replication (semi-sync, async, sync) MySQL auto-incrementing columns for primary keys (unique IDs) but MySQL can\u2019t guarantee uniqueness across physical and logical databases.","title":"Reasons for\u00a0SQL"},{"location":"System%20Design/Concepts/Databases/SQL%20or%20NoSQL/#facebook","text":"There are many reasons why we use MySQL at Facebook. MySQL is amenable to automation, making it easy for a small team to manage thousands of MySQL servers while providing high-quality service. The automation includes backup, restore, failover, schema changes, and replacing failed hardware. MySQL also has flexible and extensible replication features, including asynchronous replication and lossless semi-synchronous replication, which makes it possible to do failover without losing data. Semisynchronous replication falls between asynchronous and fully synchronous replication. The source waits until at least one replica has received and logged the events (the required number of replicas is configurable), and then commits the transaction. The source does not wait for all replicas to acknowledge receipt, and it requires only an acknowledgement from the replicas, not that the events have been fully executed and committed on the replica side. Semisynchronous replication therefore guarantees that if the source crashes, all the transactions that it has committed have been transmitted to at least one replica.","title":"Facebook"},{"location":"System%20Design/Concepts/Databases/SQL%20or%20NoSQL/#facebook-move-to-myrocks-on-mysql","text":"In the past, Facebook used InnoDB, a B+Tree based storage engine as the backend. The challenge was to find an index structure using less space and write amplification [1]. LSM-tree [2] has the potential to greatly improve these two bottlenecks. https://vldb.org/pvldb/vol13/p3217-matsunobu.pdf","title":"Facebook move to MyRocks on MySQL"},{"location":"System%20Design/Concepts/Databases/SQL%20or%20NoSQL/#reasons-for-nosql","text":"Common types : Key-Value, Document, Wide-Column, Graph Dynamic schemas /Non-relational data No need for complex joins Easily scaled for writes / reads by simply adding more servers. Consistent hashing","title":"Reasons for\u00a0NoSQL"},{"location":"System%20Design/Concepts/Databases/SQL%20or%20NoSQL/#example","text":"If you need to store a list, you can store it in both k/v and mysql. In k/v, you can have a key and the value is the list. It's great if the list doesn't change often and you often have to read the whole list. If you store as multiple rows in mysql, getting the whole list is more expensive but getting 1 row is easier, adding a row also easier, etc. There are many things to discuss here.","title":"Example"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20Detailed/","text":"Scaling SQL Detailed Replication and Sharding There are two basic strategies. Replication copies the entire dataset to another machine, so that the number of read queries can be doubled and so we have a hot standby of our data. Sharding (or partitioning) splits the dataset into multiple shards, and assigns the shards to different servers, so that the data writes and data size can be scaled beyond what a single server can handle. Replication allows more reads, but not more writes, since every write must be copied to every machine, a phenomenon called write amplification. Sharding allows more writes as well as reads, but risks losing data since the data is not stored redundantly and also makes it hard to do cross-shard queries and transactions. Often replication is combined with sharding. In this case we can handle more reads and writes, but also have the reliability advantage. 3 replicas per shard are usually enough. In this case we still have the downsides of write amplification and of difficulties with cross-shard queries. Leader Follower The simplest and most common way of replication is leader/follower. Here we have one database server which is the leader, and the others are the followers. All writes go to the leader, then get replicated out to the followers. This can be easily implemented by taking the WAL that the leader is already keeping and transmitting it to the other servers, where it is replayed. Oftentimes databases will keep a separate log for replication, because the WAL format is often overoptimized for disk writes. We can read from the followers to reduce load on the leader. The followers also are hot standbys and can replace a leader if it crashes. Problem #1: how do we avoid stale reads when it takes a short while for changes to replicate over? For example, are we sure we will read our own writes? Or, are we sure reads are monotonic (if we read from an older follower, might we see a value disappear that we just read from a more recent follower?) The only way to be sure everything is fresh is to read everything from the leader, but this nullifies the performance advantage. This problem is often punted over to the developer, who is responsible for making sure stale reads are not a risk to their code. Problem #2: when do we acknowledge the write back to the client? If we wait until the write is replicated (synchronous mode), we block and slow down, and risk timeouts if the other server suddenly becomes unavailable. But if we don\u2019t wait (asynchronous mode), we might not be able to read our own writes, and we risk losing unreplicated data when the leader crashes. Which brings us to\u2026 Problem #3: how to do failover without downtime? All clients send their writes to the leader, but what happens if the leader fails? We must appoint a new leader from the followers. However, how can we detect leader failure? We might use a heartbeat: if the leader stops sending a heartbeat signal, the followers hold an election and choose a new leader from the followers. However, the network may be partitioned, and the leader is still accepting writes, blissfully unaware it\u2019s no longer a leader, and now those writes will be discarded after it rejoins, while they were acknowledged to the client. Also, when the new leader was elected, some changes from the old leader might not have replicated over yet, those are now lost. In any case, while the new leader is being appointed there is no leader, so no writes can occur. In other words, the database is down (briefly). This is the approach most traditional RDBMS\u2019s use. We cannot scale writes using this approach. Going leaderless has its disadvantages, so is there a way to make leaders work and still scaling writes? One way of doing this is to shard the dataset across multiple replica sets. Many distributed databases work this way: MongoDB, HBase, Elasticsearch and Spanner (more on that later). Sharding Now the problem becomes one of how to shard the data across servers. If we want the database to do it automatically, there are two major strategies: Sharding by a range of the key. E.g. split the A-Z key space into A-L, M-Z. This can suffer from \u201chotspotting\u201d, where all writes end up on the same shard because they have a similar key (e.g. using an ISO timestamp as a key), which nullifies the benefits of sharding. Sharding by a range of the hash of the key. This spreads keys evenly across the shards, but comes with the downside of no longer being able to do range queries efficiently. In other words, as a developer you have to be aware of the sharding strategy and its consequences.","title":"Scaling SQL Detailed"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20Detailed/#scaling-sql-detailed","text":"","title":"Scaling SQL Detailed"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20Detailed/#replication-and-sharding","text":"There are two basic strategies. Replication copies the entire dataset to another machine, so that the number of read queries can be doubled and so we have a hot standby of our data. Sharding (or partitioning) splits the dataset into multiple shards, and assigns the shards to different servers, so that the data writes and data size can be scaled beyond what a single server can handle. Replication allows more reads, but not more writes, since every write must be copied to every machine, a phenomenon called write amplification. Sharding allows more writes as well as reads, but risks losing data since the data is not stored redundantly and also makes it hard to do cross-shard queries and transactions. Often replication is combined with sharding. In this case we can handle more reads and writes, but also have the reliability advantage. 3 replicas per shard are usually enough. In this case we still have the downsides of write amplification and of difficulties with cross-shard queries.","title":"Replication and Sharding"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20Detailed/#leader-follower","text":"The simplest and most common way of replication is leader/follower. Here we have one database server which is the leader, and the others are the followers. All writes go to the leader, then get replicated out to the followers. This can be easily implemented by taking the WAL that the leader is already keeping and transmitting it to the other servers, where it is replayed. Oftentimes databases will keep a separate log for replication, because the WAL format is often overoptimized for disk writes. We can read from the followers to reduce load on the leader. The followers also are hot standbys and can replace a leader if it crashes. Problem #1: how do we avoid stale reads when it takes a short while for changes to replicate over? For example, are we sure we will read our own writes? Or, are we sure reads are monotonic (if we read from an older follower, might we see a value disappear that we just read from a more recent follower?) The only way to be sure everything is fresh is to read everything from the leader, but this nullifies the performance advantage. This problem is often punted over to the developer, who is responsible for making sure stale reads are not a risk to their code. Problem #2: when do we acknowledge the write back to the client? If we wait until the write is replicated (synchronous mode), we block and slow down, and risk timeouts if the other server suddenly becomes unavailable. But if we don\u2019t wait (asynchronous mode), we might not be able to read our own writes, and we risk losing unreplicated data when the leader crashes. Which brings us to\u2026 Problem #3: how to do failover without downtime? All clients send their writes to the leader, but what happens if the leader fails? We must appoint a new leader from the followers. However, how can we detect leader failure? We might use a heartbeat: if the leader stops sending a heartbeat signal, the followers hold an election and choose a new leader from the followers. However, the network may be partitioned, and the leader is still accepting writes, blissfully unaware it\u2019s no longer a leader, and now those writes will be discarded after it rejoins, while they were acknowledged to the client. Also, when the new leader was elected, some changes from the old leader might not have replicated over yet, those are now lost. In any case, while the new leader is being appointed there is no leader, so no writes can occur. In other words, the database is down (briefly). This is the approach most traditional RDBMS\u2019s use. We cannot scale writes using this approach. Going leaderless has its disadvantages, so is there a way to make leaders work and still scaling writes? One way of doing this is to shard the dataset across multiple replica sets. Many distributed databases work this way: MongoDB, HBase, Elasticsearch and Spanner (more on that later).","title":"Leader Follower"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20Detailed/#sharding","text":"Now the problem becomes one of how to shard the data across servers. If we want the database to do it automatically, there are two major strategies: Sharding by a range of the key. E.g. split the A-Z key space into A-L, M-Z. This can suffer from \u201chotspotting\u201d, where all writes end up on the same shard because they have a similar key (e.g. using an ISO timestamp as a key), which nullifies the benefits of sharding. Sharding by a range of the hash of the key. This spreads keys evenly across the shards, but comes with the downside of no longer being able to do range queries efficiently. In other words, as a developer you have to be aware of the sharding strategy and its consequences.","title":"Sharding"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/","text":"Scaling SQL and NoSQL Vertical v.s. Horizontal Scaling Vertical scaling Vertical scaling is the process of improving the processing power of the server, by increasing its hardware capacity (e.g. CPU, RAM). While this could be a plausible option, there\u2019s a threshold to the hardware improvements \u2014 it\u2019s capped at what\u2019s currently available. In addition, after a certain point, vertical scaling becomes too expensive to be a viable option. Horizontal Scaling Horizontal scaling, on the other hand, achieves scale by increasing the number of servers. Theoretically, you could scale to have as many servers in parallel as you wish, which is why horizontal scaling is the preferred option when databases have to scale. As servers are distributed, we gain the benefits of being able to store more data, but we also inherit the problems of a distributed system. We\u2019ll look at the underlying limitations of scaling and how we can work around them. Database Sharding What is sharding? The concept of database sharding is key to scaling, and it applies to both SQL and NoSQL databases. As the name suggests, we\u2019re slicing up the database into multiple pieces (shards). Each shard has a unique index that corresponds to the type of data it stores. For example, if we choose to index our database by name, shard A can store the data of users with names that start with A\u2013F, shard B could store data from users ages G\u2013M, and so on. When a search query comes in, the database can quickly refer to the index and quickly pinpoint the shard we should look at instead of scanning the entire database. Costs of sharding However, it\u2019s essential to note that database sharding comes at a cost, especially for SQL databases. Monolithic databases like Oracle, MySQL, PostgreSQL do not support automatic sharding and engineers would have to manually write logic to handle the sharding. Often, because of the high cost of maintainability, changing schemas (e.g. how the databases are sharded) becomes challenging. Scaling SQL Databases Cluster proxy Assuming that we\u2019ve already applied the sharding logic to our SQL database, you might be asking, how would a query service know which shard to communicate with to retrieve data? If you\u2019re thinking about a middleman, you\u2019re on the right track. A solution is to place a proxy, like a load balancer, that sits in front of the query service and the database. Underneath the hood, the load balancer relies on a Configuration Service such as ZooKeeper to keep track of the shards and their indices. By processing the query, the load balancer will know exactly which shard to direct the request to. Shard proxy Thus far, we\u2019ve done a decent job at speeding up the query by looking at a specific shard instead of scanning the entire database \u2014 but we can still do better. Similar to what we\u2019ve done before, we can introduce another proxy that sits between the load balancer and the shard, which we\u2019ll call the shard proxy. If the shard is sufficiently large, we can once again shard that proxy to speed up the queries. This process of repeated sharding is called hierarchical sharding. Apart from redirecting to the specific index, a shard proxy can also further improve queries through actions like caching frequently used data, monitor database health, store and publish metrics about data, and terminate queries if a request is taking too long. Availability Now that we\u2019re no longer dealing with a monolithic database, we need to ensure that the data is still available when one instance goes down. In distributed systems, the solution to ensuring availability is to almost always introduce replication, so that there\u2019s no single bottleneck of failure and there will be backup copies available. Going back to the shards, we can ensure availability using a master-slave architecture. The idea is to have a master shard which is read/write and slave shards that are read-only, in order to ensure a single origin of truth. Whenever there is a write operation, it will be applied directly to the master shard, which will, in turn, propagate the changes to the slave shards. Read queries can be directed to the slave shards in order to reduce the load on the master shard. In the event that the master shard becomes unavailable, the shard proxy can choose a single slave shard to replace the master shard. At a data center level, we could replicate the database to ensure that we have a backup in the event that a data center fails. Consistency SQL databases were not designed with scalability in mind but with ACID properties (Atomicity, Consistency, Isolation, and Durability). As such, a single instance of a SQL database is guaranteed to be consistent. However, once you explore distributed systems with SQL databases and ensure the availability of data, you\u2019re bound to run into the issues of consistency which are inevitable in distributed systems. A simple analogy would be a factory owner making an announcement without a loudspeaker. If there are only a few workers, each of them would be able to instantly receive his message. Conversely, when there are thousands of workers, the message might take a while to reach the person at the back since each worker would have to rely on the person in front of them to convey them the message. Going back to the master-slave architecture, it will take some time for the data to be propagated from the master to its slaves. Therefore, it exists at a window of time that the master and its slaves can have different states. In scaling a SQL database, we sacrifice consistency for eventual consistency. Scaling NoSQL Databases Cassandra wide column dataBase Contrary to SQL databases, NoSQL databases were designed with scale in mind. Therefore, the scaling process is largely invisible to the end-user as it is done automatically. We\u2019ll nevertheless explore what\u2019s happening beneath the hood. While there are many different types of NoSQL databases and different ways of scaling, I will be using Cassandra\u2019s wide column databases as a reference. Shards/nodes are equal Similar to how we shard a SQL database, NoSQL also has shards of databases that correspond to different indices. However, one key difference is that instead of a master-slave architecture, every shard is equal. In addition, since there will be graph notions involved, I\u2019ll be referring to shards as nodes moving forward, as that\u2019s a more intuitive label. Nodes are able to communicate and exchange information about each other \u2014 they know where a particular data is stored, if the data is not contained within itself. Usually, there are a fixed number of nodes that each node communicates with. I imagine that this is to prevent a O(V\u00b2) time complexity, where V is the number of nodes, and to cap it at O(1) time. When a query comes in, an initial coordinator node is picked within the cluster to server the request via an algorithm. Such algorithms include round-robin, or shortest distance (from request to node). If the coordinator node does not have the data, it will forward the request to another node who has the data, or knows another node that has it. For example, the data for user F is stored on node A. Node D, the coordinator node, can simply communicate with node A, and knows that the data is stored on node A. Therefore, Node D can simply redirect the request to node A. Similarly, when a write request goes to a coordinator node, it will use consistent hashing to determine which node should the data be stored in. Gossip protocol The whole idea of sharing information through nodes is often referred to as the gossip protocol, where information is transferred from node to node. The term epidemic protocol is an alternative term, because the information is transmitted similarly to how viruses spread in a community. Availability As mentioned previously, data is replicated to prevent a single point of failure and maintain availability. In Cassandra, since the nodes are considered equals, a node can simply replicate its data in other nodes instead of employing a master-slave architecture. As we already know, when we have a distributed system, data replication happens asynchronously and takes a long time. Therefore, instead of waiting for all nodes to respond to the coordinator node signaling that a successful replication occurred, the coordinator node only needs to wait for x number of nodes to respond. This approach for waiting for x number of successful write responses for a replication to be successful is called quorum writes. Besides replication at the cluster level, multiple copies of the entire cluster are also stored across different data centers to prevent a single point of failure at the data center level. Consistency In most NoSQL databases, availability is prioritized over consistency since they have BASE (Basically Available, Soft State, Eventually Consistent) properties. In other words, it\u2019s preferred to show stale data than no data at all. After all, there will be eventual consistency amongst the nodes, as explained in the gossip protocol earlier. Given these properties, it would make sense that financial systems that require high precision of data would use SQL databases, whereas less \u201csignificant\u201d data like view counts could rely on NoSQL databases. When a read query goes to the coordinator nodes, which then initiates parallel read requests to replica nodes, it\u2019s possible that different responses are being returned. This discrepancy could be due to the fact that certain replica nodes were unavailable during the replication process, or that the data has yet to be propagated to some nodes. Just as we have quorum writes, we also have quorum reads, in which the response that x number of nodes agree on is returned.","title":"Scaling SQL and NoSQL"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#scaling-sql-and-nosql","text":"","title":"Scaling SQL and NoSQL"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#vertical-vs-horizontal-scaling","text":"","title":"Vertical v.s. Horizontal Scaling"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#vertical-scaling","text":"Vertical scaling is the process of improving the processing power of the server, by increasing its hardware capacity (e.g. CPU, RAM). While this could be a plausible option, there\u2019s a threshold to the hardware improvements \u2014 it\u2019s capped at what\u2019s currently available. In addition, after a certain point, vertical scaling becomes too expensive to be a viable option.","title":"Vertical scaling"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#horizontal-scaling","text":"Horizontal scaling, on the other hand, achieves scale by increasing the number of servers. Theoretically, you could scale to have as many servers in parallel as you wish, which is why horizontal scaling is the preferred option when databases have to scale. As servers are distributed, we gain the benefits of being able to store more data, but we also inherit the problems of a distributed system. We\u2019ll look at the underlying limitations of scaling and how we can work around them.","title":"Horizontal Scaling"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#database-sharding","text":"","title":"Database Sharding"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#what-is-sharding","text":"The concept of database sharding is key to scaling, and it applies to both SQL and NoSQL databases. As the name suggests, we\u2019re slicing up the database into multiple pieces (shards). Each shard has a unique index that corresponds to the type of data it stores. For example, if we choose to index our database by name, shard A can store the data of users with names that start with A\u2013F, shard B could store data from users ages G\u2013M, and so on. When a search query comes in, the database can quickly refer to the index and quickly pinpoint the shard we should look at instead of scanning the entire database.","title":"What is sharding?"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#costs-of-sharding","text":"However, it\u2019s essential to note that database sharding comes at a cost, especially for SQL databases. Monolithic databases like Oracle, MySQL, PostgreSQL do not support automatic sharding and engineers would have to manually write logic to handle the sharding. Often, because of the high cost of maintainability, changing schemas (e.g. how the databases are sharded) becomes challenging.","title":"Costs of sharding"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#scaling-sql-databases","text":"","title":"Scaling SQL Databases"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#cluster-proxy","text":"Assuming that we\u2019ve already applied the sharding logic to our SQL database, you might be asking, how would a query service know which shard to communicate with to retrieve data? If you\u2019re thinking about a middleman, you\u2019re on the right track. A solution is to place a proxy, like a load balancer, that sits in front of the query service and the database. Underneath the hood, the load balancer relies on a Configuration Service such as ZooKeeper to keep track of the shards and their indices. By processing the query, the load balancer will know exactly which shard to direct the request to.","title":"Cluster proxy"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#shard-proxy","text":"Thus far, we\u2019ve done a decent job at speeding up the query by looking at a specific shard instead of scanning the entire database \u2014 but we can still do better. Similar to what we\u2019ve done before, we can introduce another proxy that sits between the load balancer and the shard, which we\u2019ll call the shard proxy. If the shard is sufficiently large, we can once again shard that proxy to speed up the queries. This process of repeated sharding is called hierarchical sharding. Apart from redirecting to the specific index, a shard proxy can also further improve queries through actions like caching frequently used data, monitor database health, store and publish metrics about data, and terminate queries if a request is taking too long.","title":"Shard proxy"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#availability","text":"Now that we\u2019re no longer dealing with a monolithic database, we need to ensure that the data is still available when one instance goes down. In distributed systems, the solution to ensuring availability is to almost always introduce replication, so that there\u2019s no single bottleneck of failure and there will be backup copies available. Going back to the shards, we can ensure availability using a master-slave architecture. The idea is to have a master shard which is read/write and slave shards that are read-only, in order to ensure a single origin of truth. Whenever there is a write operation, it will be applied directly to the master shard, which will, in turn, propagate the changes to the slave shards. Read queries can be directed to the slave shards in order to reduce the load on the master shard. In the event that the master shard becomes unavailable, the shard proxy can choose a single slave shard to replace the master shard. At a data center level, we could replicate the database to ensure that we have a backup in the event that a data center fails.","title":"Availability"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#consistency","text":"SQL databases were not designed with scalability in mind but with ACID properties (Atomicity, Consistency, Isolation, and Durability). As such, a single instance of a SQL database is guaranteed to be consistent. However, once you explore distributed systems with SQL databases and ensure the availability of data, you\u2019re bound to run into the issues of consistency which are inevitable in distributed systems. A simple analogy would be a factory owner making an announcement without a loudspeaker. If there are only a few workers, each of them would be able to instantly receive his message. Conversely, when there are thousands of workers, the message might take a while to reach the person at the back since each worker would have to rely on the person in front of them to convey them the message. Going back to the master-slave architecture, it will take some time for the data to be propagated from the master to its slaves. Therefore, it exists at a window of time that the master and its slaves can have different states. In scaling a SQL database, we sacrifice consistency for eventual consistency.","title":"Consistency"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#scaling-nosql-databases","text":"","title":"Scaling NoSQL Databases"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#cassandra-wide-column-database","text":"Contrary to SQL databases, NoSQL databases were designed with scale in mind. Therefore, the scaling process is largely invisible to the end-user as it is done automatically. We\u2019ll nevertheless explore what\u2019s happening beneath the hood. While there are many different types of NoSQL databases and different ways of scaling, I will be using Cassandra\u2019s wide column databases as a reference.","title":"Cassandra wide column dataBase"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#shardsnodes-are-equal","text":"Similar to how we shard a SQL database, NoSQL also has shards of databases that correspond to different indices. However, one key difference is that instead of a master-slave architecture, every shard is equal. In addition, since there will be graph notions involved, I\u2019ll be referring to shards as nodes moving forward, as that\u2019s a more intuitive label. Nodes are able to communicate and exchange information about each other \u2014 they know where a particular data is stored, if the data is not contained within itself. Usually, there are a fixed number of nodes that each node communicates with. I imagine that this is to prevent a O(V\u00b2) time complexity, where V is the number of nodes, and to cap it at O(1) time. When a query comes in, an initial coordinator node is picked within the cluster to server the request via an algorithm. Such algorithms include round-robin, or shortest distance (from request to node). If the coordinator node does not have the data, it will forward the request to another node who has the data, or knows another node that has it. For example, the data for user F is stored on node A. Node D, the coordinator node, can simply communicate with node A, and knows that the data is stored on node A. Therefore, Node D can simply redirect the request to node A. Similarly, when a write request goes to a coordinator node, it will use consistent hashing to determine which node should the data be stored in.","title":"Shards/nodes are equal"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#gossip-protocol","text":"The whole idea of sharing information through nodes is often referred to as the gossip protocol, where information is transferred from node to node. The term epidemic protocol is an alternative term, because the information is transmitted similarly to how viruses spread in a community.","title":"Gossip protocol"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#availability_1","text":"As mentioned previously, data is replicated to prevent a single point of failure and maintain availability. In Cassandra, since the nodes are considered equals, a node can simply replicate its data in other nodes instead of employing a master-slave architecture. As we already know, when we have a distributed system, data replication happens asynchronously and takes a long time. Therefore, instead of waiting for all nodes to respond to the coordinator node signaling that a successful replication occurred, the coordinator node only needs to wait for x number of nodes to respond. This approach for waiting for x number of successful write responses for a replication to be successful is called quorum writes. Besides replication at the cluster level, multiple copies of the entire cluster are also stored across different data centers to prevent a single point of failure at the data center level.","title":"Availability"},{"location":"System%20Design/Concepts/Databases/Scaling%20SQL%20and%20NoSQL/#consistency_1","text":"In most NoSQL databases, availability is prioritized over consistency since they have BASE (Basically Available, Soft State, Eventually Consistent) properties. In other words, it\u2019s preferred to show stale data than no data at all. After all, there will be eventual consistency amongst the nodes, as explained in the gossip protocol earlier. Given these properties, it would make sense that financial systems that require high precision of data would use SQL databases, whereas less \u201csignificant\u201d data like view counts could rely on NoSQL databases. When a read query goes to the coordinator nodes, which then initiates parallel read requests to replica nodes, it\u2019s possible that different responses are being returned. This discrepancy could be due to the fact that certain replica nodes were unavailable during the replication process, or that the data has yet to be propagated to some nodes. Just as we have quorum writes, we also have quorum reads, in which the response that x number of nodes agree on is returned.","title":"Consistency"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/","text":"Back of Envelope Calculations Note: All the numbers in this post are heavily rounded as their purpose is to give a rough guide for design decisions in the moment. You should always do more precise calculations before starting on a project/feature. Byte Number Sizes The number of zeros after thousands increments by 3. 1 Byte = 8 Bits 1 KB = 10^3 Bytes 1 MB = 10^6 Bytes 1 GB = 10^9 Bytes 1 TB = 10^{12} Bytes 1 PB = 10^{15} Bytes Object Sizes Data The numbers vary depending on the language and implementation. char: 1B (8 bits) char (Unicode): 2B (16 bits) Short: 2B (16 bits) Int: 4B (32 bits) Long: 8B (64 bits) UUID/GUID: 16B Objects File: 100 KB Web Page: 100 KB (not including images) Picture: 200 KB Short Posted Video: 2MB Steaming Video: 50MB per minute Long/Lat: 8B Lengths Maximum URL Size: ~2000 (depends on browser) ASCII charset: 128 Unicode charset: 143, 859 Cost of Operations Read sequentially from HDD: 30 MB/s Read sequentially from SSD: 1 GB/s Read sequentially from memory: 4 GB/s Read sequentially from 1Gbps Ethernet: 100MB/s Cross continental network: 6--7 world-wide round trips per second. Systems These are not exact numbers, which very much depend on the implementation and what is hosting the service. The purpose of the numbers is to have a general idea of the performance across different types of services. SQL Databases Storage: 60TB Connections: 30K Requests: 25K per second Cache [ Redis --- Requests ][ Redis --- connections ] Storage: 300 GB Connections: 10k Requests: 100k per second Web Servers Requests: 5--10k requests per second Queues/Streams Requests: 1000--3000 requests/s Throughput: 1MB-50MB/s (Write) / 2MB-100MB/s (Read)","title":"Back of Envelope Calculations"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#back-of-envelope-calculations","text":"Note: All the numbers in this post are heavily rounded as their purpose is to give a rough guide for design decisions in the moment. You should always do more precise calculations before starting on a project/feature.","title":"Back of Envelope Calculations"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#byte-number-sizes","text":"The number of zeros after thousands increments by 3. 1 Byte = 8 Bits 1 KB = 10^3 Bytes 1 MB = 10^6 Bytes 1 GB = 10^9 Bytes 1 TB = 10^{12} Bytes 1 PB = 10^{15} Bytes","title":"Byte Number Sizes"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#object-sizes","text":"","title":"Object Sizes"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#data","text":"The numbers vary depending on the language and implementation. char: 1B (8 bits) char (Unicode): 2B (16 bits) Short: 2B (16 bits) Int: 4B (32 bits) Long: 8B (64 bits) UUID/GUID: 16B","title":"Data"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#objects","text":"File: 100 KB Web Page: 100 KB (not including images) Picture: 200 KB Short Posted Video: 2MB Steaming Video: 50MB per minute Long/Lat: 8B","title":"Objects"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#lengths","text":"Maximum URL Size: ~2000 (depends on browser) ASCII charset: 128 Unicode charset: 143, 859","title":"Lengths"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#cost-of-operations","text":"Read sequentially from HDD: 30 MB/s Read sequentially from SSD: 1 GB/s Read sequentially from memory: 4 GB/s Read sequentially from 1Gbps Ethernet: 100MB/s Cross continental network: 6--7 world-wide round trips per second.","title":"Cost of Operations"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#systems","text":"These are not exact numbers, which very much depend on the implementation and what is hosting the service. The purpose of the numbers is to have a general idea of the performance across different types of services.","title":"Systems"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#sql-databases","text":"Storage: 60TB Connections: 30K Requests: 25K per second","title":"SQL Databases"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#cache","text":"[ Redis --- Requests ][ Redis --- connections ] Storage: 300 GB Connections: 10k Requests: 100k per second","title":"Cache"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#web-servers","text":"Requests: 5--10k requests per second","title":"Web Servers"},{"location":"System%20Design/Concepts/Misc/backOfTheEnvelope/#queuesstreams","text":"Requests: 1000--3000 requests/s Throughput: 1MB-50MB/s (Write) / 2MB-100MB/s (Read)","title":"Queues/Streams"},{"location":"System%20Design/Concepts/Misc/encodingAndDecoding/","text":"Encoding And Decoding Data is made up of bits, 0 or 1. 8 bits = Byte. 256 distinct arrangements (2^8). ASCII is a way to map a byte to a character or a symbol. ASCII only has 7 bits, so only 128 possible characters. Extended ASCII was created to make it 8 bits. If network only uses 7 bits,we don't have a way to send our 8 bits over the network. Base64 addresses this problem. Base64 Basically, 3 blocks of 8 bits get converted into 4 blocks of 6 bits. Now we have 6 bit groups, 2^64, where base64 gets its name. Now for ease of explanation, each of these groups of 6 bits are turned into numbers and mapped to the index table. Now we have a 7 Bit friendly message SVG5 . 1 2 3 4 5 6 7 8 9 string = 'Hey' string_bytes = 'Hey' . encode ( 'utf-8' ) b 'Hey' b64_encoded_bytes = base64 . b64encode ( string_bytes ) b 'SGV5' b64_decoded_bytes = base64 . b64decode ( b64_encoded_bytes ) b 'Hey'","title":"Encoding And Decoding"},{"location":"System%20Design/Concepts/Misc/encodingAndDecoding/#encoding-and-decoding","text":"Data is made up of bits, 0 or 1. 8 bits = Byte. 256 distinct arrangements (2^8). ASCII is a way to map a byte to a character or a symbol. ASCII only has 7 bits, so only 128 possible characters. Extended ASCII was created to make it 8 bits. If network only uses 7 bits,we don't have a way to send our 8 bits over the network. Base64 addresses this problem.","title":"Encoding And Decoding"},{"location":"System%20Design/Concepts/Misc/encodingAndDecoding/#base64","text":"Basically, 3 blocks of 8 bits get converted into 4 blocks of 6 bits. Now we have 6 bit groups, 2^64, where base64 gets its name. Now for ease of explanation, each of these groups of 6 bits are turned into numbers and mapped to the index table. Now we have a 7 Bit friendly message SVG5 . 1 2 3 4 5 6 7 8 9 string = 'Hey' string_bytes = 'Hey' . encode ( 'utf-8' ) b 'Hey' b64_encoded_bytes = base64 . b64encode ( string_bytes ) b 'SGV5' b64_decoded_bytes = base64 . b64decode ( b64_encoded_bytes ) b 'Hey'","title":"Base64"},{"location":"System%20Design/Examples/dropbox/","text":"Dropbox Random notes about Dropbox architecture Sharded Metadata DDB Schema 1. PK: file_id 2. SK: version_id (version of folder containing item, not item itself!) 3. List<chunk_file_path> containing locations of each chunk in the current version in S3. 4. List<Hash of each chunk> containing hash of each chunk. Lets us find changes quickly. 5. Other attributes Versioning by storing 1 row for each file + version. Each row will have list of chunks Files stored in S3 at a chunk level Files should be stored in 4 Mb Chunks We should encrypt each block due to security reasons. Easier file uploads incase of failures We can easily find which chunks have changed by taking hash of each chunk and comparing whats in Metadata Table We can send update to user by comparing chunk list of their current version and the latest version in Metadata table and sending proper chunks to user. Short polling has alot of network overhead. Websockets optimal as there will be alot of We should store versionId at a folder level in metadata table instead of at an item level. This way we can lookup whether a device is up to date very fast and find which files need updating by doing a query on index folder_id_version_id > folder_id_X We can store device_id_folder_id -> version_id to have latest versionId per device per folder. This way push service can know if it even needs to push. We can also just do max of metadata table index folder_id_version_id Client Application The main components of the desktop client are Watcher, Chunker, Indexer, and Internal DB as described below. Watcher monitors the sync folders and notifies the Indexer of any action performed by the user for example when user create, delete, or update files or folders. Chunker splits the files into smaller pieces called chunks. To reconstruct a file, chunks will be joined back together in the correct order. A chunking algorithm can detect the parts of the files that have been modified by user and only transfer those parts to the Cloud Storage, saving on cloud storage space, bandwidth usage, and synchronization time. Indexer processes the events received from the Watcher and updates the internal database with information about the chunks of the modified files. Once the chunks are successfully submitted to the Cloud Storage, the Indexer will communicate with the Synchronization Service using the Message Queuing Service to update the Metadata Database with the changes. Internal Database keeps track of the chunks, files, their versions, and their location in the file system FROM SQL WEBSITE","title":"Dropbox"},{"location":"System%20Design/Examples/dropbox/#dropbox","text":"Random notes about Dropbox architecture Sharded Metadata DDB Schema 1. PK: file_id 2. SK: version_id (version of folder containing item, not item itself!) 3. List<chunk_file_path> containing locations of each chunk in the current version in S3. 4. List<Hash of each chunk> containing hash of each chunk. Lets us find changes quickly. 5. Other attributes Versioning by storing 1 row for each file + version. Each row will have list of chunks Files stored in S3 at a chunk level Files should be stored in 4 Mb Chunks We should encrypt each block due to security reasons. Easier file uploads incase of failures We can easily find which chunks have changed by taking hash of each chunk and comparing whats in Metadata Table We can send update to user by comparing chunk list of their current version and the latest version in Metadata table and sending proper chunks to user. Short polling has alot of network overhead. Websockets optimal as there will be alot of We should store versionId at a folder level in metadata table instead of at an item level. This way we can lookup whether a device is up to date very fast and find which files need updating by doing a query on index folder_id_version_id > folder_id_X We can store device_id_folder_id -> version_id to have latest versionId per device per folder. This way push service can know if it even needs to push. We can also just do max of metadata table index folder_id_version_id","title":"Dropbox"},{"location":"System%20Design/Examples/dropbox/#client-application","text":"The main components of the desktop client are Watcher, Chunker, Indexer, and Internal DB as described below. Watcher monitors the sync folders and notifies the Indexer of any action performed by the user for example when user create, delete, or update files or folders. Chunker splits the files into smaller pieces called chunks. To reconstruct a file, chunks will be joined back together in the correct order. A chunking algorithm can detect the parts of the files that have been modified by user and only transfer those parts to the Cloud Storage, saving on cloud storage space, bandwidth usage, and synchronization time. Indexer processes the events received from the Watcher and updates the internal database with information about the chunks of the modified files. Once the chunks are successfully submitted to the Cloud Storage, the Indexer will communicate with the Synchronization Service using the Message Queuing Service to update the Metadata Database with the changes. Internal Database keeps track of the chunks, files, their versions, and their location in the file system","title":"Client Application"},{"location":"System%20Design/Examples/dropbox/#from-sql-website","text":"","title":"FROM SQL WEBSITE"},{"location":"System%20Design/Examples/trendingTopics/","text":"Trending Topics Requirements Design \"Trending Topics\" System should be able to ingest hashtags What is the timespan for trending topics (1 day, 1 week, etc)? 24 hours Do we want real-time trending topics? (streaming or batching) Is it just by frequency of occurence, or are there otehr things like weights by influencers? Use cases: User views a page with a hashtag User navigates to a \"Trending topics\" page where hashtags with the most views during the last 24 hours can be found Functional/NonFunctional Requirements Searching and displaying topics by hashtag Storing/viewing the pages with hashtags Location specific trending topics Receives request: 1 2 3 4 GetTrendingTopics: Timestamp startTime; int periodSeconds; int count; GET /v1/topics/top?count=100&periodSeconds=3600 Processing service to update. Non-Functional Requirements Consistency vs availability service must be available [Weak/strong/eventual => consistency | Failover/replication => availability] Latency and Throughput requirements? Estimations 200M daily active users Average user 2 tweets per day -> 400M tweets per day 3E8/1E5 = 3E3 = ~3000 writes per second. Say peak is 10K QPS Say we have 10x more reads than writes (conservative) Peak load can have 100K QPS We have 120K read events per second to digest by our system Bucketsize functional requirements to microservices API Gateway Load balancing. SSL termination. Rate limiter. Receives requests from users and depending on geographical location and load redirects the request to stateless frontend service. Post Service The service allows users to save and view topics. It uses a cache to get the most recently viewed posts. When the service receives the request to view a topic, we can log this event to the message queue either async or we can use a log and publish this event using an external service running on the same machine. Message Queue Receives and delivers information about page views. The reason we want to use the message queue is that we don\u2019t want to affect performance of the topics service with a request that is not needed to get a response with post information. Stream Processing We can use Storm and implement a sliding window algorithm to compute the topics in real time. It aggregates and persists the top trending topics for the last 24 hours. It gets a hashId and a timestamp from the message queue, aggregates them and puts the aggregated trending topics to the database every N minutes (let's say 5 minutes). Trending Topics Database < Timestamp, Sorted List (HashtagId - Count) > Trending topics service Receives request: 1 2 3 4 GetTrendingTopics: Timestamp startTime; int periodSeconds; int count; It loads all data for this period from the database, aggregates it using a priority queue and returns a response with the list of hashtags and the number of views. HLD We keep the following key-value in the Post database: (Should be renamed hashtags) key:hashTagId - value:hashTagName In the trending streaming processor we use hashTagId to save some space. Later Trending topics service extract hashtag names from Post service by hashTagId. We can use Base64 with 8 letter length. 64^8 = (2^6)^8 = 2^48 = (2^10)^5 = (1E3)^5 = 1E15 = 1000T hashtags we can store. Purpose is to reduce # of bytes we send. Deep dive to Trending Service This is a depth oriented problem. First thing to clarify is whether we need trending topics for the last 24 hours (or whatever is reasonable time) or for all time using a formula to promote recent topics. Let's say we want to see trending topics only for the last 24 hours and we also want to keep the history of trending topics. Let\u2019s take a look at our streaming processing service. If we want to have the latest 100 trending topics, we may use a topology like this: Sliding window processor reads hashtags and timestamps from message queue and keeps a hashmap like: topicId -> array of timeslots In sliding windowk, data structure is Map For example, we could have 10 slots representing 30 seconds each. Every 30 seconds, sum of all slots for each hashId are aggregated and then pushed to intermediate aggregator service. Then latest slot is flushed. Therefore, every 30 seconds, we are reporting last 5 minutes of trending. We can increase granularity as needed. 30 1 second slots reporting 30 second data. Every 30 seconds we emit an aggregated count for each topic to an intermediate aggregator. We should use shuffling by topic (similar to map/reduce shuffling) so intermediate aggregators always receive information about the same topics. Because intermediate aggregators have all counts for the topic, they can use priority queue to keep only the top 100 topics. Every 30 seconds intermediate aggregators send information about 100 topics to the final aggregator that aggregates top 100 topics across all intermediate aggregators. It also persists the result in the database. There is only 1 final aggregator. Trending topics service. It receives the request from API Gateway to get top N (up to 100) trending topics for a given time period. It creates a range query for the given time period and aggregates the result. We can keep the result in cache for several seconds since the query can be expensive and it\u2019s okay to have a little stale data. Gather non functional / capacity requirements and check whether and how to scale each tier. 120K events per second. Let\u2019s say we may have 10K unique hashtags per day. Let\u2019s consider the Sliding window aggregator. It updates the counter in the hashmap and emits aggregated values every 30 seconds. Sliding Window Aggregators Memory Memory needed to create a sliding window for 10K unique hashtags (we don\u2019t keep old hashtags and remove them when we emit aggregated counters): hashTagId - long (8 bytes) Counter - integer (4 bytes), for 30 seconds we need 30 counters. 10K * 8 + 10K * 30 * 4 = 8E4 + 1.2E6 B We may also have hashmap and array overhead so it can be like 2E6 B = 2Mb. It's a pretty small number and the service will not be memory bound. Throughput Input data: a server has 200Gb of RAM, 10 cores, 4Tb storage. One event size is 8 bytes (timestamp) + 8 bytes (hashtag id) = 16 bytes. TCP/IP header is 120 bytes. Sliding window aggregators process all data in memory. They read events from the message queue and increment counters. We have 10 cores so we can process 10 events in parallel. Processing 1 event should take less than 1ms (lock/unlock 50 ns? + main memory reference ~6*100 ns + read 1k from main memory 250 ns = 1000ns = 1us). Instead of reading a single event at a time from the queue, we can read several events and batch process them: 100 * 1us = 100 us Reading from the network will take 20 us (1Kb over 1Gb network takes 10 us): 100 * 16 (100 view events) + 120(header) = 1720 ~=2K Total time to process 100 view events is 120 us. Single thread can process ~=800 events in 1ms and 800K events in 1 second. Sending counters to intermediate aggregator: Size = 1k hashtags * 8 bytes + 1k counters * 4 bytes = 12K = 120us. In real life there is also some additional overhead for garbage collection (if we use java), creating new sliding windows for new hashtags. Given that let\u2019s assume a single node can digest 100K events per second. Since we need redundancy, we can use 2 or 3 machines for sliding window aggregators. Intermediate aggregator Memory We keep a priority queue in memory that contains only the top 1000 hashtags and their counter. 1K tags * 8 bytes + 1k counters * 4 bytes = 12K Throughput The maximum intermediate aggregator may take 3 requests during 30 seconds. The size of the request is 12K and network latency could be 120us. The single node can process all view events. But we need at least 2 machines for redundancy. Final aggregator Memory The same memory size as the Intermediate aggregator. Throughput The same as for the Intermediate aggregator but we need to save the top 1000 topics in the database every 30 seconds. It adds an additional 50 ms to the throughput for every 30 seconds. The single node can process all requests.","title":"Trending Topics"},{"location":"System%20Design/Examples/trendingTopics/#trending-topics","text":"","title":"Trending Topics"},{"location":"System%20Design/Examples/trendingTopics/#requirements","text":"Design \"Trending Topics\" System should be able to ingest hashtags What is the timespan for trending topics (1 day, 1 week, etc)? 24 hours Do we want real-time trending topics? (streaming or batching) Is it just by frequency of occurence, or are there otehr things like weights by influencers? Use cases: User views a page with a hashtag User navigates to a \"Trending topics\" page where hashtags with the most views during the last 24 hours can be found","title":"Requirements"},{"location":"System%20Design/Examples/trendingTopics/#functionalnonfunctional-requirements","text":"Searching and displaying topics by hashtag Storing/viewing the pages with hashtags Location specific trending topics Receives request: 1 2 3 4 GetTrendingTopics: Timestamp startTime; int periodSeconds; int count; GET /v1/topics/top?count=100&periodSeconds=3600 Processing service to update.","title":"Functional/NonFunctional Requirements"},{"location":"System%20Design/Examples/trendingTopics/#non-functional-requirements","text":"Consistency vs availability service must be available [Weak/strong/eventual => consistency | Failover/replication => availability] Latency and Throughput requirements?","title":"Non-Functional Requirements"},{"location":"System%20Design/Examples/trendingTopics/#estimations","text":"200M daily active users Average user 2 tweets per day -> 400M tweets per day 3E8/1E5 = 3E3 = ~3000 writes per second. Say peak is 10K QPS Say we have 10x more reads than writes (conservative) Peak load can have 100K QPS We have 120K read events per second to digest by our system","title":"Estimations"},{"location":"System%20Design/Examples/trendingTopics/#bucketsize-functional-requirements-to-microservices","text":"API Gateway Load balancing. SSL termination. Rate limiter. Receives requests from users and depending on geographical location and load redirects the request to stateless frontend service. Post Service The service allows users to save and view topics. It uses a cache to get the most recently viewed posts. When the service receives the request to view a topic, we can log this event to the message queue either async or we can use a log and publish this event using an external service running on the same machine. Message Queue Receives and delivers information about page views. The reason we want to use the message queue is that we don\u2019t want to affect performance of the topics service with a request that is not needed to get a response with post information. Stream Processing We can use Storm and implement a sliding window algorithm to compute the topics in real time. It aggregates and persists the top trending topics for the last 24 hours. It gets a hashId and a timestamp from the message queue, aggregates them and puts the aggregated trending topics to the database every N minutes (let's say 5 minutes). Trending Topics Database < Timestamp, Sorted List (HashtagId - Count) > Trending topics service Receives request: 1 2 3 4 GetTrendingTopics: Timestamp startTime; int periodSeconds; int count; It loads all data for this period from the database, aggregates it using a priority queue and returns a response with the list of hashtags and the number of views.","title":"Bucketsize functional requirements to microservices"},{"location":"System%20Design/Examples/trendingTopics/#hld","text":"We keep the following key-value in the Post database: (Should be renamed hashtags) key:hashTagId - value:hashTagName In the trending streaming processor we use hashTagId to save some space. Later Trending topics service extract hashtag names from Post service by hashTagId. We can use Base64 with 8 letter length. 64^8 = (2^6)^8 = 2^48 = (2^10)^5 = (1E3)^5 = 1E15 = 1000T hashtags we can store. Purpose is to reduce # of bytes we send.","title":"HLD"},{"location":"System%20Design/Examples/trendingTopics/#deep-dive-to-trending-service","text":"This is a depth oriented problem. First thing to clarify is whether we need trending topics for the last 24 hours (or whatever is reasonable time) or for all time using a formula to promote recent topics. Let's say we want to see trending topics only for the last 24 hours and we also want to keep the history of trending topics. Let\u2019s take a look at our streaming processing service. If we want to have the latest 100 trending topics, we may use a topology like this: Sliding window processor reads hashtags and timestamps from message queue and keeps a hashmap like: topicId -> array of timeslots In sliding windowk, data structure is Map For example, we could have 10 slots representing 30 seconds each. Every 30 seconds, sum of all slots for each hashId are aggregated and then pushed to intermediate aggregator service. Then latest slot is flushed. Therefore, every 30 seconds, we are reporting last 5 minutes of trending. We can increase granularity as needed. 30 1 second slots reporting 30 second data. Every 30 seconds we emit an aggregated count for each topic to an intermediate aggregator. We should use shuffling by topic (similar to map/reduce shuffling) so intermediate aggregators always receive information about the same topics. Because intermediate aggregators have all counts for the topic, they can use priority queue to keep only the top 100 topics. Every 30 seconds intermediate aggregators send information about 100 topics to the final aggregator that aggregates top 100 topics across all intermediate aggregators. It also persists the result in the database. There is only 1 final aggregator. Trending topics service. It receives the request from API Gateway to get top N (up to 100) trending topics for a given time period. It creates a range query for the given time period and aggregates the result. We can keep the result in cache for several seconds since the query can be expensive and it\u2019s okay to have a little stale data.","title":"Deep dive to Trending Service"},{"location":"System%20Design/Examples/trendingTopics/#gather-non-functional-capacity-requirements-and-check-whether-and-how-to-scale-each-tier","text":"120K events per second. Let\u2019s say we may have 10K unique hashtags per day. Let\u2019s consider the Sliding window aggregator. It updates the counter in the hashmap and emits aggregated values every 30 seconds.","title":"Gather non functional / capacity requirements and check whether and how to scale each tier."},{"location":"System%20Design/Examples/trendingTopics/#sliding-window-aggregators","text":"Memory Memory needed to create a sliding window for 10K unique hashtags (we don\u2019t keep old hashtags and remove them when we emit aggregated counters): hashTagId - long (8 bytes) Counter - integer (4 bytes), for 30 seconds we need 30 counters. 10K * 8 + 10K * 30 * 4 = 8E4 + 1.2E6 B We may also have hashmap and array overhead so it can be like 2E6 B = 2Mb. It's a pretty small number and the service will not be memory bound. Throughput Input data: a server has 200Gb of RAM, 10 cores, 4Tb storage. One event size is 8 bytes (timestamp) + 8 bytes (hashtag id) = 16 bytes. TCP/IP header is 120 bytes. Sliding window aggregators process all data in memory. They read events from the message queue and increment counters. We have 10 cores so we can process 10 events in parallel. Processing 1 event should take less than 1ms (lock/unlock 50 ns? + main memory reference ~6*100 ns + read 1k from main memory 250 ns = 1000ns = 1us). Instead of reading a single event at a time from the queue, we can read several events and batch process them: 100 * 1us = 100 us Reading from the network will take 20 us (1Kb over 1Gb network takes 10 us): 100 * 16 (100 view events) + 120(header) = 1720 ~=2K Total time to process 100 view events is 120 us. Single thread can process ~=800 events in 1ms and 800K events in 1 second. Sending counters to intermediate aggregator: Size = 1k hashtags * 8 bytes + 1k counters * 4 bytes = 12K = 120us. In real life there is also some additional overhead for garbage collection (if we use java), creating new sliding windows for new hashtags. Given that let\u2019s assume a single node can digest 100K events per second. Since we need redundancy, we can use 2 or 3 machines for sliding window aggregators.","title":"Sliding Window Aggregators"},{"location":"System%20Design/Examples/trendingTopics/#intermediate-aggregator","text":"Memory We keep a priority queue in memory that contains only the top 1000 hashtags and their counter. 1K tags * 8 bytes + 1k counters * 4 bytes = 12K Throughput The maximum intermediate aggregator may take 3 requests during 30 seconds. The size of the request is 12K and network latency could be 120us. The single node can process all view events. But we need at least 2 machines for redundancy.","title":"Intermediate aggregator"},{"location":"System%20Design/Examples/trendingTopics/#final-aggregator","text":"Memory The same memory size as the Intermediate aggregator. Throughput The same as for the Intermediate aggregator but we need to save the top 1000 topics in the database every 30 seconds. It adds an additional 50 ms to the throughput for every 30 seconds. The single node can process all requests.","title":"Final aggregator"},{"location":"System%20Design/Real%20World%20Examples/Facebook/FacebookIris/","text":"Iris - Building Mobile-First Infrastructure for Messenger Clients The original protocol for getting data down to Messenger apps was pull-based. When receiving a message, the app first received a lightweight push notification indicating new data was available. This triggered the app to send the server a complicated HTTPS query and receive a very large JSON response with the updated conversation view. Instead of this model, we decided to move to a push-based snapshot + delta model. In this model, the client retrieves an initial snapshot of their messages (typically the only HTTPS pull ever made) and then subscribes to delta updates, which are immediately pushed to the app through MQTT (a low-power, low-bandwidth protocol) as messages are received. When the client is pushed an update, it simply applies them to its local copy of the snapshot. As a result, without ever making an HTTPS request, the app can quickly display an up-to-date view. We further optimized this flow by moving away from JSON to Thrift which allowed us to reduce our payload size on the wire by roughly 50%. Servers Messaging data has traditionally been stored on spinning disks. In the pull-based model, we\u2019d write to disk before sending a trigger to Messenger to read from disk. Thus, this giant storage tier would serve real-time message data as well as the full conversation history. One large storage tier doesn't scale well to synchronize recent messages to the app in real time. So in order to support this new, faster sync protocol and maintain consistency between the Messenger app and long-term storage, we need to be able to stream the same sequence of updates in real time to Messenger and to the storage tier in parallel on a per user basis. Iris is a totally ordered queue of messaging updates (new messages, state change for messages read, etc.) with separate pointers into the queue indicating the last update sent to your Messenger app and the traditional storage tier. When successfully sending a message to disk or to your phone, the corresponding pointer is advanced. When your phone is offline, or there is a disk outage, the pointer stays in place while new messages can still be enqueued and other pointers advanced. As a result, long disk write latencies don't hinder Messenger's real-time communication, and we can keep Messenger and the traditional storage tier in sync at independent rates. Effectively, this queue allows a tiered storage model based on recency: The most recent messages are immediately sent to online apps and to the disk storage tier from Iris's memory A week's worth of messages are served by the queue's backing store in the case of disk outage or the Messenger app being offline for a while Older conversation history and full inbox snapshot fetches are served from the traditional disk storage tier We opted to build the queue storage on top of MySQL and flash. For MySQL we decided to use semi-sync replication , which can give you durability across multiple servers. By leveraging this technology, we can handle database hardware failures in under 30 seconds, and the latency for enqueueing a new message is an order of magnitude less than writing to the traditional disk storage. Since we enqueue a message once to MySQL and then push it to apps and disk in parallel, Messenger receives messages faster and more reliably. [1] Lessons learned By sending less data and reducing HTTPS fetches, apps receive updates with lower latency and higher reliability. The network is a scarce resource that must be used as efficiently as possible. Every byte wasted has a very real impact on the experience of the application. Extending desktop-focused infrastructure for a mobile world could work well, but building new mobile first infrastructure with protocols designed for pushable devices offers even better experiences.","title":"Iris - Building Mobile-First Infrastructure for Messenger"},{"location":"System%20Design/Real%20World%20Examples/Facebook/FacebookIris/#iris-building-mobile-first-infrastructure-for-messenger","text":"","title":"Iris - Building Mobile-First Infrastructure for Messenger"},{"location":"System%20Design/Real%20World%20Examples/Facebook/FacebookIris/#clients","text":"The original protocol for getting data down to Messenger apps was pull-based. When receiving a message, the app first received a lightweight push notification indicating new data was available. This triggered the app to send the server a complicated HTTPS query and receive a very large JSON response with the updated conversation view. Instead of this model, we decided to move to a push-based snapshot + delta model. In this model, the client retrieves an initial snapshot of their messages (typically the only HTTPS pull ever made) and then subscribes to delta updates, which are immediately pushed to the app through MQTT (a low-power, low-bandwidth protocol) as messages are received. When the client is pushed an update, it simply applies them to its local copy of the snapshot. As a result, without ever making an HTTPS request, the app can quickly display an up-to-date view. We further optimized this flow by moving away from JSON to Thrift which allowed us to reduce our payload size on the wire by roughly 50%.","title":"Clients"},{"location":"System%20Design/Real%20World%20Examples/Facebook/FacebookIris/#servers","text":"Messaging data has traditionally been stored on spinning disks. In the pull-based model, we\u2019d write to disk before sending a trigger to Messenger to read from disk. Thus, this giant storage tier would serve real-time message data as well as the full conversation history. One large storage tier doesn't scale well to synchronize recent messages to the app in real time. So in order to support this new, faster sync protocol and maintain consistency between the Messenger app and long-term storage, we need to be able to stream the same sequence of updates in real time to Messenger and to the storage tier in parallel on a per user basis. Iris is a totally ordered queue of messaging updates (new messages, state change for messages read, etc.) with separate pointers into the queue indicating the last update sent to your Messenger app and the traditional storage tier. When successfully sending a message to disk or to your phone, the corresponding pointer is advanced. When your phone is offline, or there is a disk outage, the pointer stays in place while new messages can still be enqueued and other pointers advanced. As a result, long disk write latencies don't hinder Messenger's real-time communication, and we can keep Messenger and the traditional storage tier in sync at independent rates. Effectively, this queue allows a tiered storage model based on recency: The most recent messages are immediately sent to online apps and to the disk storage tier from Iris's memory A week's worth of messages are served by the queue's backing store in the case of disk outage or the Messenger app being offline for a while Older conversation history and full inbox snapshot fetches are served from the traditional disk storage tier We opted to build the queue storage on top of MySQL and flash. For MySQL we decided to use semi-sync replication , which can give you durability across multiple servers. By leveraging this technology, we can handle database hardware failures in under 30 seconds, and the latency for enqueueing a new message is an order of magnitude less than writing to the traditional disk storage. Since we enqueue a message once to MySQL and then push it to apps and disk in parallel, Messenger receives messages faster and more reliably. [1]","title":"Servers"},{"location":"System%20Design/Real%20World%20Examples/Facebook/FacebookIris/#lessons-learned","text":"By sending less data and reducing HTTPS fetches, apps receive updates with lower latency and higher reliability. The network is a scarce resource that must be used as efficiently as possible. Every byte wasted has a very real impact on the experience of the application. Extending desktop-focused infrastructure for a mobile world could work well, but building new mobile first infrastructure with protocols designed for pushable devices offers even better experiences.","title":"Lessons learned"},{"location":"System%20Design/Real%20World%20Examples/Facebook/Wormhole/","text":"Large-Scale Low-Latency Storage for the Social Network Messages Large historical dataset Mostly append data Private data Low amount of viewers Hbase + caches Hbase works well for write intensive workloads Graph Data Challenges Low MS resposne times < 1 MS Read your own writes 1000x reads to writes Highly interconnected data Fan out Worldwide scope PB of data TAO Distributed graph storage system Read and write through cache Many frontend and backend clusters in a region TAO serves as L1 and L2 cache If data not in TAO Follower, requests data from TAO Leader, which fetches from MySQL TAO followers offer redundant copy of data. Serves hottest data. Allows linearly scaling reads by simply adding more TAO followers. TAO leaders only have a single copy per region. Less accessed data but prevents load on database and thundering herds. TAO is basically write through cache Someone from remote region writing data. Follower -> Leader -> Main Leader -> Main SQL -> Async replication to Follower SQL Multiple requests in single RPG from Leaders in each region. Helps hide latency by batching. Leads into cache consistency model. Since it's write through, cache in posters cluster is immediately updated even before it reaches local database. Requires stickying user to cluster . Wormhole How does it handle worldwide cache invalidation? Pub-sub system Subscribe to certain events of data changes Tails MySQL binlog consuming SQL comments Also can be used for ETL to data warehouses. Conclusion Social graph data needs WW realtime access Distributed caching can hide latency issues But creates consistency solutions TAO/MySQL/Wormhole is FB solution","title":"Large-Scale Low-Latency Storage for the Social Network"},{"location":"System%20Design/Real%20World%20Examples/Facebook/Wormhole/#large-scale-low-latency-storage-for-the-social-network","text":"","title":"Large-Scale Low-Latency Storage for the Social Network"},{"location":"System%20Design/Real%20World%20Examples/Facebook/Wormhole/#messages","text":"Large historical dataset Mostly append data Private data Low amount of viewers Hbase + caches Hbase works well for write intensive workloads","title":"Messages"},{"location":"System%20Design/Real%20World%20Examples/Facebook/Wormhole/#graph-data-challenges","text":"Low MS resposne times < 1 MS Read your own writes 1000x reads to writes Highly interconnected data Fan out Worldwide scope PB of data","title":"Graph Data Challenges"},{"location":"System%20Design/Real%20World%20Examples/Facebook/Wormhole/#tao","text":"Distributed graph storage system Read and write through cache Many frontend and backend clusters in a region TAO serves as L1 and L2 cache If data not in TAO Follower, requests data from TAO Leader, which fetches from MySQL TAO followers offer redundant copy of data. Serves hottest data. Allows linearly scaling reads by simply adding more TAO followers. TAO leaders only have a single copy per region. Less accessed data but prevents load on database and thundering herds. TAO is basically write through cache Someone from remote region writing data. Follower -> Leader -> Main Leader -> Main SQL -> Async replication to Follower SQL Multiple requests in single RPG from Leaders in each region. Helps hide latency by batching. Leads into cache consistency model. Since it's write through, cache in posters cluster is immediately updated even before it reaches local database. Requires stickying user to cluster .","title":"TAO"},{"location":"System%20Design/Real%20World%20Examples/Facebook/Wormhole/#wormhole","text":"How does it handle worldwide cache invalidation? Pub-sub system Subscribe to certain events of data changes Tails MySQL binlog consuming SQL comments Also can be used for ETL to data warehouses.","title":"Wormhole"},{"location":"System%20Design/Real%20World%20Examples/Facebook/Wormhole/#conclusion","text":"Social graph data needs WW realtime access Distributed caching can hide latency issues But creates consistency solutions TAO/MySQL/Wormhole is FB solution","title":"Conclusion"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/","text":"LinkedIn Real-Time Interactions on Live Video How do likes/comments get distributed to all viewers? Challenge 1: The Delivery Pipeline Sender S sends like to Likes Backend with simple HTTP request. But how does Real time system commuicate with Receiver A? HTTP Long Polling and Server-side Events HTTP Long Polling Involves keeping an HTTP connection open until the server has data to push to the client. If when the request is made to the server there is no data available yet, the server prolongs the responds while the client waits. If during this time new data becomes available, it is sent to the client. When data is sent or when the request times out \u2014 whichever happens first \u2014 a new request is made to reestablish the connection. This is an overhead to keep creating new connections. Server-side events Websockets but only server -> client. If data needs to flow consistently from the server to the client and not the other way around (think: news feeds), a developer might use the EventSource interface to allow for server-sent events. Server-sent events open a single long-lived HTTP connection. The server then unidirectionally sends data when it has it, there is no need for the client to request it or do anything but wait for messages. Challenge 2: Connection Management LinkedIn utilizes Akka , a toolkit for building highly concurrent, distributed, and resilient message-driven applications. Akka actors are objects that have State and Behavior. Behavior determines how State should be modified when they receive messages. Each Actor has a mailbox and communicate exclusively by exchanging messages. An actor is assigned a lightweight thread every time there is a message to be processed. That thread will modify the state. Thread is now free to be assigned to next actor. In our case, each actor is managing one persisten connection. Challenge 3: Multiple Live Videos We need to ensure we don't send update to all the connections as they may not be watching the same video. We need a concept of which connection is related to which video. Challenge 4: 10K Concurrent Viewers Add a machine. Now we have more than 1 frontend server. We need a dispatcher to send events to them all. We can send the request to all frontend servers, but this is not efficient. Maybe only 1 server has a subscription related to the video being liked. Therefore we need to know which nodes are serving which videos. We can create another subscription table from video to node. Challenge 5: 100 Likes/second What's the bottleneck here? The dispatcher.If many events are published it may not keep up. But the current dispatcher nodes store the frontend server subscriptions in memory. Should each node update all the in memory subscriptions of all the dispatchers? No, make that video node subscription table distributed cache using Redis . Challenge 6: 100 Likes/s, 10K Viewers, Distribution of 1M Likes/s Subscription Flow: Viewer subscribes to frontend node. Frontend node stores the subscription in memory subscription table. Frontend subscribes to dispatcher nodes, since dispatcher needs to know which front-end nodes have connections that are subscribed to particular live video. Dispatcher stores this in distributed cache. Publish Flow: Viewer likes video. Request sent via HTTP requset to Likes Backend which stores them and sends request to any dispatcher node. Dispatcher node looks into distributed cache to find all nodes to forward request to. Frontend nodes have all connections related to that video in memory. Simply sends update to their subscribed clients. Bonus Challenge: Multiple Datacenters Dispatcher in DC1 must send request also to Dispatcher in DC2 and DC3.","title":"LinkedIn Real-Time Interactions on Live Video"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#linkedin-real-time-interactions-on-live-video","text":"How do likes/comments get distributed to all viewers?","title":"LinkedIn Real-Time Interactions on Live Video"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#challenge-1-the-delivery-pipeline","text":"Sender S sends like to Likes Backend with simple HTTP request. But how does Real time system commuicate with Receiver A? HTTP Long Polling and Server-side Events","title":"Challenge 1: The Delivery Pipeline"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#http-long-polling","text":"Involves keeping an HTTP connection open until the server has data to push to the client. If when the request is made to the server there is no data available yet, the server prolongs the responds while the client waits. If during this time new data becomes available, it is sent to the client. When data is sent or when the request times out \u2014 whichever happens first \u2014 a new request is made to reestablish the connection. This is an overhead to keep creating new connections.","title":"HTTP Long Polling"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#server-side-events","text":"Websockets but only server -> client. If data needs to flow consistently from the server to the client and not the other way around (think: news feeds), a developer might use the EventSource interface to allow for server-sent events. Server-sent events open a single long-lived HTTP connection. The server then unidirectionally sends data when it has it, there is no need for the client to request it or do anything but wait for messages.","title":"Server-side events"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#challenge-2-connection-management","text":"LinkedIn utilizes Akka , a toolkit for building highly concurrent, distributed, and resilient message-driven applications. Akka actors are objects that have State and Behavior. Behavior determines how State should be modified when they receive messages. Each Actor has a mailbox and communicate exclusively by exchanging messages. An actor is assigned a lightweight thread every time there is a message to be processed. That thread will modify the state. Thread is now free to be assigned to next actor. In our case, each actor is managing one persisten connection.","title":"Challenge 2: Connection Management"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#challenge-3-multiple-live-videos","text":"We need to ensure we don't send update to all the connections as they may not be watching the same video. We need a concept of which connection is related to which video.","title":"Challenge 3: Multiple Live Videos"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#challenge-4-10k-concurrent-viewers","text":"Add a machine. Now we have more than 1 frontend server. We need a dispatcher to send events to them all. We can send the request to all frontend servers, but this is not efficient. Maybe only 1 server has a subscription related to the video being liked. Therefore we need to know which nodes are serving which videos. We can create another subscription table from video to node.","title":"Challenge 4: 10K Concurrent Viewers"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#challenge-5-100-likessecond","text":"What's the bottleneck here? The dispatcher.If many events are published it may not keep up. But the current dispatcher nodes store the frontend server subscriptions in memory. Should each node update all the in memory subscriptions of all the dispatchers? No, make that video node subscription table distributed cache using Redis .","title":"Challenge 5: 100 Likes/second"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#challenge-6-100-likess-10k-viewers-distribution-of-1m-likess","text":"Subscription Flow: Viewer subscribes to frontend node. Frontend node stores the subscription in memory subscription table. Frontend subscribes to dispatcher nodes, since dispatcher needs to know which front-end nodes have connections that are subscribed to particular live video. Dispatcher stores this in distributed cache. Publish Flow: Viewer likes video. Request sent via HTTP requset to Likes Backend which stores them and sends request to any dispatcher node. Dispatcher node looks into distributed cache to find all nodes to forward request to. Frontend nodes have all connections related to that video in memory. Simply sends update to their subscribed clients.","title":"Challenge 6: 100 Likes/s, 10K Viewers, Distribution of 1M Likes/s"},{"location":"System%20Design/Real%20World%20Examples/LinkedIn/LinkedInRealTimeVideo/#bonus-challenge-multiple-datacenters","text":"Dispatcher in DC1 must send request also to Dispatcher in DC2 and DC3.","title":"Bonus Challenge: Multiple Datacenters"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/","text":"Scaling Push Messages ----------------; Pull method has tradeoff vs server health and UI freshness. Many unnecessary calls to server or sometimes not many enough. Zuul (Netflix notification stack) moved from pull to Websockets / ServerSentEvents Zuul Push Architecture Zuul Push Servers sit on network edge and accept incoming client connections Client connect to Zuul using Websockets/SSE. Once connected, connection is persistent Since we have multiple clients and multiple Zuul Push Servers, we need to keep track of which client is connected to which server. Push Registry handles this Push Library need a robust high throughput way to send messages to clients abstracted away from Zuul. Message Queue decouple senders and receivers making it easier independently. Allow wide variation in number of incoming messages. Act as buffer absorbing spikes. Message Processer reads messages from Message Queue and fetches client id of the message in the Push Registry to find what server to send the message to. Server then sends it to client Zuul Push Server 10M persistent always on connections. Connections are mostly idle. Based on Zuul Cloud Gateway (API Gateway for Netflix) Fronts all HTTP traffic in Netflix ecosystem. 1M req per second Non Blocking Sync I/O Each client that connects to Zuul Push Server needs to authenticate with Cookies/JWT/Custom scheme before receiving push notifications. Non-Blocking Sync I/O to support 10K+ connections per server C10K challenge is to support 10K concurrent connections on a single server. Netflix supports more than this on a server. But how? Traditional way of network programming cannot be scaled to meet C10K challenge. New thread per incoming connection. Let thread do blocking R/W on that connection. Doesn't scale because will exhaust servers memory allocating 10K stacks for 10K threads. Also CPU will be pinned down by constant context switches. Async I/O registers R/W callbacks for many open connections on a single thread. When any connection is ready to do R/W, corresponding callback is invoked on same thread. No longer need # threads == # connections. Scales better Tradeoff : Application more complicated. Need to keep track of all state of all connections inside code. Cannot rely on threadstack as its shared. Use Event/State machine in code. Netflix uses Netty to do Non-Blocking I/O. Used by Cassandra/Hadoop. Channel Inbound/Outbound handler are ~ R/W callbacks. Push Registry Redis used. Cassandra/Dynamo viable alternatives. Low read latency Write once per client but Read multiple times. Record expiry - TTL When client disconnects cleanly Push Server will take care of cleaning up record from Registry. Not all clients will disconnect cleanly. Clients/Servers crash -> Phantom records in registry. Uses TTL to clean records Sharding - High Availability Replication - Fault Tolerance Message Processing Different queues for differnet priorities. Multiple instances of message processer to scale throughput. Scale # of instances based on # of items in the queue dynamically. Operating Zuul Push : Lessons Long lived stable connections Great for client efficiency. Terrible for someone operating a server. Terrible for quick deploy/rollback. Say a few nodes in your cluster are now on a new version. How do we get clients to go to the new cluster? Kill the old cluster. But if we do that, all will swarm to new cluster resulting in Thundering Herd . Big spike in traffic. Solution : Limit client connection lifetime . Autoclose connections from server side and randomize each connection's lifetime. Without randomness, if there's network blip, many clients drop. Now they all reconnect at same time. If they all get same lifetime, they'll all disconnect/reconnect at same time resulting in recurring Thundering Herds. Randomization fixes this. Ask client to close its connection. According to TCP work, closer of connection ends up in time wait state. On linux, this can consume the connections file descriptor for upto 2 minutes. If client closes, servers file descriptor is not affected. If client doesn't comply, forcefully close from server. Server Size Initially chose largest possible size. When server died, resulted in cascading Thundering Herd. Used M4.Large 80gb RAM and 2 VCPUs = 24K connections. If few servers goes down, can handle thie increased traffic. Efficient operation != # of servers Autoscaled based on # of open connections. RPS/CPU not viable metrics since most connections are idle.","title":"Scaling Push Messages"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#scaling-push-messages","text":"----------------; Pull method has tradeoff vs server health and UI freshness. Many unnecessary calls to server or sometimes not many enough. Zuul (Netflix notification stack) moved from pull to Websockets / ServerSentEvents","title":"Scaling Push Messages"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#zuul-push-architecture","text":"Zuul Push Servers sit on network edge and accept incoming client connections Client connect to Zuul using Websockets/SSE. Once connected, connection is persistent Since we have multiple clients and multiple Zuul Push Servers, we need to keep track of which client is connected to which server. Push Registry handles this Push Library need a robust high throughput way to send messages to clients abstracted away from Zuul. Message Queue decouple senders and receivers making it easier independently. Allow wide variation in number of incoming messages. Act as buffer absorbing spikes. Message Processer reads messages from Message Queue and fetches client id of the message in the Push Registry to find what server to send the message to. Server then sends it to client","title":"Zuul Push Architecture"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#zuul-push-server","text":"10M persistent always on connections. Connections are mostly idle. Based on Zuul Cloud Gateway (API Gateway for Netflix) Fronts all HTTP traffic in Netflix ecosystem. 1M req per second Non Blocking Sync I/O Each client that connects to Zuul Push Server needs to authenticate with Cookies/JWT/Custom scheme before receiving push notifications.","title":"Zuul Push Server"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#non-blocking-sync-io-to-support-10k-connections-per-server","text":"C10K challenge is to support 10K concurrent connections on a single server. Netflix supports more than this on a server. But how? Traditional way of network programming cannot be scaled to meet C10K challenge. New thread per incoming connection. Let thread do blocking R/W on that connection. Doesn't scale because will exhaust servers memory allocating 10K stacks for 10K threads. Also CPU will be pinned down by constant context switches. Async I/O registers R/W callbacks for many open connections on a single thread. When any connection is ready to do R/W, corresponding callback is invoked on same thread. No longer need # threads == # connections. Scales better Tradeoff : Application more complicated. Need to keep track of all state of all connections inside code. Cannot rely on threadstack as its shared. Use Event/State machine in code. Netflix uses Netty to do Non-Blocking I/O. Used by Cassandra/Hadoop. Channel Inbound/Outbound handler are ~ R/W callbacks.","title":"Non-Blocking Sync I/O to support 10K+ connections per server"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#push-registry","text":"Redis used. Cassandra/Dynamo viable alternatives. Low read latency Write once per client but Read multiple times. Record expiry - TTL When client disconnects cleanly Push Server will take care of cleaning up record from Registry. Not all clients will disconnect cleanly. Clients/Servers crash -> Phantom records in registry. Uses TTL to clean records Sharding - High Availability Replication - Fault Tolerance","title":"Push Registry"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#message-processing","text":"Different queues for differnet priorities. Multiple instances of message processer to scale throughput. Scale # of instances based on # of items in the queue dynamically.","title":"Message Processing"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#operating-zuul-push-lessons","text":"","title":"Operating Zuul Push : Lessons"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#long-lived-stable-connections","text":"Great for client efficiency. Terrible for someone operating a server. Terrible for quick deploy/rollback. Say a few nodes in your cluster are now on a new version. How do we get clients to go to the new cluster? Kill the old cluster. But if we do that, all will swarm to new cluster resulting in Thundering Herd . Big spike in traffic. Solution : Limit client connection lifetime . Autoclose connections from server side and randomize each connection's lifetime. Without randomness, if there's network blip, many clients drop. Now they all reconnect at same time. If they all get same lifetime, they'll all disconnect/reconnect at same time resulting in recurring Thundering Herds. Randomization fixes this. Ask client to close its connection. According to TCP work, closer of connection ends up in time wait state. On linux, this can consume the connections file descriptor for upto 2 minutes. If client closes, servers file descriptor is not affected. If client doesn't comply, forcefully close from server.","title":"Long lived stable connections"},{"location":"System%20Design/Real%20World%20Examples/Netflix/ScalingPush/#server-size","text":"Initially chose largest possible size. When server died, resulted in cascading Thundering Herd. Used M4.Large 80gb RAM and 2 VCPUs = 24K connections. If few servers goes down, can handle thie increased traffic. Efficient operation != # of servers Autoscaled based on # of open connections. RPS/CPU not viable metrics since most connections are idle.","title":"Server Size"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/","text":"Distributed Cache Problem Statement A web application backed by a data store. This data store may be a database or another web service. Client makes a call to the web application, which in turn makes a call to the data store and result is returned back to the client. There may be several issues with this setup. First, calls to the data store may take a long time to execute or may utilize a lot of system resources. It would be good to store at least some results of these calls in memory, so that these results are retrieved and returned back to the client much faster. And if the data store is down or experiences a performance degradation and calls to the data store start to fail, our web application may still process requests as usual, at least for some period of time. So, storing data in memory will help to address these issues. When client request comes, we first check the cache and try to retrieve information from memory. And only if data is unavailable or stale, we then make a call to the datastore. And why do we call it a distributed cache? Because amount of data is too large to be stored in memory of a single machine and we need to split the data and store it across several machines. Caches are everywhere nowadays. Even on this channel, remember when we designed distributed message queue or notification service or rate limiter, all those designs relied on a cache of some sort. Functional Requirements put(key, value) : Stores object in the cache under some unique key get(key) : Retrieves object from the cache based on the key. Non-Functional Requirements Scalable (scales out easily together with increasing number of requests and data) : High scalability will help to ensure our cache can handle increased number of put and get requests. And be able to handle increasing amount of data we may need to store in the cache. Highly Available (tolerates hardware / network failures, no single point of failure) : High availability will help to ensure that data in the cache is not lost during hardware failures and cache is accessible in case of network partitions. This will minimize number of cache misses and as a result number of calls to the datastore. Highly Performant (fast put / get) : High performance is probably the number one requirement for the cache. The whole point of the cache is to be fast as it is called on every request. Interview Tip If you need to design a distributed system, think about the following 3 requirements first: Scalability Availability Performance And if data persistence is important think of durability as well. These 4 will give you and your interviewer a lot of room for discussion. If you recollect a CAP theorem, availability requirement may be replaced with consistency.Remember that interviewer is your friend and you both have the same goal. Your goal is to provide as many positive data points as possible. And the interviewer's goal is to collect as many data points as possible. In practice it means that you should start approaching any design problem with some small and simple steps. And evolve your solution with every next step. This is a win-win situation. You demonstrate progress, ability to deal with ambiguity and simplify things. While the interviewer gets all the necessary data points to bring them to the table and champion your case in the discussion with other interviewers from the interview loop. Local Cache Need a HashMap to add and retrieve key,value pairs in constant time. But when HashMap is full, we need to evict some old data. What data should be evicted? In short, HashMap doesn't know which entry has been used recently to create eviction policy. We need one more data structure to keep track of what was used when. Doubly Linked List LRU cache algorithm explanation When GET operation is called, we first check if this item is in the cache (hash table). If item is not in the cache, we return null immediately. If item is found, we need to move it to the head of the list and return the item back to the caller. And why do we need to move the item to the list head? To preserve the order of use. Item at the head of the list is the most recently used. And item at the tail of the list is the least recently used. So, when cache is full and we need to free space for a new item, we remove an item at the tail of the list. When PUT operation is called, we also check if item is in the cache. And if found, we update the item value and move the item to the head of the list. In case item is not in the cache, we need to check if cache is full. If cache has capacity (not full), we simply add this item to the hash table and to the list (at the head position). If cache is full, we need to free some space first. We take an item at the tail and remove it from both the hash table and the list. Now we have space for a new element and we add it to both the hash table and the list. Stepping into the distributed world Let's think how to make it distributed. We can start with a really straightforward idea, when we move the least recently used cache we just implemented to its own host. The benefit of this, we can now make each host to store only chunk of data, called shard. Because data is split across several hosts, we now can store much more data in memory. Service hosts know about all shards, and they forward put and get requests to a particular shard. The same idea, but a slightly different realization, is to use service hosts for the cache. We run cache as a separate process on a service host. And data is also split into shards. And similar to the first option, when service needs to make a call to the cache, it picks the shard that stores data and makes a call. Let's call these options as distributed cache cluster and co-located cache. And take a look at the benefits of each option. Dedicated cluster helps to isolate cache resources from the service resources. Both the cache and the service do not share memory and CPU anymore. And can scale on their own. Dedicated cluster can be used by multiple services. And we can utilize the same cluster across several microservices our team owns. And dedicated cluster also gives us flexibility in choosing hardware. We can choose hardware hosts with a lot of memory and high network bandwidth. Public clouds nowadays provide a variety of memory optimized hardware. As for co-located cache, the biggest benefit is that we do not need a separate cluster. This helps to save on hardware cost and usually less operationally intensive than a separate cluster. And with co-location, both the service and the cache scale out at the same time. We just add more hosts to the service cluster when needed. How do cache clients know which cache shard to call? Choosing A Cache Host A naive approach is to use MOD approach. HASH_FUNCTION(key) MOD NumberOfCacheHosts . But what happens when we add/remove hosts. Mod function will return completely different cache hosts for the same key than they did previously resulting in high percentage of cache misses which is not acceptable in production systems. A better option is to use Consistent Hashing with Virtual Nodes For each host, we calculate K replica Ids. Then you take these replica ids and generate K points on the ring with same hash function. For any request, take the hash and see where it falls on the ring. We walk the ring until we find the first host. Consistent hashing is much better than MOD hashing, as significantly smaller fraction of keys is re-hashed when new host is added or host is removed from the cache cluster. Now we know how cache cluster host is selected for both put and get, but who is actually doing this selection? On the service side, who is responsible for running all these hash calculations and routing requests to the selected cache host? Cache client is that component. It's a small and lightweight library, that is integrated with the service code and is responsible for the cache host selection. Cache client knows about all cache servers. And all clients should have the same list. Otherwise, different clients will have their own view of the consistent hashing circle and the same key may be routed to different cache hosts. Client stores list of cache hosts in sorted order (for a fast host lookup) and binary search can be used to find a cache server that owns the key. With consistent hashing, we are finding first value greater than hash of the key. Cache client talks to cache hosts using TCP or UDP protocol. And if cache host is unavailable, client proceeds as though it was a cache miss. Maintaing A List Of Cache Hosts As you may see, list of cache hosts is the most important knowledge for clients. And what we need to understand, is how this list is created, maintained and shared among all the clients. Let's discuss several options. In the first option we store a list of cache hosts in a file and deploy this file to service hosts using some continuous deployment pipeline. We can use configuration management tools such as chef and puppet to deploy file to every service host. This is the simplest option. But not very flexible. Every time list changes we need to make a code change and deploy it out to every service host. What if we keep the file, but simplify the deployment process? Specifically, we may put the file to the shared storage and make service hosts poll for the file periodically. This is exactly what the second option is about. All service hosts try to retrieve the file from some common location, for example S3 storage service. To implement this option, we may introduce a daemon process that runs on each service host and polls data from the storage once a minute or several minutes. The drawback of this approach is that we still need to maintain the file manually. Make changes and deploy it to the shared storage every time cache host dies or new host is added. It would be great if we could somehow monitor cache server health and if something bad happens to the cache server, all service hosts are notified and stop sending any requests to the unavailable cache server. And if a new cache server is added, all service hosts are also notified and start sending requests to it. To implement this approach, we will need a new service, configuration service, whose purpose is to discover cache hosts and monitor their health. In ZooKeeper , the cache clients can subscribe to this list and receive notification when it changes. Each cache server registers itself with the configuration service and sends heartbeats to the configuration service periodically. As long as heartbeats come, server is keep registered in the system. If heartbeats stop coming, the configuration service unregisters a cache server that is no longer alive or inaccessible. And every cache client grabs the list of registered cache servers from the configuration service. The third option is the hardest from implementation standpoint and its operational cost is higher. But it helps to fully automate the list maintenance. And in a couple of minutes you will see one other benefit of using configuration service for a distributed cache. Summary of what we have discussed so far To store more data in memory we partition data into shards. And put each shard on its own server. Every cache client knows about all cache shards. And cache clients use consistent hashing algorithm to pick a shard for storing and retrieving a particular cache key. Let's recall non-functional requirements we defined in the beginning of our design. We wanted to build fast, highly scalable and available distributed cache. Have we built a highly performant cache? Yes. Least recently used cache implementation uses constant time operations. Cache client picks cache server in log n time, very fast. And connection between cache client and cache server is done over TCP or UDP, also fast. So, performance is there. But what about other two: scalability and availability . Scalability is also there. We can easily create more shards and have more data stored in memory. Although those of you who did data sharding in real systems know that common problem for shards is that some of them may become hot. Meaning that some shards process much more requests then their peers. Resulting in a bottleneck. And adding more cache servers may not be very effective. With consistent hashing in place, a new cache server will further split some shard into two smaller shards. But we do not want to split any shard, we need to split requests for a particular key which will all go to the same shard. And high availability is not there at all. If some shard dies or becomes unavailable due to a network partition, all cache data for that shard is lost and all requests to that shard will result in a cache miss, until keys are re-hashed. Can you think of a mechanism that will help us to improve availability and better deal with a hot shard problem? And such mechanism is a data replication . There are many different techniques for data replication. We can distinguish two categories of data replication protocols. Achieving High Availability The first category includes a set of probabilistic protocols like gossip, epidemic broadcast trees, bimodal multicast. These protocols tend to favor eventual consistency . The second category includes consensus protocols such as 2 or 3 phase commit, paxos, raft, chain replication. These protocols tend to favor strong consistency . Let's keep things simple and use leader follower replication. For each shard we will designate a master cache server and several read replicas. Replicas (or followers) try to be an exact copy of the master. Every time the connection between master and replica breaks, replica attempts to automatically reconnect to the master. And replicas live in different data centers, so that cache data is still available when one data center is down. All put calls go through the master node, while get calls are handled by both master node and all the replicas. And because calls to a cache shard are now spread across several nodes, it is much easier to deal with hot shards. We may scale out by adding more read replicas. And while talking about leaders, we need to mention how these leaders are elected. There are two options: we can rely on a separate component, let's call it a Configuration service or if we want to avoid a separate component, we can implement leader election in the cache cluster. Configuration service is responsible for monitoring of both leaders and followers and failover, if some leader is not working as expected, configuration service can promote follower to leader. And as we discussed before, configuration service is a source of authority for clients. Cache clients use configuration service to discover all cache servers. Configuration service is a distributed service by its nature. It usually consists of an odd number of nodes (to achieve quorum easier), nodes are located on machines that fail independently (so that configuration service remains available in case for example network partitions) and all nodes talk to each other using TCP protocol. Zookeeper is a good candidate for a configuration service, we can use it here. Redis also implemented Redis Sentinel for this purpose. Ok, by introducing data replication we are able to better deal with hot shard problem and also increased availability. Increased, but we did not actually achieve true high availability. Why? Because there are still points of failure. We do data replication asynchronously, to have a better performance. We do not want to wait until leader sever replicates data to all the followers. And if leader server got some data and failed before this data was replicated by any of the followers, data is lost. And this is actually an acceptable behavior in many real-life use cases, when we deal with cache. The first priority of the cache is to be fast, and if it loses data in some rare scenarios, it should not be a big deal. This is just a cache miss and we should design our service in a way that such failures are expected. Other Interview Questions Distributed cache we built favors performance and availability over consistency. There are several things that lead to inconsistency. We replicate data asynchronously to have a better performance. So, a get call processed by the master node, may return a different result than a get call for the same key but processed by a read replica. Another potential source of inconsistency is when clients have a different list of cache servers. Cache servers may go down and go up again, and it is possible that a client write values that no other clients can read. Yes, we can fix these issues. Introduce synchronous replication. And make sure all clients share a single view of the cache servers list. But this will increase latency and overall complexity of the system. I highly encourage you to discuss these tradeoffs with your interviewer when you get a chance. Least recently used algorithm evicts data from cache when cache is full. But if cache is not full, some items may sit there for a long time. And such items may become stale. To address this issue, we may introduce some metadata for a cache entry and include time-to-live attribute. There are two common approaches how expired items are cleaned up from cache. We can passively expire an item, when some client tries to access it, and the item is found to be expired. Or we can actively expire, when we create a maintenance thread that runs at regular intervals and removes expired items. As there may be billions of items in the cache, we cannot simply iterate over all cache items. Usually, some probabilistic algorithms are used, when several random items are tested with every run. Services that use distributed (or remote) cache, often use local cache as well. If data is not found in the local cache, call to the distributed cache is initiated. To make life of service teams easier, so they do not need to deal with both caches, we can implement a support for the local cache inside the cache client. So, when cache client instance is created, we also construct a local cache. This way we hide all the complexity behind a single component - cache client. We can utilize previously introduced LRU cache implementation as a local cache, or use well-known 3-rd party implementations, for example Guava cache. Caches are optimized for maximum performance, as well as simplicity. And not optimized for security. Caches are usually accessed by trusted clients inside trusted environments and we should not expose cache servers directly to the internet, if it is not absolutely required. For these reasons we should use a firewall to restrict access to cache server ports and ensure only approved clients can access the cache. Clients may also encrypt data before storing it in cache and decrypt it on the way out. But we should expect performance implications. Our cache has to be instrumented with metrics and logging . This is especially important if we launch our distributed cache as a service. Because so many service teams in the organization may use our cache, every time those services experience performance degradation, they will come to us as one of the potential sources of these degradations. And we should be able to answer their questions. What metrics we may want to emit: number of faults while calling the cache, latency, number of hits and misses, CPU and memory utilization on cache hosts, network I/O. With regards to logging we may capture the details of every request to the cache. The basic information like who and when accessed the cache, what was the key and return status code. Log entries should be small, but useful. As you have seen cache client has many responsibilities: maintain a list of cache servers, pick a shard to route a request to, handle a remote call and any potential failures, emit metrics. Ideally, client software should be very simple, dumb if you want. And we can simplify the cache client. One idea is to introduce a proxy, that will sit between cache clients and cache servers and will be responsible for picking a cache shard. Take a look at the twemproxy project, created by Twitter. Another idea, is to make cache servers responsible for picking a shard. Client sends request to a random cache server and cache server applies consistent hashing (or some other partitioning algorithm) and redirects request to the shard that stores the data. This idea is utilized by Redis cluster. Consistent hashing algorithm is great. Simple and effective. But it has two major flaws: so called domino effect and the fact that cache servers do not split the circle evenly. Let's clarify this. Domino effect may appear when cache server dies. And all of its load is transferred to the next server. This transfer might overload the next server, and then that server would fail, causing a chain reaction of failures. And to understand the second problem, remember how we placed cache servers on the circle. Some servers may reside close to each other and some may be far apart. Causing uneven distribution of keys among the cache servers. To deal with these problems, several modifications of the consistent hashing algorithm have been introduced. One simple idea is to add each server on the circle multiple times. You can also read about Jump Hash algorithm (a paper published by Google in 2014) or proportional hashing (algorithm used by Yahoo! Video Platform). Recap We started with a single host and implemented least recently used cache. Because local cache has limited capacity and does not scale, we decided to run our LRU cache as a standalone process. Either on its own host or on the service host. And we made each process responsible for its own part of the data set. We introduced a consistent hashing ring, a logical structure that helps to assign owners for ranges of cache keys. And we introduced a cache client, that is responsible to routing requests for each key to a specific shard that stores data for this key. We could have stopped right there. These simple ideas proved to be very effective in practice. Memcached, which is an open source high-performance distributed cache is built on top of these principles. But we went further, as we wanted to improve availability of our cache and read scalability. And introduced master-slave data replication. For monitoring leaders and read replicas and to provide failover support, we brought in a configuration service. Which is also used by cache clients for discovering cache servers.","title":"Distributed Cache"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#distributed-cache","text":"","title":"Distributed Cache"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#problem-statement","text":"A web application backed by a data store. This data store may be a database or another web service. Client makes a call to the web application, which in turn makes a call to the data store and result is returned back to the client. There may be several issues with this setup. First, calls to the data store may take a long time to execute or may utilize a lot of system resources. It would be good to store at least some results of these calls in memory, so that these results are retrieved and returned back to the client much faster. And if the data store is down or experiences a performance degradation and calls to the data store start to fail, our web application may still process requests as usual, at least for some period of time. So, storing data in memory will help to address these issues. When client request comes, we first check the cache and try to retrieve information from memory. And only if data is unavailable or stale, we then make a call to the datastore. And why do we call it a distributed cache? Because amount of data is too large to be stored in memory of a single machine and we need to split the data and store it across several machines. Caches are everywhere nowadays. Even on this channel, remember when we designed distributed message queue or notification service or rate limiter, all those designs relied on a cache of some sort.","title":"Problem Statement"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#functional-requirements","text":"put(key, value) : Stores object in the cache under some unique key get(key) : Retrieves object from the cache based on the key.","title":"Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#non-functional-requirements","text":"Scalable (scales out easily together with increasing number of requests and data) : High scalability will help to ensure our cache can handle increased number of put and get requests. And be able to handle increasing amount of data we may need to store in the cache. Highly Available (tolerates hardware / network failures, no single point of failure) : High availability will help to ensure that data in the cache is not lost during hardware failures and cache is accessible in case of network partitions. This will minimize number of cache misses and as a result number of calls to the datastore. Highly Performant (fast put / get) : High performance is probably the number one requirement for the cache. The whole point of the cache is to be fast as it is called on every request.","title":"Non-Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#interview-tip","text":"If you need to design a distributed system, think about the following 3 requirements first: Scalability Availability Performance And if data persistence is important think of durability as well. These 4 will give you and your interviewer a lot of room for discussion. If you recollect a CAP theorem, availability requirement may be replaced with consistency.Remember that interviewer is your friend and you both have the same goal. Your goal is to provide as many positive data points as possible. And the interviewer's goal is to collect as many data points as possible. In practice it means that you should start approaching any design problem with some small and simple steps. And evolve your solution with every next step. This is a win-win situation. You demonstrate progress, ability to deal with ambiguity and simplify things. While the interviewer gets all the necessary data points to bring them to the table and champion your case in the discussion with other interviewers from the interview loop.","title":"Interview Tip"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#local-cache","text":"Need a HashMap to add and retrieve key,value pairs in constant time. But when HashMap is full, we need to evict some old data. What data should be evicted? In short, HashMap doesn't know which entry has been used recently to create eviction policy. We need one more data structure to keep track of what was used when. Doubly Linked List","title":"Local Cache"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#lru-cache-algorithm-explanation","text":"When GET operation is called, we first check if this item is in the cache (hash table). If item is not in the cache, we return null immediately. If item is found, we need to move it to the head of the list and return the item back to the caller. And why do we need to move the item to the list head? To preserve the order of use. Item at the head of the list is the most recently used. And item at the tail of the list is the least recently used. So, when cache is full and we need to free space for a new item, we remove an item at the tail of the list. When PUT operation is called, we also check if item is in the cache. And if found, we update the item value and move the item to the head of the list. In case item is not in the cache, we need to check if cache is full. If cache has capacity (not full), we simply add this item to the hash table and to the list (at the head position). If cache is full, we need to free some space first. We take an item at the tail and remove it from both the hash table and the list. Now we have space for a new element and we add it to both the hash table and the list.","title":"LRU cache algorithm explanation"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#stepping-into-the-distributed-world","text":"Let's think how to make it distributed. We can start with a really straightforward idea, when we move the least recently used cache we just implemented to its own host. The benefit of this, we can now make each host to store only chunk of data, called shard. Because data is split across several hosts, we now can store much more data in memory. Service hosts know about all shards, and they forward put and get requests to a particular shard. The same idea, but a slightly different realization, is to use service hosts for the cache. We run cache as a separate process on a service host. And data is also split into shards. And similar to the first option, when service needs to make a call to the cache, it picks the shard that stores data and makes a call. Let's call these options as distributed cache cluster and co-located cache. And take a look at the benefits of each option. Dedicated cluster helps to isolate cache resources from the service resources. Both the cache and the service do not share memory and CPU anymore. And can scale on their own. Dedicated cluster can be used by multiple services. And we can utilize the same cluster across several microservices our team owns. And dedicated cluster also gives us flexibility in choosing hardware. We can choose hardware hosts with a lot of memory and high network bandwidth. Public clouds nowadays provide a variety of memory optimized hardware. As for co-located cache, the biggest benefit is that we do not need a separate cluster. This helps to save on hardware cost and usually less operationally intensive than a separate cluster. And with co-location, both the service and the cache scale out at the same time. We just add more hosts to the service cluster when needed. How do cache clients know which cache shard to call?","title":"Stepping into the distributed world"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#choosing-a-cache-host","text":"A naive approach is to use MOD approach. HASH_FUNCTION(key) MOD NumberOfCacheHosts . But what happens when we add/remove hosts. Mod function will return completely different cache hosts for the same key than they did previously resulting in high percentage of cache misses which is not acceptable in production systems. A better option is to use Consistent Hashing with Virtual Nodes For each host, we calculate K replica Ids. Then you take these replica ids and generate K points on the ring with same hash function. For any request, take the hash and see where it falls on the ring. We walk the ring until we find the first host. Consistent hashing is much better than MOD hashing, as significantly smaller fraction of keys is re-hashed when new host is added or host is removed from the cache cluster. Now we know how cache cluster host is selected for both put and get, but who is actually doing this selection? On the service side, who is responsible for running all these hash calculations and routing requests to the selected cache host? Cache client is that component. It's a small and lightweight library, that is integrated with the service code and is responsible for the cache host selection. Cache client knows about all cache servers. And all clients should have the same list. Otherwise, different clients will have their own view of the consistent hashing circle and the same key may be routed to different cache hosts. Client stores list of cache hosts in sorted order (for a fast host lookup) and binary search can be used to find a cache server that owns the key. With consistent hashing, we are finding first value greater than hash of the key. Cache client talks to cache hosts using TCP or UDP protocol. And if cache host is unavailable, client proceeds as though it was a cache miss.","title":"Choosing A Cache Host"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#maintaing-a-list-of-cache-hosts","text":"As you may see, list of cache hosts is the most important knowledge for clients. And what we need to understand, is how this list is created, maintained and shared among all the clients. Let's discuss several options. In the first option we store a list of cache hosts in a file and deploy this file to service hosts using some continuous deployment pipeline. We can use configuration management tools such as chef and puppet to deploy file to every service host. This is the simplest option. But not very flexible. Every time list changes we need to make a code change and deploy it out to every service host. What if we keep the file, but simplify the deployment process? Specifically, we may put the file to the shared storage and make service hosts poll for the file periodically. This is exactly what the second option is about. All service hosts try to retrieve the file from some common location, for example S3 storage service. To implement this option, we may introduce a daemon process that runs on each service host and polls data from the storage once a minute or several minutes. The drawback of this approach is that we still need to maintain the file manually. Make changes and deploy it to the shared storage every time cache host dies or new host is added. It would be great if we could somehow monitor cache server health and if something bad happens to the cache server, all service hosts are notified and stop sending any requests to the unavailable cache server. And if a new cache server is added, all service hosts are also notified and start sending requests to it. To implement this approach, we will need a new service, configuration service, whose purpose is to discover cache hosts and monitor their health. In ZooKeeper , the cache clients can subscribe to this list and receive notification when it changes. Each cache server registers itself with the configuration service and sends heartbeats to the configuration service periodically. As long as heartbeats come, server is keep registered in the system. If heartbeats stop coming, the configuration service unregisters a cache server that is no longer alive or inaccessible. And every cache client grabs the list of registered cache servers from the configuration service. The third option is the hardest from implementation standpoint and its operational cost is higher. But it helps to fully automate the list maintenance. And in a couple of minutes you will see one other benefit of using configuration service for a distributed cache.","title":"Maintaing A List Of Cache Hosts"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#summary-of-what-we-have-discussed-so-far","text":"To store more data in memory we partition data into shards. And put each shard on its own server. Every cache client knows about all cache shards. And cache clients use consistent hashing algorithm to pick a shard for storing and retrieving a particular cache key. Let's recall non-functional requirements we defined in the beginning of our design. We wanted to build fast, highly scalable and available distributed cache. Have we built a highly performant cache? Yes. Least recently used cache implementation uses constant time operations. Cache client picks cache server in log n time, very fast. And connection between cache client and cache server is done over TCP or UDP, also fast. So, performance is there. But what about other two: scalability and availability . Scalability is also there. We can easily create more shards and have more data stored in memory. Although those of you who did data sharding in real systems know that common problem for shards is that some of them may become hot. Meaning that some shards process much more requests then their peers. Resulting in a bottleneck. And adding more cache servers may not be very effective. With consistent hashing in place, a new cache server will further split some shard into two smaller shards. But we do not want to split any shard, we need to split requests for a particular key which will all go to the same shard. And high availability is not there at all. If some shard dies or becomes unavailable due to a network partition, all cache data for that shard is lost and all requests to that shard will result in a cache miss, until keys are re-hashed. Can you think of a mechanism that will help us to improve availability and better deal with a hot shard problem? And such mechanism is a data replication . There are many different techniques for data replication. We can distinguish two categories of data replication protocols.","title":"Summary of what we have discussed so far"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#achieving-high-availability","text":"The first category includes a set of probabilistic protocols like gossip, epidemic broadcast trees, bimodal multicast. These protocols tend to favor eventual consistency . The second category includes consensus protocols such as 2 or 3 phase commit, paxos, raft, chain replication. These protocols tend to favor strong consistency . Let's keep things simple and use leader follower replication. For each shard we will designate a master cache server and several read replicas. Replicas (or followers) try to be an exact copy of the master. Every time the connection between master and replica breaks, replica attempts to automatically reconnect to the master. And replicas live in different data centers, so that cache data is still available when one data center is down. All put calls go through the master node, while get calls are handled by both master node and all the replicas. And because calls to a cache shard are now spread across several nodes, it is much easier to deal with hot shards. We may scale out by adding more read replicas. And while talking about leaders, we need to mention how these leaders are elected. There are two options: we can rely on a separate component, let's call it a Configuration service or if we want to avoid a separate component, we can implement leader election in the cache cluster. Configuration service is responsible for monitoring of both leaders and followers and failover, if some leader is not working as expected, configuration service can promote follower to leader. And as we discussed before, configuration service is a source of authority for clients. Cache clients use configuration service to discover all cache servers. Configuration service is a distributed service by its nature. It usually consists of an odd number of nodes (to achieve quorum easier), nodes are located on machines that fail independently (so that configuration service remains available in case for example network partitions) and all nodes talk to each other using TCP protocol. Zookeeper is a good candidate for a configuration service, we can use it here. Redis also implemented Redis Sentinel for this purpose. Ok, by introducing data replication we are able to better deal with hot shard problem and also increased availability. Increased, but we did not actually achieve true high availability. Why? Because there are still points of failure. We do data replication asynchronously, to have a better performance. We do not want to wait until leader sever replicates data to all the followers. And if leader server got some data and failed before this data was replicated by any of the followers, data is lost. And this is actually an acceptable behavior in many real-life use cases, when we deal with cache. The first priority of the cache is to be fast, and if it loses data in some rare scenarios, it should not be a big deal. This is just a cache miss and we should design our service in a way that such failures are expected.","title":"Achieving High Availability"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#other-interview-questions","text":"Distributed cache we built favors performance and availability over consistency. There are several things that lead to inconsistency. We replicate data asynchronously to have a better performance. So, a get call processed by the master node, may return a different result than a get call for the same key but processed by a read replica. Another potential source of inconsistency is when clients have a different list of cache servers. Cache servers may go down and go up again, and it is possible that a client write values that no other clients can read. Yes, we can fix these issues. Introduce synchronous replication. And make sure all clients share a single view of the cache servers list. But this will increase latency and overall complexity of the system. I highly encourage you to discuss these tradeoffs with your interviewer when you get a chance. Least recently used algorithm evicts data from cache when cache is full. But if cache is not full, some items may sit there for a long time. And such items may become stale. To address this issue, we may introduce some metadata for a cache entry and include time-to-live attribute. There are two common approaches how expired items are cleaned up from cache. We can passively expire an item, when some client tries to access it, and the item is found to be expired. Or we can actively expire, when we create a maintenance thread that runs at regular intervals and removes expired items. As there may be billions of items in the cache, we cannot simply iterate over all cache items. Usually, some probabilistic algorithms are used, when several random items are tested with every run. Services that use distributed (or remote) cache, often use local cache as well. If data is not found in the local cache, call to the distributed cache is initiated. To make life of service teams easier, so they do not need to deal with both caches, we can implement a support for the local cache inside the cache client. So, when cache client instance is created, we also construct a local cache. This way we hide all the complexity behind a single component - cache client. We can utilize previously introduced LRU cache implementation as a local cache, or use well-known 3-rd party implementations, for example Guava cache. Caches are optimized for maximum performance, as well as simplicity. And not optimized for security. Caches are usually accessed by trusted clients inside trusted environments and we should not expose cache servers directly to the internet, if it is not absolutely required. For these reasons we should use a firewall to restrict access to cache server ports and ensure only approved clients can access the cache. Clients may also encrypt data before storing it in cache and decrypt it on the way out. But we should expect performance implications. Our cache has to be instrumented with metrics and logging . This is especially important if we launch our distributed cache as a service. Because so many service teams in the organization may use our cache, every time those services experience performance degradation, they will come to us as one of the potential sources of these degradations. And we should be able to answer their questions. What metrics we may want to emit: number of faults while calling the cache, latency, number of hits and misses, CPU and memory utilization on cache hosts, network I/O. With regards to logging we may capture the details of every request to the cache. The basic information like who and when accessed the cache, what was the key and return status code. Log entries should be small, but useful. As you have seen cache client has many responsibilities: maintain a list of cache servers, pick a shard to route a request to, handle a remote call and any potential failures, emit metrics. Ideally, client software should be very simple, dumb if you want. And we can simplify the cache client. One idea is to introduce a proxy, that will sit between cache clients and cache servers and will be responsible for picking a cache shard. Take a look at the twemproxy project, created by Twitter. Another idea, is to make cache servers responsible for picking a shard. Client sends request to a random cache server and cache server applies consistent hashing (or some other partitioning algorithm) and redirects request to the shard that stores the data. This idea is utilized by Redis cluster. Consistent hashing algorithm is great. Simple and effective. But it has two major flaws: so called domino effect and the fact that cache servers do not split the circle evenly. Let's clarify this. Domino effect may appear when cache server dies. And all of its load is transferred to the next server. This transfer might overload the next server, and then that server would fail, causing a chain reaction of failures. And to understand the second problem, remember how we placed cache servers on the circle. Some servers may reside close to each other and some may be far apart. Causing uneven distribution of keys among the cache servers. To deal with these problems, several modifications of the consistent hashing algorithm have been introduced. One simple idea is to add each server on the circle multiple times. You can also read about Jump Hash algorithm (a paper published by Google in 2014) or proportional hashing (algorithm used by Yahoo! Video Platform).","title":"Other Interview Questions"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedCache/#recap","text":"We started with a single host and implemented least recently used cache. Because local cache has limited capacity and does not scale, we decided to run our LRU cache as a standalone process. Either on its own host or on the service host. And we made each process responsible for its own part of the data set. We introduced a consistent hashing ring, a logical structure that helps to assign owners for ranges of cache keys. And we introduced a cache client, that is responsible to routing requests for each key to a specific shard that stores data for this key. We could have stopped right there. These simple ideas proved to be very effective in practice. Memcached, which is an open source high-performance distributed cache is built on top of these principles. But we went further, as we wanted to improve availability of our cache and read scalability. And introduced master-slave data replication. For monitoring leaders and read replicas and to provide failover support, we brought in a configuration service. Which is also used by cache clients for discovering cache servers.","title":"Recap"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/","text":"Distributed Message Queue Synchronous vs Asynchronous Synchronous Communication When producer makes a call to a consumer, waits for a response. Easier and faster to implement. Harder to deal with consumer service failures. Need to think; When and how to properly retry failed requests? How not to overwhelm consumer service with too many requests? How to deal with a slow consumer service host? Asynchronous Communication Queue : Producer sends data to that component and exactly one consumer gets this data to a short time after. It is distributed, because data is stored across several machines. Do not confuse queue with topic. In case of a topic, message goes to all subscribers. In case of a queue, message is received by only one consumer. Functional Requirements sendMessage(messageBody) receiveMessage() Non-Functional Requirements Scalable (handles load increases, more queues and messages) Highly Available (survives hardware/network failures) Highly Performant (single digit latency for main operations) Durable (once submitted, data is persisted) High-level Architecture VIP : Virtual IP : Refers to the symbolic hostname (myWebService.domain.com) that resolves to a load balancer system. Load Balancer : A device that routes client requests across a number of servers. FrontEnd Web Service : A component responsible for initial request processing, like validation, authentication. Queue Metadata : Queue's name, creation date / time, owner and any other configuration settings will be stored in a DB. Metadata service : As a best practice, this metadata DB should be hidden behind some interface, a dedicated web service responsible for handling calls to that DB. BackEnd Web Service : Responsible for message persistence and processing. VIP and Load Balancer Load balancing is a big topic and unless interviewer asks to deep dive on it, try to stay on topic to main question of the interview. Internals may not matter, however to ensure that non-functional requirements to the system are met we need to explain how Load Balancer help us achieve high throughput and availability. When domain name is hit, request is transferred to one of the VIPs registered in DNS for our domain name. VIP is resolved to a load balancer device, which has a knowledge of FrontEnd hosts. Several Points: First, load balancer seems like a single point of failure. What happens if load balancer device goes down? Second, load balancers have limits with regards to number of requests they can process and number of bytes they can transfer. What happens when our distributed message queue service becomes so popular that load balancer limits are reached? To address high availability concerns, load balancers utilize a concept of primary and secondary nodes. The primary node accepts connections and serves requests while the secondary node monitors the primary. If, the primary node is unable to accept connections, the secondary node takes over. As for scalability concerns, a concept of multiple VIPs (sometimes referred as VIP partitioning) can be utilized. In DNS we assign multiple A-records to the same DNS name for the service. As a result, requests are partitioned across several load balancers. The \"A\" stands for \"address\" and this is the most fundamental type of DNS record: it indicates the IP address of a given domain . And by spreading load balancers across several data centers, we improve both availability and performance. FrontEnd Service FrontEnd is a lightweight web service, consisting of stateless machines located across several data centers. FrontEnd service is responsible for: Request validation : Helps to ensure that all the required parameters are present in the request and values of these parameters honor constraints. For example, in our case we want to make sure queue name comes with every send message request. And message size does not exceed a specified threshold. Authentication and authorization : During authentication check we verify that message sender is a registered customer of our distributed queue service. And during authorization check we verify that sender is allowed to publish messages to the queue it claims. TLS termination : TLS is a protocol that aims to provide privacy and data integrity. TLS termination refers to the process of decrypting request and passing on an unencrypted request to the backend service. And we want to do TLS termination on FrontEnd hosts because TLS on the load balancer is expensive. Termination is usually handled by not a FrontEnd service itself, but a separate HTTP proxy that runs as a process on the same host. Server-side data encryption : Because we want to store messages securely on backend hosts, messages are encrypted as soon as FrontEnd receives them. Messages are stored in encrypted form and FrontEnd decrypts them only when they are sent back to a consumer. Caching : Cache stores copies of source data. In FrontEnd cache we will store metadata information about the most actively used queues. As well as user identity information to save on calls to authentication and authorization services. Rate limiting (Throttling) : Rate limiting or throttling is the process of limiting the number of requests you can submit to a given operation in a given amount of time. Throttling protects the web service from being overwhelmed with requests. Leaky bucket algorithm is one of the most famous. Request dispatching : FrontEnd service makes remote calls to at least two other web services: Metadata service and backend service. FrontEnd service creates HTTP clients for both services and makes sure that calls to these services are properly isolated. It means that when one service let's say Metadata service experiences a slowdown, requests to backend service are not impacted. There are common patterns like bulkhead and circuit breaker that helps to implement resources isolation and make service more resilient in cases when remote calls start to fail. Request deduplication : It may occur when a response from a successful send message request failed to reach a client. Lesser an issue for 'at least once' delivery semantics, a bigger issue for 'exactly once' and 'at most once' delivery semantics, when we need to guarantee that message was never processed more than one time. Caching is usually used to store previously seen request ids to avoid deduplication. Usage data collection : When we gather real-time information that can be used for audit. And even though FrontEnd service has many responsibilities, the rule of thumb is to keep it as simple as possible. Metadata Service Metadata service stores information about queues. Every time queue is created, we store information about it in the database. Conceptually, Metadata service is a caching layer between the FrontEnd and a persistent storage. It handles many reads and a relatively small number of writes. As we read every time message arrives and write only when new queue is created. Even though strongly consistent storage is preferred to avoid potential concurrent updates, it is not strictly required. Different approaches of organizing cache clusters: The first option is when cache is relatively small and we can store the whole data set on every cluster node. FrontEnd host calls a randomly chosen Metadata service host, because all the cache cluster nodes contain the same information. Second approach is to partition data into small chunks, called shards. Because data set is too big and cannot be placed into a memory of a single host. So, we store each such chunk of data on a separate node in a cluster. FrontEnd then knows which shard stores the data and calls the shard directly. And the third option is similar to the second one. We also partition data into shards, but FrontEnd does not know on what shard data is stored. So, FrontEnd calls a random Metadata service host and host itself knows where to forward the request to. In option one, we can introduce a load balancer between FrontEnd and Metadata service. As all Metadata service hosts are equal and FrontEnd does not care which Metadata host handles the request. In option two and three, Metadata hosts represent a consistent hashing ring. Backend Service To understand how backend service architecture may look like, start asking the right questions. Where and how messages are stored? Is database an option? Yes but not the best one. We're building a distributed message queue, a system that needs to handle high throughput, which means all the throughput will be uploaded to the database. In other words, a problem of building a distributed message queue becomes a problem of developing a database that can handle high throughput. These kinds of databases exist out there. It's reasonable for a junior engineer to recommend to use a database. As we may need to store messages for days/weeks, we need more durable storage like local disk. Newly arrived messages may live in memory for a short period of time or until memory on the backend host is fully utilized. File system also works. How do we replicate data? We will send copies of messages to some other hosts, so that data can survive host hardware or software failures. How FrontEnd hosts select backend hosts for both storing messages and retrieving them. We can leverage Metadata service. Message comes to the FrontEnd, FrontEnd consults Metadata service what backend host to send data to. Message is sent to a selected backend host and data is replicated. When receive message call comes, FrontEnd talks to Metadata service to identify a backend host that stores the data. We will consider two options of how backend hosts relate to each other. Option A : Leader - Follower Relationship Each backend instance is considered a leader for a particular set of queues. And by leader we mean that all requests for a particular queue (like send message and receive message requests) go to this leader instance. Send message request comes to a FrontEnd instance. Message comes to a queue with ID equal to q1. FrontEnd service calls Metadata service to identify a leader backend instance for this queue. In this particular example, instance B is a leader for q1. Message is sent to the leader and the leader is fully responsible for data replication. When receive message request comes to a FrontEnd instance, it also makes a request to the Metadata service to identify the leader for the queue. Message is then retrieved from the leader instance and leader is responsible for cleaning up the original message and all the replicas. We need a component that will help us with leader election and management. Let's call it In-cluster manager. And as already mentioned, in-cluster manager is responsible for maintaining a mapping between queues, leaders and followers. In-cluster manager is a very sophisticated component. It has to be reliable, scalable and performant. Option B : Small cluster of independent hosts We have a set of small clusters, each cluster consists of 3-4 machines distributed across several data centers. When send message request comes, similar to the previous design option, we also need to call Metadata service to identify which cluster is responsible for storing messages for the q1 queue. After that we just make a call to a randomly selected instance in the cluster. And instance is responsible for data replication across all nodes in the cluster. When receive message request comes and we identified which cluster stores messages for the q1 queue, we once again call a randomly selected host and retrieve the message. Selected host is responsible for the message cleanup. As you may see, we no longer need a component for leader election, but we still need something that will help us to manage queue to cluster assignments. Let's call this component an Out-cluster manager. And this component will be responsible for maintaining a mapping between queues and clusters. In-cluster Manager vs Out-cluster Manager In-cluster manager manages queue assignment within the cluster, out-cluster manager manages queue assignment across clusters. In-cluster manager needs to know about each and every instance in the cluster. Out-cluster manager may not know about each particular instance, but it needs to know about each cluster. In-cluster manager listens to heartbeats from instances. Out-cluster manager monitors health of each independent cluster. In-cluster manager deals with host failures and needs to adjust to the fact that instances may die and new instances may be added to the cluster, out-cluster manager is responsible for tracking each cluster utilization and deal with overheated clusters. Meaning that new queues may no longer be assigned to clusters that reached their capacity limits. In-cluster manager splits queue into parts (partitions) and each partition gets a leader server. Out-cluster manager may split queue across several clusters. So that messages for the same queue are equally distributed between several clusters. What else is important? Queue creation and deletion Queue can be auto-created, for example when the first message for the queue hits FrontEnd service, or we can define API for queue creation. API is a better option, as we will have more control over queue configuration parameters. Delete queue operation is a bit controversial, as it may cause a lot of harm and must be executed with caution. For this reason, you may find examples of well-known distributed queues that do not expose deleteQueue API via public REST endpoint. Instead, this operation may be exposed through a command line utility, so that only experienced admin users may call it. Message deletion There are several options at our disposal. One option is not to delete a message right after it was consumed. In this case consumers have to be responsible for what they already consumed. And it is not as easy as it sounds. As we need to maintain some kind of an order for messages in the queue and keep track of the offset, which is the position of a message within a queue. Messages can then be deleted several days later, by a job. This idea is used by Apache Kafka. The second option, is to do something similar to what Amazon SQS is doing. Messages are also not deleted immediately, but marked as invisible (known as visibility timeout), so that other consumers may not get already retrieved message. Consumer that retrieved the message, needs to then call delete message API to delete the message from a backend host. And if the message was not explicitly deleted by a consumer, message becomes visible and may be delivered and processed twice. When a consumer consumes a message, a lease is granted for duration of visibility timeout. When consumer calls delete API, consumer must still hold the lease or else delete API will fail. Message replication Messages need to be replicated to achieve high durability. Otherwise, if we only have one copy of data, it may be lost due to unexpected hardware failure. Messages can be replicated synchronously or asynchronously. Synchronously means that when backend host receives new message, it waits until data is replicated to other hosts. And only if replication is fully completed, successful response is returned to a producer. Asynchronous replication means that response is returned back to a producer as soon as message is stored on a single backend host. In this case we will need to store data in log file incase of system crash. Message is later replicated to other hosts. Both options have pros and cons. Synchronous replication provides higher durability, but with a cost of higher latency for send message operation. Asynchronous replication is more performant, but does not guarantee that message will survive backend host failure. Message delivery semantics There are three main message delivery guarantees. At most once, when messages may be lost but are never redelivered. At least once, when messages are never lost but may be redelivered. And exactly once, when each message is delivered once and only once. Will anyone ever want other than exactly once delivery? The simple answer is that it is hard to achieve exactly once delivery in practice. In a distributed message queue system there are many potential points of failure. Producer may fail to deliver or deliver multiple times, data replication may fail, consumers may fail to retrieve or process the message. All this adds complexity and leads to the fact that most distributed queue solutions today support at-least-once delivery, as it provides a good balance between durability, availability and performance. Push vs Pull With a pull model, consumer constantly sends retrieve message requests and when new message is available in the queue, it is sent back to a consumer. With a push model, consumer is not constantly bombarding FrontEnd service with receive calls. Instead, consumer is notified as soon as new message arrives to the queue. And as always, there are pros and cons. From a distributed message queue perspective pull is easier to implement than a push. But from a consumer perspective, we need to do more work if we pull. FIFO FIFO stands for first-in, first-out, meaning that the oldest message in a queue is always processed first. But in distributed systems, it is hard to maintain a strict order. Message A may be produced prior to message B, but it is hard to guarantee that message A will be stored and consumed prior to message B. For these reasons many distributed queue solutions out there either does not guarantee a strict order. Or have limitations around throughput, as queue cannot be fast while it's doing many additional validations and coordination to guarantee a strict order. Security We need to make sure that messages are securely transferred to and from a queue. Encryption using SSL over HTTPS helps to protect messages in transit. And we also may encrypt messages while storing them on backend hosts. Monitoring Monitoring is critical for every system. With regards to distributed message queue, we need to monitor components (or microservices) that we built: fronted, metadata and backend services. As well as provide visibility into customer's experience. In other words, we need to monitor health of our distributed queue system and give customers ability to track state of their queues. Each service we built has to emit metrics and write log data. As operators of these services we need to create dashboards for each microservice and setup alerts. And customers of our queue have to be able to create dashboards and set up alerts as well. For this purpose, integration with monitoring system is required. Many times this topic is omitted by interviwers but it's very important. Final Look Is our system scalable? Yes. As every component is scalable. When load increases, we just add more load balancers, more FrontEnd hosts, more Metadata service cache shards, more backend clusters and hosts. Is our system highly available? Yes. As there is no a single point of failure, each component is deployed across several data centers. Individual hosts may die, network partitions may happen, but with this redundancy in place our system will continue to operate. Is our system highly performant? It's actually very well depends on the implementation, hardware and network setup. Each individual microservice needs to be fast. And we need to run our software in high-performance data centers. Is our system durable? Sure. We replicate data while storing and ensure messages are not lost during the transfer from a producer and to a consumer.","title":"Distributed Message Queue"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#distributed-message-queue","text":"","title":"Distributed Message Queue"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#synchronous-vs-asynchronous","text":"","title":"Synchronous vs Asynchronous"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#synchronous-communication","text":"When producer makes a call to a consumer, waits for a response. Easier and faster to implement. Harder to deal with consumer service failures. Need to think; When and how to properly retry failed requests? How not to overwhelm consumer service with too many requests? How to deal with a slow consumer service host?","title":"Synchronous Communication"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#asynchronous-communication","text":"Queue : Producer sends data to that component and exactly one consumer gets this data to a short time after. It is distributed, because data is stored across several machines. Do not confuse queue with topic. In case of a topic, message goes to all subscribers. In case of a queue, message is received by only one consumer.","title":"Asynchronous Communication"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#functional-requirements","text":"sendMessage(messageBody) receiveMessage()","title":"Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#non-functional-requirements","text":"Scalable (handles load increases, more queues and messages) Highly Available (survives hardware/network failures) Highly Performant (single digit latency for main operations) Durable (once submitted, data is persisted)","title":"Non-Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#high-level-architecture","text":"VIP : Virtual IP : Refers to the symbolic hostname (myWebService.domain.com) that resolves to a load balancer system. Load Balancer : A device that routes client requests across a number of servers. FrontEnd Web Service : A component responsible for initial request processing, like validation, authentication. Queue Metadata : Queue's name, creation date / time, owner and any other configuration settings will be stored in a DB. Metadata service : As a best practice, this metadata DB should be hidden behind some interface, a dedicated web service responsible for handling calls to that DB. BackEnd Web Service : Responsible for message persistence and processing.","title":"High-level Architecture"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#vip-and-load-balancer","text":"Load balancing is a big topic and unless interviewer asks to deep dive on it, try to stay on topic to main question of the interview. Internals may not matter, however to ensure that non-functional requirements to the system are met we need to explain how Load Balancer help us achieve high throughput and availability. When domain name is hit, request is transferred to one of the VIPs registered in DNS for our domain name. VIP is resolved to a load balancer device, which has a knowledge of FrontEnd hosts. Several Points: First, load balancer seems like a single point of failure. What happens if load balancer device goes down? Second, load balancers have limits with regards to number of requests they can process and number of bytes they can transfer. What happens when our distributed message queue service becomes so popular that load balancer limits are reached? To address high availability concerns, load balancers utilize a concept of primary and secondary nodes. The primary node accepts connections and serves requests while the secondary node monitors the primary. If, the primary node is unable to accept connections, the secondary node takes over. As for scalability concerns, a concept of multiple VIPs (sometimes referred as VIP partitioning) can be utilized. In DNS we assign multiple A-records to the same DNS name for the service. As a result, requests are partitioned across several load balancers. The \"A\" stands for \"address\" and this is the most fundamental type of DNS record: it indicates the IP address of a given domain . And by spreading load balancers across several data centers, we improve both availability and performance.","title":"VIP and Load Balancer"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#frontend-service","text":"FrontEnd is a lightweight web service, consisting of stateless machines located across several data centers. FrontEnd service is responsible for: Request validation : Helps to ensure that all the required parameters are present in the request and values of these parameters honor constraints. For example, in our case we want to make sure queue name comes with every send message request. And message size does not exceed a specified threshold. Authentication and authorization : During authentication check we verify that message sender is a registered customer of our distributed queue service. And during authorization check we verify that sender is allowed to publish messages to the queue it claims. TLS termination : TLS is a protocol that aims to provide privacy and data integrity. TLS termination refers to the process of decrypting request and passing on an unencrypted request to the backend service. And we want to do TLS termination on FrontEnd hosts because TLS on the load balancer is expensive. Termination is usually handled by not a FrontEnd service itself, but a separate HTTP proxy that runs as a process on the same host. Server-side data encryption : Because we want to store messages securely on backend hosts, messages are encrypted as soon as FrontEnd receives them. Messages are stored in encrypted form and FrontEnd decrypts them only when they are sent back to a consumer. Caching : Cache stores copies of source data. In FrontEnd cache we will store metadata information about the most actively used queues. As well as user identity information to save on calls to authentication and authorization services. Rate limiting (Throttling) : Rate limiting or throttling is the process of limiting the number of requests you can submit to a given operation in a given amount of time. Throttling protects the web service from being overwhelmed with requests. Leaky bucket algorithm is one of the most famous. Request dispatching : FrontEnd service makes remote calls to at least two other web services: Metadata service and backend service. FrontEnd service creates HTTP clients for both services and makes sure that calls to these services are properly isolated. It means that when one service let's say Metadata service experiences a slowdown, requests to backend service are not impacted. There are common patterns like bulkhead and circuit breaker that helps to implement resources isolation and make service more resilient in cases when remote calls start to fail. Request deduplication : It may occur when a response from a successful send message request failed to reach a client. Lesser an issue for 'at least once' delivery semantics, a bigger issue for 'exactly once' and 'at most once' delivery semantics, when we need to guarantee that message was never processed more than one time. Caching is usually used to store previously seen request ids to avoid deduplication. Usage data collection : When we gather real-time information that can be used for audit. And even though FrontEnd service has many responsibilities, the rule of thumb is to keep it as simple as possible.","title":"FrontEnd Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#metadata-service","text":"Metadata service stores information about queues. Every time queue is created, we store information about it in the database. Conceptually, Metadata service is a caching layer between the FrontEnd and a persistent storage. It handles many reads and a relatively small number of writes. As we read every time message arrives and write only when new queue is created. Even though strongly consistent storage is preferred to avoid potential concurrent updates, it is not strictly required. Different approaches of organizing cache clusters: The first option is when cache is relatively small and we can store the whole data set on every cluster node. FrontEnd host calls a randomly chosen Metadata service host, because all the cache cluster nodes contain the same information. Second approach is to partition data into small chunks, called shards. Because data set is too big and cannot be placed into a memory of a single host. So, we store each such chunk of data on a separate node in a cluster. FrontEnd then knows which shard stores the data and calls the shard directly. And the third option is similar to the second one. We also partition data into shards, but FrontEnd does not know on what shard data is stored. So, FrontEnd calls a random Metadata service host and host itself knows where to forward the request to. In option one, we can introduce a load balancer between FrontEnd and Metadata service. As all Metadata service hosts are equal and FrontEnd does not care which Metadata host handles the request. In option two and three, Metadata hosts represent a consistent hashing ring.","title":"Metadata Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#backend-service","text":"To understand how backend service architecture may look like, start asking the right questions. Where and how messages are stored? Is database an option? Yes but not the best one. We're building a distributed message queue, a system that needs to handle high throughput, which means all the throughput will be uploaded to the database. In other words, a problem of building a distributed message queue becomes a problem of developing a database that can handle high throughput. These kinds of databases exist out there. It's reasonable for a junior engineer to recommend to use a database. As we may need to store messages for days/weeks, we need more durable storage like local disk. Newly arrived messages may live in memory for a short period of time or until memory on the backend host is fully utilized. File system also works. How do we replicate data? We will send copies of messages to some other hosts, so that data can survive host hardware or software failures. How FrontEnd hosts select backend hosts for both storing messages and retrieving them. We can leverage Metadata service. Message comes to the FrontEnd, FrontEnd consults Metadata service what backend host to send data to. Message is sent to a selected backend host and data is replicated. When receive message call comes, FrontEnd talks to Metadata service to identify a backend host that stores the data. We will consider two options of how backend hosts relate to each other.","title":"Backend Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#option-a-leader-follower-relationship","text":"Each backend instance is considered a leader for a particular set of queues. And by leader we mean that all requests for a particular queue (like send message and receive message requests) go to this leader instance. Send message request comes to a FrontEnd instance. Message comes to a queue with ID equal to q1. FrontEnd service calls Metadata service to identify a leader backend instance for this queue. In this particular example, instance B is a leader for q1. Message is sent to the leader and the leader is fully responsible for data replication. When receive message request comes to a FrontEnd instance, it also makes a request to the Metadata service to identify the leader for the queue. Message is then retrieved from the leader instance and leader is responsible for cleaning up the original message and all the replicas. We need a component that will help us with leader election and management. Let's call it In-cluster manager. And as already mentioned, in-cluster manager is responsible for maintaining a mapping between queues, leaders and followers. In-cluster manager is a very sophisticated component. It has to be reliable, scalable and performant.","title":"Option A : Leader - Follower Relationship"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#option-b-small-cluster-of-independent-hosts","text":"We have a set of small clusters, each cluster consists of 3-4 machines distributed across several data centers. When send message request comes, similar to the previous design option, we also need to call Metadata service to identify which cluster is responsible for storing messages for the q1 queue. After that we just make a call to a randomly selected instance in the cluster. And instance is responsible for data replication across all nodes in the cluster. When receive message request comes and we identified which cluster stores messages for the q1 queue, we once again call a randomly selected host and retrieve the message. Selected host is responsible for the message cleanup. As you may see, we no longer need a component for leader election, but we still need something that will help us to manage queue to cluster assignments. Let's call this component an Out-cluster manager. And this component will be responsible for maintaining a mapping between queues and clusters.","title":"Option B : Small cluster of independent hosts"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#in-cluster-manager-vs-out-cluster-manager","text":"In-cluster manager manages queue assignment within the cluster, out-cluster manager manages queue assignment across clusters. In-cluster manager needs to know about each and every instance in the cluster. Out-cluster manager may not know about each particular instance, but it needs to know about each cluster. In-cluster manager listens to heartbeats from instances. Out-cluster manager monitors health of each independent cluster. In-cluster manager deals with host failures and needs to adjust to the fact that instances may die and new instances may be added to the cluster, out-cluster manager is responsible for tracking each cluster utilization and deal with overheated clusters. Meaning that new queues may no longer be assigned to clusters that reached their capacity limits. In-cluster manager splits queue into parts (partitions) and each partition gets a leader server. Out-cluster manager may split queue across several clusters. So that messages for the same queue are equally distributed between several clusters.","title":"In-cluster Manager vs Out-cluster Manager"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#what-else-is-important","text":"","title":"What else is important?"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#queue-creation-and-deletion","text":"Queue can be auto-created, for example when the first message for the queue hits FrontEnd service, or we can define API for queue creation. API is a better option, as we will have more control over queue configuration parameters. Delete queue operation is a bit controversial, as it may cause a lot of harm and must be executed with caution. For this reason, you may find examples of well-known distributed queues that do not expose deleteQueue API via public REST endpoint. Instead, this operation may be exposed through a command line utility, so that only experienced admin users may call it.","title":"Queue creation and deletion"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#message-deletion","text":"There are several options at our disposal. One option is not to delete a message right after it was consumed. In this case consumers have to be responsible for what they already consumed. And it is not as easy as it sounds. As we need to maintain some kind of an order for messages in the queue and keep track of the offset, which is the position of a message within a queue. Messages can then be deleted several days later, by a job. This idea is used by Apache Kafka. The second option, is to do something similar to what Amazon SQS is doing. Messages are also not deleted immediately, but marked as invisible (known as visibility timeout), so that other consumers may not get already retrieved message. Consumer that retrieved the message, needs to then call delete message API to delete the message from a backend host. And if the message was not explicitly deleted by a consumer, message becomes visible and may be delivered and processed twice. When a consumer consumes a message, a lease is granted for duration of visibility timeout. When consumer calls delete API, consumer must still hold the lease or else delete API will fail.","title":"Message deletion"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#message-replication","text":"Messages need to be replicated to achieve high durability. Otherwise, if we only have one copy of data, it may be lost due to unexpected hardware failure. Messages can be replicated synchronously or asynchronously. Synchronously means that when backend host receives new message, it waits until data is replicated to other hosts. And only if replication is fully completed, successful response is returned to a producer. Asynchronous replication means that response is returned back to a producer as soon as message is stored on a single backend host. In this case we will need to store data in log file incase of system crash. Message is later replicated to other hosts. Both options have pros and cons. Synchronous replication provides higher durability, but with a cost of higher latency for send message operation. Asynchronous replication is more performant, but does not guarantee that message will survive backend host failure.","title":"Message replication"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#message-delivery-semantics","text":"There are three main message delivery guarantees. At most once, when messages may be lost but are never redelivered. At least once, when messages are never lost but may be redelivered. And exactly once, when each message is delivered once and only once. Will anyone ever want other than exactly once delivery? The simple answer is that it is hard to achieve exactly once delivery in practice. In a distributed message queue system there are many potential points of failure. Producer may fail to deliver or deliver multiple times, data replication may fail, consumers may fail to retrieve or process the message. All this adds complexity and leads to the fact that most distributed queue solutions today support at-least-once delivery, as it provides a good balance between durability, availability and performance.","title":"Message delivery semantics"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#push-vs-pull","text":"With a pull model, consumer constantly sends retrieve message requests and when new message is available in the queue, it is sent back to a consumer. With a push model, consumer is not constantly bombarding FrontEnd service with receive calls. Instead, consumer is notified as soon as new message arrives to the queue. And as always, there are pros and cons. From a distributed message queue perspective pull is easier to implement than a push. But from a consumer perspective, we need to do more work if we pull.","title":"Push vs Pull"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#fifo","text":"FIFO stands for first-in, first-out, meaning that the oldest message in a queue is always processed first. But in distributed systems, it is hard to maintain a strict order. Message A may be produced prior to message B, but it is hard to guarantee that message A will be stored and consumed prior to message B. For these reasons many distributed queue solutions out there either does not guarantee a strict order. Or have limitations around throughput, as queue cannot be fast while it's doing many additional validations and coordination to guarantee a strict order.","title":"FIFO"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#security","text":"We need to make sure that messages are securely transferred to and from a queue. Encryption using SSL over HTTPS helps to protect messages in transit. And we also may encrypt messages while storing them on backend hosts.","title":"Security"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#monitoring","text":"Monitoring is critical for every system. With regards to distributed message queue, we need to monitor components (or microservices) that we built: fronted, metadata and backend services. As well as provide visibility into customer's experience. In other words, we need to monitor health of our distributed queue system and give customers ability to track state of their queues. Each service we built has to emit metrics and write log data. As operators of these services we need to create dashboards for each microservice and setup alerts. And customers of our queue have to be able to create dashboards and set up alerts as well. For this purpose, integration with monitoring system is required. Many times this topic is omitted by interviwers but it's very important.","title":"Monitoring"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/distributedMessageQueue/#final-look","text":"Is our system scalable? Yes. As every component is scalable. When load increases, we just add more load balancers, more FrontEnd hosts, more Metadata service cache shards, more backend clusters and hosts. Is our system highly available? Yes. As there is no a single point of failure, each component is deployed across several data centers. Individual hosts may die, network partitions may happen, but with this redundancy in place our system will continue to operate. Is our system highly performant? It's actually very well depends on the implementation, hardware and network setup. Each individual microservice needs to be fast. And we need to run our software in high-performance data centers. Is our system durable? Sure. We replicate data while storing and ensure messages are not lost during the transfer from a producer and to a consumer.","title":"Final Look"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/","text":"Notification Service Problem Statement There is a component called Publisher which produces messages that need to be delivered to a group of other components, called Subscribers. We could have setup a synchronous communication between Publisher and Subscribers, when Publisher calls each Subscriber in some order and waits for the response. But this introduces many different challenges: hard to scale such system when number of subscribers and messages grow and hard to extend such solution to support different types of subscribers. Instead, we can introduce a new system that can register an arbitrary large number of publishers and subscribers and coordinates message delivery between them. The problem statement is ambiguous as usual. To make it less vague we need to start asking clarifying questions. When we talk about functional requirements, we want to define system behavior, or more specifically APIs - a set of operations the system will support. When we talk about non-functional requirements, we basically mean such system qualities as scalability, maintainability, testability and others. Functional Requirements createTopic(topicName) publish(topicName, message) subscribe(topicName, endpoint) Non-Functional Requirements Scalable (supports an arbitrarily large number of topics, publishers and subscribers) Highly Available (tolerates hardware / network failures, no single point of failure) Highly Performant (keep end-to-end latency as low as possible, so that messages are delivered to subscribers as soon as possible) Durable (messages must not be lost, each subscriber must receive every message at least once) Topic : Represents a named resource to which messages are sent. You can think of it as a bucket that stores messages from a publisher and all subscribers receive a copy of a message from the bucket. High Level Architecture Quite similar to Distributed Message Queue All requests coming from our clients will go through a load balancer first. This will ensure requests are equally distributed among requests processing servers. And the component that does this initial request processing is a FrontEnd service . We will use a database to store information about topics and subscriptions. We will hide the database behind another miscroservice, Metadata service . There are several reasons for this decision. First, separation of concerns, a design principle that teaches us to provide access to the database through a well-defined interface. It greatly simplifies maintenance and ability to make changes in the future. Second, Metadata service will act as a caching layer between the database and other components. We do not want to hit database with every message published to the system. We want to retrieve topic metadata from cache. Next, we need to store messages for some period of time. This period will generally be short if all subscribers are available and message was successfully sent to all of them. Or we may need to store messages a bit longer (say several days), so that messages can be retried later if some subscriber is not available right now. And one more component we need is the one that retrieves messages from the message store and sends them to subscribers. Sender also needs to call Metadata service to retrieve information about subscribers. When create topic and subscribe APIs are called, we just need to store all this information in the database. The pattern that consists of load balancer, frontend, metadata store and service is so common in the world of distributed systems, that you can apply it during many system design interview discussions. Frontend Web Service For more detail see Distributed Message Queue When request lands on the host, the first component that picks it up is a Reverse Proxy. Reverse proxy : Lightweight server responsible for several things: Such as SSL termination, when requests that come over HTTPS are decrypted and passed further in unencrypted form. At the same time proxy is responsible for encrypting responses while sending them back to clients. Second responsibility is compression (for example with gzip), when proxy compresses responses before returning them back to clients. This way we reduce the amount of bandwidth required for data transfer. This feature is not relevant for notification service, as in our case responses are tiny and mostly represent acknowledgment that request have completed successfully. But this feature may be very useful for systems when large amount of data is returned by the service. Next proxy feature we may use is to properly handle FrontEnd service slowness. We may return HTTP 503 status code (which is service unavailable) if FrontEnd service becomes slow or completely unavailable. Reverse proxy then passes request to the FrontEnd web service. We know that for every message that is published, FrontEnd service needs to call Metadata service to get information about the message topic. To minimize number of calls to Metadata service, FrontEnd may use local cache. We may use some well-known cache implementations (like Google Guava) or create our own implementation of LRU cache (least recently used). FrontEnd service also writes a bunch of different logs. We definitely need to log information about service health, write down exceptions that happen in the service. We also need to emit metrics. This is just a key-value data that may be later aggregated and used to monitor service health and gather statistics. For example, number of requests, faults, calls latency, we will need all this information for system monitoring. Also, we may need to write information that may be used for audit, for example log who and when made requests to a specific API in the system. Important to understand here, is that FrontEnd service is responsible for writing log data. But the actual log data processing is managed by other components, usually called agents. Agents are responsible for data aggregation and transferring logs to other system, for post processing and storage. This separation of responsibilities is what helps to make FrontEnd service simpler, faster and more robust. Metadata Service The next component in the notification system is a Metadata service. A web service responsible for storing information about topics and subscriptions in the database. It is a distributed cache. When our notification service becomes so popular that we have millions of topics, all this information cannot be loaded into a memory on a single host. Instead, information about topics is divided between hosts in a cluster. Cluster represents a consistent hashing ring. Each FrontEnd host calculates a hash, for example MD5 hash, using some key, for example a combination of topic name and topic owner identifier. Based on the hash value, FrontEnd host picks a corresponding Metadata service host. Let's discuss in more details two different approaches how FrontEnd hosts know which Metadata service host to call. In the first option we introduce a component responsible for coordination. This component knows about all the Metadata service hosts, as those hosts constantly send heartbeats to it. Each FrontEnd host asks Configuration service what Metadata service host contains data for a specified hash value. Every time we scale out and add more Metadata service hosts, Configuration service becomes aware of the changes and re-maps hash key ranges. In the second option we do not use any coordinator. Instead, we make sure that every FrontEnd host can obtain information about all Metadata service hosts. And every FrontEnd host is notified when more Metadata service hosts are added or if any Metadata host died due to a hardware failure. There are different mechanisms that can help FrontEnd hosts discover Metadata service hosts. We will not dive into this topic here, only mention the Gossip protocol. This protocol is based on the way that epidemics spread. Computer systems typically implement this type of protocol with a form of random \"peer selection\": with a given frequency, each machine picks another machine at random and shares data. Temporary Storage After being initially processed by FrontEnd service, message is then passed to the Temporary Storage service. Why do we call it temporary and not just storage? Because messages supposed to stay in this storage for a very short period of time. Sooner we can deliver messages to subscribers is better, unless topic is configured to deliver messages with some delay. What do we expect from the Temporary Storage service? First, it must be fast, highly-available and scalable. Second, it has to guarantee data persistence, so that messages survive unavailability of a subscriber. And can be re-delivered later. Several design options can be considered and this is a great opportunity for you to show breadth and depth of your knowledge to the interviewer. Let's see how many different directions the interview may go from here. You can start discussing databases with the interviewer, consider pros and cons of SQL versus NoSQL databases, evaluate different NoSQL database types and give specific names to the interviewer. For example, when we consider SQL or NoSQL for storing messages, we may mention that we do not need ACID transactions, we do not need to run complex dynamic queries, we do not plan to use this storage for analytics or data warehousing. Instead, we need a database that can be easily scaled for both writes and reads. It should be highly available and tolerate network partitions. Summing all these up, it is clear that NoSQL wins for our use case. If we need to choose a particular NoSQL database type, we need to mention that messages have limited size (let's say not more than 1 MB), meaning that we do not actually need a document store. And there is no any specific relationship between messages. And thus, we can exclude graph type as well. Which leaves us with either column or key-value database types. And we can mention several well-regarded names of these two database types. For example, Apache Cassandra and Amazon DynamoDB. Next option we can evaluate is in-memory storage. We better choose an in-memory store that supports persistence, so that messages can live for several days before being dropped. And also mention some great in-memory storage solutions like Redis. One more option to consider - message queues. Distributed message queues have all the characteristics we require and also can be discussed in more details. And if you want to further impress interviewer, you can talk about other options, for example stream-processing platforms. Discuss pros and cons and compare this option with a distributed queue solution. And of course, do not forget to mention some best-in-class solutions, for example Apache Kafka and Amazon Kinesis. Here we reference AWS solutions, but feel free to mention similar solutions from other public clouds. Whatever solution you feel comfortable with. Sender Service Once data is stored in temporary storage, we can now start sending message to subscribers. We can see that the ideas on which Sender Service is built upon can easily be applied to other distributed systems. If you design a solution that requires data retrieval, processing, and sending messages in a fan-out matter, think of the following ideas. Message Retrieval The first thing that Sender does is message retrieval. This is achieved by having a pool of threads, where each thread tries to read data from the Temporary Storage. We can implement a naive approach and always start a predefined number of message retrieval threads. The problem with this approach is that some threads may be idle, as there may not be enough messages to retrieve. Or another extreme, when all threads may quickly become occupied and the only way to scale message retrieval would be adding more Sender hosts. A better approach is to keep track of idle threads and adjust number of message retrieval threads dynamically. If we have too many idle threads, no new threads should be created. If all threads are busy, more threads in the pool can start reading messages. This not only helps to better scale the Sender service, it also protects Temporary Storage service from being constantly bombarded by Sender service. This is especially useful when Temporary Storage service experiences performance degradation, and Sender service can lower the rate of message reads to help Temporary Storage service to recover faster. How can we implement this auto-scaling solution? Semaphores to the rescue. Conceptually, a semaphore maintains a set of permits. Before retrieving the next message, thread must acquire a permit from the semaphore. When the thread has finished reading the message a permit is returned to the semaphore, allowing another thread from the pool to start reading messages. What we can do is to adjust a number of these permits dynamically, based on the existing and desired message read rate. Post Message Retrieval After message is retrieved, we need to call Metadata service to obtain information about subscribers. Probably, some of you are wondering why we need to call Metadata service here, if we already called it in FrontEnd service and could have passed information about subscribers along with the message. Good point to discuss with the interviewer. One of the main reasons not to pass this information along with the message itself, is that list of subscribers may be relatively big. For example, several thousands of HTTP endpoints, or a long list of email addresses. We will need to store all this information with every incoming message and our Temporary Storage service will need to pay this price. Not all key-value and column storages can store big messages, which may require us to use document database instead. These all are very good consideration to mention to the interviewer. After we obtained a list of subscribers, we can start sending messages to all of them. Should we just iterate over a list of subscribers and make a remote call to each one of them? What should we do if message delivery failed to one subscriber from the list? What if one subscriber is slow and delivery to other subscribers is impacted due to this? If you think about all these questions or if interviewer makes you thinking about them, you will probably figure out that this option is far from optimal. The better option is to split message delivery into tasks. Where each task is responsible for delivery to a single subscriber. This way we can deliver all messages in parallel and isolate any bad subscriber. So, let's introduce two new components - Task Creator and Executor . These components are responsible for creating and scheduling a single message delivery task. How can we implement these components? Create a pool of threads, where each thread is responsible for executing a task. And similar to the Message Retriever component, we can also use semaphores to keep track of available threads in the pool. If we have enough threads to process newly created tasks, we simply submit all tasks for processing. If we do not have enough threads available at the moment, we may postpone or stop message processing and return the message back to the Temporary Storage. In this case, a different Sender service host may pick up this message. And that host may have enough threads to process the message. This approach allows to better handle slow Sender service host issues. Each task is responsible for message delivery to a single subscriber. Tasks may delegate actual delivery to other microservices. For example, a microservice responsible for sending emails or SMS messages. What else is important How to make sure notifications will not be sent to users as spam? We need to register subscribers. All subscribers need to confirm they agree to get notification from our service. Every time new subscriber is registered, we may send a confirmation message to the HTTP endpoint or email. Endpoint and email owners need to confirm the subscription request. When publishers send messages to our notification service, FrontEnd service will make sure duplicate submissions are eliminated. This helps to avoid duplicates while accepting messages from publishers. But when the Sender service delivers messages to subscribers, retries caused by network issues or subscriber's internal issues may cause duplicate messages at the subscriber end. So, subscribers also become responsible for avoiding duplicates. If retries cause duplicates, maybe we do not need to retry? Retry is one of the options to guarantee at least once message delivery. Our system may retry for hours or even days until messages is successfully delivered or maximum allowed number of retries have been reached. Other option may be to send undelivered messages to a different subscriber. Or store such messages in a system that can be monitored by subscribers and subscribers then decide what to do with undelivered messages. It would be great if our notification system provides a way for subscribers to define retry policy and what to do in cases when a message cannot be delivered after retries limit is reached. Does our system guarantee any specific message order, for example first-come-first-delivered? And the short answer is no, it does not guarantee any specific order. Even if messages are published with some attribute that preserves the order, for example sequence number or timestamps, delivery of messages does not honor this. Delivery tasks can be executed in any order, slower Sender hosts may fall behind, message delivery attempt may fail and retry will arrive in a wrong order. Security always has to be a top priority. We need to make sure only authenticated publishers can publish messages, only registered subscribers can receive them, messages are always delivered to the specified set of subscribers and never to anyone else. Encryption using SSL over HTTP helps to protect messages in transit. And we also need to encrypt messages while storing them. We need to setup monitoring for every microservice we discussed, as well as for the end-to-end customer experience. We also need to give customers ability to track state of their topics. For example, number of messages waiting for delivery, number of messages failed to deliver, etc. This usually means that integration with a monitoring system is required. Final Look Did we design a scalable system? Yes. Every component is horizontally scalable. Sender service also has a great potential for vertical scaling, when more powerful hosts can execute more delivery tasks. Did we design a highly available system? Yes. There is no single point of failure, each component is deployed across several data centers. Did we design a highly performant system? FrontEnd service is fast. We made it responsible for several relatively small and cheap activities. We delegated several other activities to agents that run asynchronously and does not impact request processing. Metadata service is a distributed cache. It is fast as we store data for active topics in memory. We discussed several Temporary Storage design options and mentioned 3-rd party solutions that are fast. And our Sender service splits message delivery into granular tasks, so that each one of them can be optimally performed. Did we design a durable system? Yes. Whatever Temporary Storage solution we choose, data will be stored in the redundant manner, when several copies of a message is stored across several machines, and ideally across several data centers. We also retry messages for a period of time to make sure they are delivered to every subscriber at least once.","title":"Notification Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#notification-service","text":"","title":"Notification Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#problem-statement","text":"There is a component called Publisher which produces messages that need to be delivered to a group of other components, called Subscribers. We could have setup a synchronous communication between Publisher and Subscribers, when Publisher calls each Subscriber in some order and waits for the response. But this introduces many different challenges: hard to scale such system when number of subscribers and messages grow and hard to extend such solution to support different types of subscribers. Instead, we can introduce a new system that can register an arbitrary large number of publishers and subscribers and coordinates message delivery between them. The problem statement is ambiguous as usual. To make it less vague we need to start asking clarifying questions. When we talk about functional requirements, we want to define system behavior, or more specifically APIs - a set of operations the system will support. When we talk about non-functional requirements, we basically mean such system qualities as scalability, maintainability, testability and others.","title":"Problem Statement"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#functional-requirements","text":"createTopic(topicName) publish(topicName, message) subscribe(topicName, endpoint)","title":"Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#non-functional-requirements","text":"Scalable (supports an arbitrarily large number of topics, publishers and subscribers) Highly Available (tolerates hardware / network failures, no single point of failure) Highly Performant (keep end-to-end latency as low as possible, so that messages are delivered to subscribers as soon as possible) Durable (messages must not be lost, each subscriber must receive every message at least once) Topic : Represents a named resource to which messages are sent. You can think of it as a bucket that stores messages from a publisher and all subscribers receive a copy of a message from the bucket.","title":"Non-Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#high-level-architecture","text":"Quite similar to Distributed Message Queue All requests coming from our clients will go through a load balancer first. This will ensure requests are equally distributed among requests processing servers. And the component that does this initial request processing is a FrontEnd service . We will use a database to store information about topics and subscriptions. We will hide the database behind another miscroservice, Metadata service . There are several reasons for this decision. First, separation of concerns, a design principle that teaches us to provide access to the database through a well-defined interface. It greatly simplifies maintenance and ability to make changes in the future. Second, Metadata service will act as a caching layer between the database and other components. We do not want to hit database with every message published to the system. We want to retrieve topic metadata from cache. Next, we need to store messages for some period of time. This period will generally be short if all subscribers are available and message was successfully sent to all of them. Or we may need to store messages a bit longer (say several days), so that messages can be retried later if some subscriber is not available right now. And one more component we need is the one that retrieves messages from the message store and sends them to subscribers. Sender also needs to call Metadata service to retrieve information about subscribers. When create topic and subscribe APIs are called, we just need to store all this information in the database. The pattern that consists of load balancer, frontend, metadata store and service is so common in the world of distributed systems, that you can apply it during many system design interview discussions.","title":"High Level Architecture"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#frontend-web-service","text":"For more detail see Distributed Message Queue When request lands on the host, the first component that picks it up is a Reverse Proxy. Reverse proxy : Lightweight server responsible for several things: Such as SSL termination, when requests that come over HTTPS are decrypted and passed further in unencrypted form. At the same time proxy is responsible for encrypting responses while sending them back to clients. Second responsibility is compression (for example with gzip), when proxy compresses responses before returning them back to clients. This way we reduce the amount of bandwidth required for data transfer. This feature is not relevant for notification service, as in our case responses are tiny and mostly represent acknowledgment that request have completed successfully. But this feature may be very useful for systems when large amount of data is returned by the service. Next proxy feature we may use is to properly handle FrontEnd service slowness. We may return HTTP 503 status code (which is service unavailable) if FrontEnd service becomes slow or completely unavailable. Reverse proxy then passes request to the FrontEnd web service. We know that for every message that is published, FrontEnd service needs to call Metadata service to get information about the message topic. To minimize number of calls to Metadata service, FrontEnd may use local cache. We may use some well-known cache implementations (like Google Guava) or create our own implementation of LRU cache (least recently used). FrontEnd service also writes a bunch of different logs. We definitely need to log information about service health, write down exceptions that happen in the service. We also need to emit metrics. This is just a key-value data that may be later aggregated and used to monitor service health and gather statistics. For example, number of requests, faults, calls latency, we will need all this information for system monitoring. Also, we may need to write information that may be used for audit, for example log who and when made requests to a specific API in the system. Important to understand here, is that FrontEnd service is responsible for writing log data. But the actual log data processing is managed by other components, usually called agents. Agents are responsible for data aggregation and transferring logs to other system, for post processing and storage. This separation of responsibilities is what helps to make FrontEnd service simpler, faster and more robust.","title":"Frontend Web Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#metadata-service","text":"The next component in the notification system is a Metadata service. A web service responsible for storing information about topics and subscriptions in the database. It is a distributed cache. When our notification service becomes so popular that we have millions of topics, all this information cannot be loaded into a memory on a single host. Instead, information about topics is divided between hosts in a cluster. Cluster represents a consistent hashing ring. Each FrontEnd host calculates a hash, for example MD5 hash, using some key, for example a combination of topic name and topic owner identifier. Based on the hash value, FrontEnd host picks a corresponding Metadata service host. Let's discuss in more details two different approaches how FrontEnd hosts know which Metadata service host to call. In the first option we introduce a component responsible for coordination. This component knows about all the Metadata service hosts, as those hosts constantly send heartbeats to it. Each FrontEnd host asks Configuration service what Metadata service host contains data for a specified hash value. Every time we scale out and add more Metadata service hosts, Configuration service becomes aware of the changes and re-maps hash key ranges. In the second option we do not use any coordinator. Instead, we make sure that every FrontEnd host can obtain information about all Metadata service hosts. And every FrontEnd host is notified when more Metadata service hosts are added or if any Metadata host died due to a hardware failure. There are different mechanisms that can help FrontEnd hosts discover Metadata service hosts. We will not dive into this topic here, only mention the Gossip protocol. This protocol is based on the way that epidemics spread. Computer systems typically implement this type of protocol with a form of random \"peer selection\": with a given frequency, each machine picks another machine at random and shares data.","title":"Metadata Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#temporary-storage","text":"After being initially processed by FrontEnd service, message is then passed to the Temporary Storage service. Why do we call it temporary and not just storage? Because messages supposed to stay in this storage for a very short period of time. Sooner we can deliver messages to subscribers is better, unless topic is configured to deliver messages with some delay. What do we expect from the Temporary Storage service? First, it must be fast, highly-available and scalable. Second, it has to guarantee data persistence, so that messages survive unavailability of a subscriber. And can be re-delivered later. Several design options can be considered and this is a great opportunity for you to show breadth and depth of your knowledge to the interviewer. Let's see how many different directions the interview may go from here. You can start discussing databases with the interviewer, consider pros and cons of SQL versus NoSQL databases, evaluate different NoSQL database types and give specific names to the interviewer. For example, when we consider SQL or NoSQL for storing messages, we may mention that we do not need ACID transactions, we do not need to run complex dynamic queries, we do not plan to use this storage for analytics or data warehousing. Instead, we need a database that can be easily scaled for both writes and reads. It should be highly available and tolerate network partitions. Summing all these up, it is clear that NoSQL wins for our use case. If we need to choose a particular NoSQL database type, we need to mention that messages have limited size (let's say not more than 1 MB), meaning that we do not actually need a document store. And there is no any specific relationship between messages. And thus, we can exclude graph type as well. Which leaves us with either column or key-value database types. And we can mention several well-regarded names of these two database types. For example, Apache Cassandra and Amazon DynamoDB. Next option we can evaluate is in-memory storage. We better choose an in-memory store that supports persistence, so that messages can live for several days before being dropped. And also mention some great in-memory storage solutions like Redis. One more option to consider - message queues. Distributed message queues have all the characteristics we require and also can be discussed in more details. And if you want to further impress interviewer, you can talk about other options, for example stream-processing platforms. Discuss pros and cons and compare this option with a distributed queue solution. And of course, do not forget to mention some best-in-class solutions, for example Apache Kafka and Amazon Kinesis. Here we reference AWS solutions, but feel free to mention similar solutions from other public clouds. Whatever solution you feel comfortable with.","title":"Temporary Storage"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#sender-service","text":"Once data is stored in temporary storage, we can now start sending message to subscribers. We can see that the ideas on which Sender Service is built upon can easily be applied to other distributed systems. If you design a solution that requires data retrieval, processing, and sending messages in a fan-out matter, think of the following ideas. Message Retrieval The first thing that Sender does is message retrieval. This is achieved by having a pool of threads, where each thread tries to read data from the Temporary Storage. We can implement a naive approach and always start a predefined number of message retrieval threads. The problem with this approach is that some threads may be idle, as there may not be enough messages to retrieve. Or another extreme, when all threads may quickly become occupied and the only way to scale message retrieval would be adding more Sender hosts. A better approach is to keep track of idle threads and adjust number of message retrieval threads dynamically. If we have too many idle threads, no new threads should be created. If all threads are busy, more threads in the pool can start reading messages. This not only helps to better scale the Sender service, it also protects Temporary Storage service from being constantly bombarded by Sender service. This is especially useful when Temporary Storage service experiences performance degradation, and Sender service can lower the rate of message reads to help Temporary Storage service to recover faster. How can we implement this auto-scaling solution? Semaphores to the rescue. Conceptually, a semaphore maintains a set of permits. Before retrieving the next message, thread must acquire a permit from the semaphore. When the thread has finished reading the message a permit is returned to the semaphore, allowing another thread from the pool to start reading messages. What we can do is to adjust a number of these permits dynamically, based on the existing and desired message read rate. Post Message Retrieval After message is retrieved, we need to call Metadata service to obtain information about subscribers. Probably, some of you are wondering why we need to call Metadata service here, if we already called it in FrontEnd service and could have passed information about subscribers along with the message. Good point to discuss with the interviewer. One of the main reasons not to pass this information along with the message itself, is that list of subscribers may be relatively big. For example, several thousands of HTTP endpoints, or a long list of email addresses. We will need to store all this information with every incoming message and our Temporary Storage service will need to pay this price. Not all key-value and column storages can store big messages, which may require us to use document database instead. These all are very good consideration to mention to the interviewer. After we obtained a list of subscribers, we can start sending messages to all of them. Should we just iterate over a list of subscribers and make a remote call to each one of them? What should we do if message delivery failed to one subscriber from the list? What if one subscriber is slow and delivery to other subscribers is impacted due to this? If you think about all these questions or if interviewer makes you thinking about them, you will probably figure out that this option is far from optimal. The better option is to split message delivery into tasks. Where each task is responsible for delivery to a single subscriber. This way we can deliver all messages in parallel and isolate any bad subscriber. So, let's introduce two new components - Task Creator and Executor . These components are responsible for creating and scheduling a single message delivery task. How can we implement these components? Create a pool of threads, where each thread is responsible for executing a task. And similar to the Message Retriever component, we can also use semaphores to keep track of available threads in the pool. If we have enough threads to process newly created tasks, we simply submit all tasks for processing. If we do not have enough threads available at the moment, we may postpone or stop message processing and return the message back to the Temporary Storage. In this case, a different Sender service host may pick up this message. And that host may have enough threads to process the message. This approach allows to better handle slow Sender service host issues. Each task is responsible for message delivery to a single subscriber. Tasks may delegate actual delivery to other microservices. For example, a microservice responsible for sending emails or SMS messages.","title":"Sender Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#what-else-is-important","text":"How to make sure notifications will not be sent to users as spam? We need to register subscribers. All subscribers need to confirm they agree to get notification from our service. Every time new subscriber is registered, we may send a confirmation message to the HTTP endpoint or email. Endpoint and email owners need to confirm the subscription request. When publishers send messages to our notification service, FrontEnd service will make sure duplicate submissions are eliminated. This helps to avoid duplicates while accepting messages from publishers. But when the Sender service delivers messages to subscribers, retries caused by network issues or subscriber's internal issues may cause duplicate messages at the subscriber end. So, subscribers also become responsible for avoiding duplicates. If retries cause duplicates, maybe we do not need to retry? Retry is one of the options to guarantee at least once message delivery. Our system may retry for hours or even days until messages is successfully delivered or maximum allowed number of retries have been reached. Other option may be to send undelivered messages to a different subscriber. Or store such messages in a system that can be monitored by subscribers and subscribers then decide what to do with undelivered messages. It would be great if our notification system provides a way for subscribers to define retry policy and what to do in cases when a message cannot be delivered after retries limit is reached. Does our system guarantee any specific message order, for example first-come-first-delivered? And the short answer is no, it does not guarantee any specific order. Even if messages are published with some attribute that preserves the order, for example sequence number or timestamps, delivery of messages does not honor this. Delivery tasks can be executed in any order, slower Sender hosts may fall behind, message delivery attempt may fail and retry will arrive in a wrong order. Security always has to be a top priority. We need to make sure only authenticated publishers can publish messages, only registered subscribers can receive them, messages are always delivered to the specified set of subscribers and never to anyone else. Encryption using SSL over HTTP helps to protect messages in transit. And we also need to encrypt messages while storing them. We need to setup monitoring for every microservice we discussed, as well as for the end-to-end customer experience. We also need to give customers ability to track state of their topics. For example, number of messages waiting for delivery, number of messages failed to deliver, etc. This usually means that integration with a monitoring system is required.","title":"What else is important"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/notificationService/#final-look","text":"Did we design a scalable system? Yes. Every component is horizontally scalable. Sender service also has a great potential for vertical scaling, when more powerful hosts can execute more delivery tasks. Did we design a highly available system? Yes. There is no single point of failure, each component is deployed across several data centers. Did we design a highly performant system? FrontEnd service is fast. We made it responsible for several relatively small and cheap activities. We delegated several other activities to agents that run asynchronously and does not impact request processing. Metadata service is a distributed cache. It is fast as we store data for active topics in memory. We discussed several Temporary Storage design options and mentioned 3-rd party solutions that are fast. And our Sender service splits message delivery into granular tasks, so that each one of them can be optimally performed. Did we design a durable system? Yes. Whatever Temporary Storage solution we choose, data will be stored in the redundant manner, when several copies of a message is stored across several machines, and ideally across several data centers. We also retry messages for a period of time to make sure they are delivered to every subscriber at least once.","title":"Final Look"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/","text":"Rate Limiting (local and distributed) Problem Statement / Requirement Clarification Let's imaging we launched a web application. And the application became highly popular. Meaning that thousands of clients send thousands of requests every second to the front-end web service of our application. Everything works well. Until suddenly one or several clients started to send much more requests than they did previously. And this may happen due to a various of reasons. For example, our client is another popular web service and it experienced a sudden traffic spike. Or developers of that web service started to run a load test. Or this is just a malicious client who tried to DDoS our service. All these situations may lead to a so called \"noisy neighbor problem\", when one client utilizes too much shared resources on a service host, like CPU, memory, disk or network I/O. And because of this, other clients of our application start to experience higher latency for their requests, or higher rate of failed requests. One of the ways to solve a \"noisy neighbor problem\" is to introduce a rate limiting (also known as throttling). Throttling helps to limit the number of requests a client can submit in a given amount of time. Requests submitted over the limit are either immediately rejected or their processing is delayed. Some questions: This problem does not have a lot of sense. It should be solved by scaling out the cluster of hosts that run our web service. And ideally, by some kind of auto-scaling And the problem with scaling up or scaling out is that it is not happening immediately. Even autoscaling takes time. And by the time scaling process completes it may already be late. Our service may already crash. How rate limiting can be achieved? Specifically, you mention load balancers and their ability to limit a number of simultaneous requests that load balancer sends to each application server. Load balancers indeed may prevent too many requests to be forwarded to an application server. Load balancer will either reject any request over the limit or send the request to a queue, so that it can be processed later. But the problem with this mechanism - it is indiscriminate. Let's say our web service exposes several different operations. Some of them are fast operations, they take little time to complete. But some operations are slow and heavy and each request may take a lot of processing power. Load balancer does not have knowledge about a cost of each operation. And if we want to limit number of requests for a particular operation, we can do this on application server only, not at a load balancer level. The problem does not seem to be a system design problem. Algorithmic problem? Yes, as we need to define data structures and algorithm to count how many requests client has made so far. Object-oriented design problem? Probably, as we may need to design a set of classes to manage throttling rules. Rules define an allowed throttling limit for each operation. So, if we implement throttling for a single host, are we done? In an ideal world - yes. But not in the real world. If we have a load balancer in front of our web service and this load balancer spreads requests evenly across application servers and each request takes the same amount of time to complete. In this case this is a single instance problem and there is no need in any distributed solution. Application servers do not need to talk to each other. They throttle requests independently. But in the real-world load balancers cannot distribute requests in a perfectly even manner. Plus, as we discussed before different web service operations cost differently. And each application server itself may become slow due to software failures or overheated due to some other background process running on it. All this leads to a conclusion that we will need a solution where application servers will communicate with each other and share information about how many client requests each one of them processed so far. Functional Requirements allowRequest(request) : For a given request our rate limiting solution should return a boolean value, whether request is throttled or not. Non-Functional Requirements Low Latency (We need rate limiter to be fast (as it will be called on every request to the service) Accurate (We do not want to throttle customers unless it is absolutely required) Scalable (Rate limiter scales out together with the service itself, if we need to add more hosts to the web service cluster, this should not be a problem for the rate limiter) What about high availability and fault tolerance? Two common requirements for many distributed systems. Are they important for a rate limiting solution? Not so much. If rate limiter cannot make a decision quickly due to any failures, the decision is always not to throttle. If we do not know whether throttle or not -> we do not throttle. Because we may need to introduce rate limiting in many services, the interviewer may ask us to think about ease of integration. So that every service team in the organization can integrate with our rate limiting solution as seamlessly as possible. This is a good requirement. Simple Solution It always makes sense to start/mention the simple solution and evolve the solution. Let's implement a solution for a single server. The first citizen of the rate limiting solution on the service host is the rules retriever. Each rule specifies a number of requests allowed for a particular client per second. These rules are defined by service owners and stored in a database. And there is a web service that manages all the operation with rules. Rules retriever is a background process that polls Rules service periodically to check if there are any new or modified rules. Rules retriever stores rules in memory on the host. When request comes, the first thing we need to do is to build a client identifier. Let's call it a key, for short. This may be a login for registered clients or remote IP address or some combination of attributes that uniquely identify the client. The key is then passed to the Rate Limiter component, that is responsible for making a decision. Rate Limiter checks the key against rules in the cache. And if match is found, Rate Limiter checks if number of requests made by the client for the last second is below a limit specified in the rule. If threshold is not exceeded, request is passed further for processing. If threshold is exceeded, the request is rejected. And there are three possible options in this case. Our service may return a specific response status code, for example service unavailable or too many requests. Or we can queue this request and process it later. Or we can simply drop this request on the floor. We know we need: A database to store the rules. And we need a service on top of this database for all the so-called CRUD operations (create, read, update, delete). We know we need a process to retrieve rules periodically and store rules in memory. And we need a component that makes a decision. You may argue whether we need the client identifier builder as a separate component or should it just be a part of the decision-making component. It is up to you. I wanted to present this builder as a separate component to stress the point that client identification is an important step of the whole process. From here interview may go in several directions. Interviewer may be interested in the Rate Limiter algorithm and ask us to implement one. Or interviewer may be interested in object-oriented design and ask us to define main classes and interfaces of the throttling library. Or interviewer may ask us to focus on a distributed throttling solution and discuss how service hosts share data between each other. Let's discuss each of these possible directions. Token Bucket Algorithm The token bucket algorithm is based on an analogy of a bucket filled with tokens. Each bucket has three characteristics: a maximum amount of tokens it can hold, amount of tokens currently available and a refill rate, the rate at which tokens are added to the bucket. Every time request comes, we take a token from the bucket. If there are no more tokens available in the bucket, request is rejected. The bucket is refilled with a constant rate. The beauty of the Token Bucket algorithm is that it simple to understand and simple to implement. There are 4 class fields: maximum bucket size, refill rate, number of currently available tokens and timestamp that indicates when bucket was last refilled. Constructor accepts two arguments: maximum bucket size and refill rate. Number of currently available tokens is set to the maximum bucket size. And timestamp is set to the current time in nanoseconds. Allow request method has one argument - number of tokens that represent a cost of the operation; usually, the cost is equal to 1, but it may be a larger value as well. The first thing we do is refilling the bucket. And right after that we check if there are enough tokens in the bucket. In case there are not enough tokens, method return false, indicating that request must be throttled. Otherwise, we need to decrease number of available tokens by the cost of the request. And the last piece is the refill method. It calculates how many tokens accumulated since the last refill and increases currently available tokens in the bucket by this number. Leaky Bucket Leaky Bucket algorithm is opposite of Token Bucket algorithm. Leaky Bucket allows requests at a constant rate, the rate of the leak. There is no concept of a token, the requests go into the bucket and flow out at a constant rate. If there is no space in the bucket for the request, the request is discarded. Object Oriented Design Job Scheduler interface is responsible for scheduling a job that runs every several seconds and retrieves rules from Rules service. RulesCache interface is responsible for storing rules in memory. ClientIdentifier builds a key that uniquely identifies a client. And RateLimiter is responsible for decision making. RetrieveJobScheduler class implements JobScheduler interface. Its responsibility is to instantiate, start and stop the scheduler, and to run retrieve rules task. TokenBucketCache stores token buckets. We can use something simple, for example Map to store buckets. Or utilize 3-rd party cache implementation, like Google Guava cache. ClientIdentifierBuilder is responsible for building a key based on user identity information (for example login). There can be other implementations as well, for example based on IP address. And for the RateLimiter interface lets introduce a TokenBucketRateLimiter class, which is responsible for calling allow request on the correspondent bucket for that client. And the last important piece is the RetrieveRulesTask, which is responsible for retrieving all the rules for this service. Let's look at how these components interact with each other. RetrieveJobScheduler runs RetrieveRulesTask, which makes a remote call to the Rules service. It then creates token buckets and puts them into the cache. When client request comes to the host, RateLimiter first makes a call to the ClientIdentifierBuilder to build a unique identifier for the client. And then it passes this key to the cache and retrieves the bucket. And the last step to do is to call allow request on the bucket. Step Into The Distributed World We have a cluster that consists of 3 hosts. And we want rate limiting solution to allow 4 requests per second for each client. How many tokens should we give to a bucket on every host? Should we give 4 divided by 3? And the answer is 4. Each bucket should have 4 tokens initially. The reason for this is that all requests for the same bucket may in theory land on the same host. Load balancers try to distributed requests evenly, but they do not know anything about keys, and requests for the same key will not be evenly distributed. Let's add load balancer into the picture and run a very simple simulation. The first request goes to host A, one token is consumed. The second request goes to host C and one token is consumed there. Two other requests, within the same 1 second interval, go to host B. And take two tokens from the bucket. All 4 allowed requests hit the cluster; we should throttle all the remaining requests for this second. But we still have tokens available. What should we do? We must allow hosts to talk to each other and share how many tokens they consumed altogether. In this case host A will see that other two hosts consumed 3 tokens. And host A will subtract this number from its bucket. Leaving it with 0 tokens available. Host B will find out that A and C consumed two tokens already. Leaving host B with 0 tokens as well. And the same logic applies to host C. Now everything looks correct. 4 requests have been processed and no more requests allowed. We gave each bucket 4 tokens. If many requests for the same bucket hit our cluster exactly at the same second. Does this mean that 12 requests may be processed, instead of only 4 allowed? Or may be a more realistic scenario. Because communication between hosts takes time, until all hosts agree on what that final number of tokens must be, may there be any requests that slip into the system at that time? Yes. Unfortunately, this is the case. We should expect that sometimes our system may be processing more requests than we expect and we need to scale out our cluster accordingly. By the way, the token bucket algorithm will still handle this use case well. We just need to slightly modify it to allow negative number of available tokens. When 12 requests hit the system, buckets will start sharing this information. After sharing, every bucket will have -8 tokens and for the duration of the next 2 seconds all requests will be throttled. So, on average we processed 12 requests within 3 seconds. Although in reality all 12 were processed within the first second. So, communication between hosts is the key. Let's see how this communication can be implemented. By the way, ideas we will discuss next are applicable not only for rate limiting solution, but many other distributed systems that require data sharing between all hosts in a cluster in a real time. Message Broadcasting The first approach is to tell everyone everything . It means that every host in the cluster knows about every other host in the cluster and share messages with each one of them. You may also heard a term full mesh that describes this network topology. How do hosts discover each other? When a new host is added, how does everyone else know? And there are several approaches used for hosts discovery. One option is to use a 3-rd party service which will listen to heartbeats coming from every host. As long as heartbeats come, host is keep registered in the system. If heartbeats stop coming, the service unregister host that is no longer alive. And all hosts in our cluster ask this 3-rd party service for the full list of members. Another option is to resolve some user provided information. For example, user specifies a VIP and because VIP knows about all the hosts behind it, we can use this information to obtain all the members. Or we can rely on a less flexible but still a good option when user provides a list of hosts via some configuration file. We then need a way to deploy this file across all cluster nodes every time this list changes. Full mesh broadcasting is relatively straightforward to implement. But the main problem with this approach is that it is not scalable. Number of messages grows quadratically with respect to the number of hosts in a cluster. Approach works well for small clusters, but we will not be able to support big clusters. So, let's investigate some other options that may require less messages to be broadcasted within the cluster. And one such option is to use a gossip protocol . This protocol is based on the way that epidemics spread. Computer systems typically implement this type of protocol with a form of random \"peer selection\": with a given frequency, each machine picks another machine at random and shares data. By the way, rate limiting solution at Yahoo uses this approach. Next option is to use distributed cache cluster . For example, Redis. Or we can implement custom distributed cache solution. The pros for this approach is that distributed cache cluster is relatively small and our service cluster can scale out independently. This cluster can be shared among many different service teams in the organization. Or each team can setup their own small cluster. Next approach also relies on a 3-rd party component. A coordination service that helps to choose a leader. Choosing a leader helps to decrease number of messages broadcasted within the cluster. Leader asks everyone to send it all the information. And then it calculates and sends back the final result. So, each host only needs to talk to a leader or a set of leaders, where each leader is responsible for its own range of keys. Consensus algorithms such as Paxos and Raft can be used to implement Coordination Service. Great option, but the main drawback is that we need to setup and maintain Coordination Service. Coordination service is typically a very sophisticated component that has to be very reliable and make sure one and only one leader is elected. But is this really a requirement for our system? Let's say we use a simple algorithm to elect a leader. But because of the simplicity of the algorithm it may not guarantee one and only one leader. So that we may end up with multiple leaders being elected. Is this an issue? Actually, no. Each leader will calculate rate and share with everyone else. This will cause unnecessary messaging overhead, but each leader will have its own correct view of the overall rate. Communication Protocols : TCP vs UDP How do hosts talk to each other. We have two options here: TCP and UDP. TCP protocol guarantees delivery of data and also guarantees that packets will be delivered in the same order in which they were sent. UDP protocol does not guarantee you are getting all the packets and order is not guaranteed. But because UDP throws all the error-checking stuff out, it is faster. So, which one is better? Both are good choices. If we want rate limiting solution to be more accurate, but with a little bit of performance overhead, we need to go with TCP. If we ok to have a bit less accurate solution, but the one that works faster, UDP should be our choice. How do we integrate all There are two options. And they are pretty standard. We can run Rate Limiter as a part of the service process or as its own process (daemon). In the first option, Rate Limiter is distributed as a collection of classes, a library that should be integrated with the service code. In the second option we have two libraries: the daemon itself and the client, that is responsible for inter-process communication between the service process and the daemon. Client is integrated with the service code. What are the pros for the first approach? It is faster, as we do not need to do any inter-process call. It is also resilient to the inter-process call failures, because there are no such calls. What are the pros for the second approach? The second approach is programming language agnostic. It means that Rate Limiter daemon can be written on a programming language that may be different from the language we use for the service implementation. As we do not need to do integration on the code level. Yes, we need to have Rate Limiter client compatible with the service code language. But not the daemon itself. Also, Rate Limiter process uses its own memory space. This isolation helps to better control behavior for both the service and the daemon. For example, daemon my store many buckets in memory, but because the service process has its own memory space, the service memory does not need to allocate space for these buckets. Which makes service memory allocation more predictable. Another good reason, and you may see it happening a lot in practice, service teams tend to be very cautious when you come to them and ask to integrate their service with your super cool library. You will hear tons of questions. Like how much memory and CPU your library consumes? What will happen in case of a network partition or any other exceptional scenario? Can we see results of the load testing for your library? These questions are also applicable to the daemon solution. But it is easier to guarantee that the service itself will not be impacted by any bugs that may be in the Rate Limiter library. As you may see, strengths of the first approach become weaknesses of the second approach. And vice versa. So, which option is better? Both are good options and it really depends on the use cases and needs of a particular service team. By the way, the second approach, when we have a daemon that communicates with other hosts in the cluster is a quite popular pattern in distributed systems. For example, it is widely used to implement auto discovery of service hosts, when hosts in a cluster identify each other. What Else? In theory, it is possible that many token buckets will be created and stored in memory. For example, when millions of clients send requests at the same second. In practice though, we do not need to keep buckets in memory if there are no requests coming from the client for some period of time. For example, client made its first request and we created a bucket. As long as this client continues to send requests and interval between these requests is less than a second or couple of seconds, we keep the bucket in memory. If there are no requests coming for this bucket for several seconds, we can remove the bucket from memory. And bucket will be re-created again when client makes a new request. As for failure modes, there may be several of them. Daemon can fail, causing other hosts in the cluster lose visibility of this failed daemon. In the result, the host with a failed daemon leaves the group and continues to throttle requests without talking to other hosts in the cluster. Nothing really bad happens. Just less requests will be throttled in total. And we will have similar results in case of a network partition, when several hosts in the cluster may not be able to broadcast messages to the rest of the group. Just less requests throttled in total. And if you wonder why, just remember our previous example with 3 hosts and 4 tokens. If hosts talk to each other, only 4 requests are allowed across all of them. If hosts do not talk to each other due to let's say network issues, each host will allow 4 requests, 12 in total. So, in case of failures in our rate limiter solution, more requests are allowed and less requests are throttled. With regards to rule management, we may need to introduce a self-service tool, so that service teams may create, update and delete their rules when needed. As for synchronization, there may be several places where we need it. First, we have synchronization in the token bucket. There is a better way to implement thread-safety in that class, using for example atomic references. Another place that may require synchronization is the token bucket cache. As we mentioned before, if there are too many buckets stored in the cache and we want to delete unused buckets and re-create them when needed, we will end up with synchronization. So, we may need to use concurrent hash map, which is a thread safe equivalent of the hash map in Java. In general, no need to be afraid of the synchronization in both those places. It may become a bottleneck eventually, but only for services with insanely large requests per second rate. For most services out there even the simplest synchronization implementation does not add to much overhead. So, what clients of our service should do with throttled calls? There are several options, as always. Clients may queue such requests and re-send them later. Or they can retry throttled requests. But do it in a smart way, and this smart way is called exponential backoff and jitter. Probably too smart. But do not worry, ideas are quite simple. An exponential backoff algorithm retries requests exponentially, increasing the waiting time between retries up to a maximum backoff time. In other words, we retry requests several times, but wait a bit longer with every retry attempt. And jitter adds randomness to retry intervals to spread out the load. If we do not add jitter, backoff algorithm will retry requests at the same time. And jitter helps to separate retries. One Final Look Service owners can use a self-service tools for rules management. Rules are stored in the database. On the service host we have rules retriever that stores retrieved rules in the local cache. When request comes, rate limiter client builds client identifier and passes it to the rate limiter to make a decision. Rate limiter communicates with a message broadcaster, that talks to other hosts in the cluster. We wanted to build a solution that is highly scalable, fast and accurate. And at this point I would really like to say that the solution we have built meets all the requirements. But this is not completely true. And the correct answer is \"it depends\". Depends on the number of hosts in the cluster, depends on the number of rules, depends on the request rate. For majority of clusters out there, where cluster size is less then several thousands of nodes and number of active buckets per second is less then tens of thousands, gossip communication over UDP will work really fast and is quite accurate. In case of a really large clusters, like tens of thousands of hosts, we may no longer rely on host-to-host communication in the service cluster as it becomes costly. And we need a separate cluster for making a throttling decision. This is a distributed cache option we discussed above. But the drawback of this approach is that it increases latency and operational cost. It would be good to have these tradeoff discussions with your interviewer. As it demonstrates both breadth and depth of your knowledge and critical thinking.","title":"Rate Limiting (local and distributed)"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#rate-limiting-local-and-distributed","text":"","title":"Rate Limiting (local and distributed)"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#problem-statement-requirement-clarification","text":"Let's imaging we launched a web application. And the application became highly popular. Meaning that thousands of clients send thousands of requests every second to the front-end web service of our application. Everything works well. Until suddenly one or several clients started to send much more requests than they did previously. And this may happen due to a various of reasons. For example, our client is another popular web service and it experienced a sudden traffic spike. Or developers of that web service started to run a load test. Or this is just a malicious client who tried to DDoS our service. All these situations may lead to a so called \"noisy neighbor problem\", when one client utilizes too much shared resources on a service host, like CPU, memory, disk or network I/O. And because of this, other clients of our application start to experience higher latency for their requests, or higher rate of failed requests. One of the ways to solve a \"noisy neighbor problem\" is to introduce a rate limiting (also known as throttling). Throttling helps to limit the number of requests a client can submit in a given amount of time. Requests submitted over the limit are either immediately rejected or their processing is delayed. Some questions: This problem does not have a lot of sense. It should be solved by scaling out the cluster of hosts that run our web service. And ideally, by some kind of auto-scaling And the problem with scaling up or scaling out is that it is not happening immediately. Even autoscaling takes time. And by the time scaling process completes it may already be late. Our service may already crash. How rate limiting can be achieved? Specifically, you mention load balancers and their ability to limit a number of simultaneous requests that load balancer sends to each application server. Load balancers indeed may prevent too many requests to be forwarded to an application server. Load balancer will either reject any request over the limit or send the request to a queue, so that it can be processed later. But the problem with this mechanism - it is indiscriminate. Let's say our web service exposes several different operations. Some of them are fast operations, they take little time to complete. But some operations are slow and heavy and each request may take a lot of processing power. Load balancer does not have knowledge about a cost of each operation. And if we want to limit number of requests for a particular operation, we can do this on application server only, not at a load balancer level. The problem does not seem to be a system design problem. Algorithmic problem? Yes, as we need to define data structures and algorithm to count how many requests client has made so far. Object-oriented design problem? Probably, as we may need to design a set of classes to manage throttling rules. Rules define an allowed throttling limit for each operation. So, if we implement throttling for a single host, are we done? In an ideal world - yes. But not in the real world. If we have a load balancer in front of our web service and this load balancer spreads requests evenly across application servers and each request takes the same amount of time to complete. In this case this is a single instance problem and there is no need in any distributed solution. Application servers do not need to talk to each other. They throttle requests independently. But in the real-world load balancers cannot distribute requests in a perfectly even manner. Plus, as we discussed before different web service operations cost differently. And each application server itself may become slow due to software failures or overheated due to some other background process running on it. All this leads to a conclusion that we will need a solution where application servers will communicate with each other and share information about how many client requests each one of them processed so far.","title":"Problem Statement / Requirement Clarification"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#functional-requirements","text":"allowRequest(request) : For a given request our rate limiting solution should return a boolean value, whether request is throttled or not.","title":"Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#non-functional-requirements","text":"Low Latency (We need rate limiter to be fast (as it will be called on every request to the service) Accurate (We do not want to throttle customers unless it is absolutely required) Scalable (Rate limiter scales out together with the service itself, if we need to add more hosts to the web service cluster, this should not be a problem for the rate limiter) What about high availability and fault tolerance? Two common requirements for many distributed systems. Are they important for a rate limiting solution? Not so much. If rate limiter cannot make a decision quickly due to any failures, the decision is always not to throttle. If we do not know whether throttle or not -> we do not throttle. Because we may need to introduce rate limiting in many services, the interviewer may ask us to think about ease of integration. So that every service team in the organization can integrate with our rate limiting solution as seamlessly as possible. This is a good requirement.","title":"Non-Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#simple-solution","text":"It always makes sense to start/mention the simple solution and evolve the solution. Let's implement a solution for a single server. The first citizen of the rate limiting solution on the service host is the rules retriever. Each rule specifies a number of requests allowed for a particular client per second. These rules are defined by service owners and stored in a database. And there is a web service that manages all the operation with rules. Rules retriever is a background process that polls Rules service periodically to check if there are any new or modified rules. Rules retriever stores rules in memory on the host. When request comes, the first thing we need to do is to build a client identifier. Let's call it a key, for short. This may be a login for registered clients or remote IP address or some combination of attributes that uniquely identify the client. The key is then passed to the Rate Limiter component, that is responsible for making a decision. Rate Limiter checks the key against rules in the cache. And if match is found, Rate Limiter checks if number of requests made by the client for the last second is below a limit specified in the rule. If threshold is not exceeded, request is passed further for processing. If threshold is exceeded, the request is rejected. And there are three possible options in this case. Our service may return a specific response status code, for example service unavailable or too many requests. Or we can queue this request and process it later. Or we can simply drop this request on the floor. We know we need: A database to store the rules. And we need a service on top of this database for all the so-called CRUD operations (create, read, update, delete). We know we need a process to retrieve rules periodically and store rules in memory. And we need a component that makes a decision. You may argue whether we need the client identifier builder as a separate component or should it just be a part of the decision-making component. It is up to you. I wanted to present this builder as a separate component to stress the point that client identification is an important step of the whole process. From here interview may go in several directions. Interviewer may be interested in the Rate Limiter algorithm and ask us to implement one. Or interviewer may be interested in object-oriented design and ask us to define main classes and interfaces of the throttling library. Or interviewer may ask us to focus on a distributed throttling solution and discuss how service hosts share data between each other. Let's discuss each of these possible directions.","title":"Simple Solution"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#token-bucket-algorithm","text":"The token bucket algorithm is based on an analogy of a bucket filled with tokens. Each bucket has three characteristics: a maximum amount of tokens it can hold, amount of tokens currently available and a refill rate, the rate at which tokens are added to the bucket. Every time request comes, we take a token from the bucket. If there are no more tokens available in the bucket, request is rejected. The bucket is refilled with a constant rate. The beauty of the Token Bucket algorithm is that it simple to understand and simple to implement. There are 4 class fields: maximum bucket size, refill rate, number of currently available tokens and timestamp that indicates when bucket was last refilled. Constructor accepts two arguments: maximum bucket size and refill rate. Number of currently available tokens is set to the maximum bucket size. And timestamp is set to the current time in nanoseconds. Allow request method has one argument - number of tokens that represent a cost of the operation; usually, the cost is equal to 1, but it may be a larger value as well. The first thing we do is refilling the bucket. And right after that we check if there are enough tokens in the bucket. In case there are not enough tokens, method return false, indicating that request must be throttled. Otherwise, we need to decrease number of available tokens by the cost of the request. And the last piece is the refill method. It calculates how many tokens accumulated since the last refill and increases currently available tokens in the bucket by this number.","title":"Token Bucket Algorithm"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#leaky-bucket","text":"Leaky Bucket algorithm is opposite of Token Bucket algorithm. Leaky Bucket allows requests at a constant rate, the rate of the leak. There is no concept of a token, the requests go into the bucket and flow out at a constant rate. If there is no space in the bucket for the request, the request is discarded.","title":"Leaky Bucket"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#object-oriented-design","text":"Job Scheduler interface is responsible for scheduling a job that runs every several seconds and retrieves rules from Rules service. RulesCache interface is responsible for storing rules in memory. ClientIdentifier builds a key that uniquely identifies a client. And RateLimiter is responsible for decision making. RetrieveJobScheduler class implements JobScheduler interface. Its responsibility is to instantiate, start and stop the scheduler, and to run retrieve rules task. TokenBucketCache stores token buckets. We can use something simple, for example Map to store buckets. Or utilize 3-rd party cache implementation, like Google Guava cache. ClientIdentifierBuilder is responsible for building a key based on user identity information (for example login). There can be other implementations as well, for example based on IP address. And for the RateLimiter interface lets introduce a TokenBucketRateLimiter class, which is responsible for calling allow request on the correspondent bucket for that client. And the last important piece is the RetrieveRulesTask, which is responsible for retrieving all the rules for this service. Let's look at how these components interact with each other. RetrieveJobScheduler runs RetrieveRulesTask, which makes a remote call to the Rules service. It then creates token buckets and puts them into the cache. When client request comes to the host, RateLimiter first makes a call to the ClientIdentifierBuilder to build a unique identifier for the client. And then it passes this key to the cache and retrieves the bucket. And the last step to do is to call allow request on the bucket.","title":"Object Oriented Design"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#step-into-the-distributed-world","text":"We have a cluster that consists of 3 hosts. And we want rate limiting solution to allow 4 requests per second for each client. How many tokens should we give to a bucket on every host? Should we give 4 divided by 3? And the answer is 4. Each bucket should have 4 tokens initially. The reason for this is that all requests for the same bucket may in theory land on the same host. Load balancers try to distributed requests evenly, but they do not know anything about keys, and requests for the same key will not be evenly distributed. Let's add load balancer into the picture and run a very simple simulation. The first request goes to host A, one token is consumed. The second request goes to host C and one token is consumed there. Two other requests, within the same 1 second interval, go to host B. And take two tokens from the bucket. All 4 allowed requests hit the cluster; we should throttle all the remaining requests for this second. But we still have tokens available. What should we do? We must allow hosts to talk to each other and share how many tokens they consumed altogether. In this case host A will see that other two hosts consumed 3 tokens. And host A will subtract this number from its bucket. Leaving it with 0 tokens available. Host B will find out that A and C consumed two tokens already. Leaving host B with 0 tokens as well. And the same logic applies to host C. Now everything looks correct. 4 requests have been processed and no more requests allowed. We gave each bucket 4 tokens. If many requests for the same bucket hit our cluster exactly at the same second. Does this mean that 12 requests may be processed, instead of only 4 allowed? Or may be a more realistic scenario. Because communication between hosts takes time, until all hosts agree on what that final number of tokens must be, may there be any requests that slip into the system at that time? Yes. Unfortunately, this is the case. We should expect that sometimes our system may be processing more requests than we expect and we need to scale out our cluster accordingly. By the way, the token bucket algorithm will still handle this use case well. We just need to slightly modify it to allow negative number of available tokens. When 12 requests hit the system, buckets will start sharing this information. After sharing, every bucket will have -8 tokens and for the duration of the next 2 seconds all requests will be throttled. So, on average we processed 12 requests within 3 seconds. Although in reality all 12 were processed within the first second. So, communication between hosts is the key. Let's see how this communication can be implemented. By the way, ideas we will discuss next are applicable not only for rate limiting solution, but many other distributed systems that require data sharing between all hosts in a cluster in a real time.","title":"Step Into The Distributed World"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#message-broadcasting","text":"The first approach is to tell everyone everything . It means that every host in the cluster knows about every other host in the cluster and share messages with each one of them. You may also heard a term full mesh that describes this network topology. How do hosts discover each other? When a new host is added, how does everyone else know? And there are several approaches used for hosts discovery. One option is to use a 3-rd party service which will listen to heartbeats coming from every host. As long as heartbeats come, host is keep registered in the system. If heartbeats stop coming, the service unregister host that is no longer alive. And all hosts in our cluster ask this 3-rd party service for the full list of members. Another option is to resolve some user provided information. For example, user specifies a VIP and because VIP knows about all the hosts behind it, we can use this information to obtain all the members. Or we can rely on a less flexible but still a good option when user provides a list of hosts via some configuration file. We then need a way to deploy this file across all cluster nodes every time this list changes. Full mesh broadcasting is relatively straightforward to implement. But the main problem with this approach is that it is not scalable. Number of messages grows quadratically with respect to the number of hosts in a cluster. Approach works well for small clusters, but we will not be able to support big clusters. So, let's investigate some other options that may require less messages to be broadcasted within the cluster. And one such option is to use a gossip protocol . This protocol is based on the way that epidemics spread. Computer systems typically implement this type of protocol with a form of random \"peer selection\": with a given frequency, each machine picks another machine at random and shares data. By the way, rate limiting solution at Yahoo uses this approach. Next option is to use distributed cache cluster . For example, Redis. Or we can implement custom distributed cache solution. The pros for this approach is that distributed cache cluster is relatively small and our service cluster can scale out independently. This cluster can be shared among many different service teams in the organization. Or each team can setup their own small cluster. Next approach also relies on a 3-rd party component. A coordination service that helps to choose a leader. Choosing a leader helps to decrease number of messages broadcasted within the cluster. Leader asks everyone to send it all the information. And then it calculates and sends back the final result. So, each host only needs to talk to a leader or a set of leaders, where each leader is responsible for its own range of keys. Consensus algorithms such as Paxos and Raft can be used to implement Coordination Service. Great option, but the main drawback is that we need to setup and maintain Coordination Service. Coordination service is typically a very sophisticated component that has to be very reliable and make sure one and only one leader is elected. But is this really a requirement for our system? Let's say we use a simple algorithm to elect a leader. But because of the simplicity of the algorithm it may not guarantee one and only one leader. So that we may end up with multiple leaders being elected. Is this an issue? Actually, no. Each leader will calculate rate and share with everyone else. This will cause unnecessary messaging overhead, but each leader will have its own correct view of the overall rate.","title":"Message Broadcasting"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#how-do-we-integrate-all","text":"There are two options. And they are pretty standard. We can run Rate Limiter as a part of the service process or as its own process (daemon). In the first option, Rate Limiter is distributed as a collection of classes, a library that should be integrated with the service code. In the second option we have two libraries: the daemon itself and the client, that is responsible for inter-process communication between the service process and the daemon. Client is integrated with the service code. What are the pros for the first approach? It is faster, as we do not need to do any inter-process call. It is also resilient to the inter-process call failures, because there are no such calls. What are the pros for the second approach? The second approach is programming language agnostic. It means that Rate Limiter daemon can be written on a programming language that may be different from the language we use for the service implementation. As we do not need to do integration on the code level. Yes, we need to have Rate Limiter client compatible with the service code language. But not the daemon itself. Also, Rate Limiter process uses its own memory space. This isolation helps to better control behavior for both the service and the daemon. For example, daemon my store many buckets in memory, but because the service process has its own memory space, the service memory does not need to allocate space for these buckets. Which makes service memory allocation more predictable. Another good reason, and you may see it happening a lot in practice, service teams tend to be very cautious when you come to them and ask to integrate their service with your super cool library. You will hear tons of questions. Like how much memory and CPU your library consumes? What will happen in case of a network partition or any other exceptional scenario? Can we see results of the load testing for your library? These questions are also applicable to the daemon solution. But it is easier to guarantee that the service itself will not be impacted by any bugs that may be in the Rate Limiter library. As you may see, strengths of the first approach become weaknesses of the second approach. And vice versa. So, which option is better? Both are good options and it really depends on the use cases and needs of a particular service team. By the way, the second approach, when we have a daemon that communicates with other hosts in the cluster is a quite popular pattern in distributed systems. For example, it is widely used to implement auto discovery of service hosts, when hosts in a cluster identify each other.","title":"How do we integrate all"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#what-else","text":"In theory, it is possible that many token buckets will be created and stored in memory. For example, when millions of clients send requests at the same second. In practice though, we do not need to keep buckets in memory if there are no requests coming from the client for some period of time. For example, client made its first request and we created a bucket. As long as this client continues to send requests and interval between these requests is less than a second or couple of seconds, we keep the bucket in memory. If there are no requests coming for this bucket for several seconds, we can remove the bucket from memory. And bucket will be re-created again when client makes a new request. As for failure modes, there may be several of them. Daemon can fail, causing other hosts in the cluster lose visibility of this failed daemon. In the result, the host with a failed daemon leaves the group and continues to throttle requests without talking to other hosts in the cluster. Nothing really bad happens. Just less requests will be throttled in total. And we will have similar results in case of a network partition, when several hosts in the cluster may not be able to broadcast messages to the rest of the group. Just less requests throttled in total. And if you wonder why, just remember our previous example with 3 hosts and 4 tokens. If hosts talk to each other, only 4 requests are allowed across all of them. If hosts do not talk to each other due to let's say network issues, each host will allow 4 requests, 12 in total. So, in case of failures in our rate limiter solution, more requests are allowed and less requests are throttled. With regards to rule management, we may need to introduce a self-service tool, so that service teams may create, update and delete their rules when needed. As for synchronization, there may be several places where we need it. First, we have synchronization in the token bucket. There is a better way to implement thread-safety in that class, using for example atomic references. Another place that may require synchronization is the token bucket cache. As we mentioned before, if there are too many buckets stored in the cache and we want to delete unused buckets and re-create them when needed, we will end up with synchronization. So, we may need to use concurrent hash map, which is a thread safe equivalent of the hash map in Java. In general, no need to be afraid of the synchronization in both those places. It may become a bottleneck eventually, but only for services with insanely large requests per second rate. For most services out there even the simplest synchronization implementation does not add to much overhead. So, what clients of our service should do with throttled calls? There are several options, as always. Clients may queue such requests and re-send them later. Or they can retry throttled requests. But do it in a smart way, and this smart way is called exponential backoff and jitter. Probably too smart. But do not worry, ideas are quite simple. An exponential backoff algorithm retries requests exponentially, increasing the waiting time between retries up to a maximum backoff time. In other words, we retry requests several times, but wait a bit longer with every retry attempt. And jitter adds randomness to retry intervals to spread out the load. If we do not add jitter, backoff algorithm will retry requests at the same time. And jitter helps to separate retries.","title":"What Else?"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/rateLimiting/#one-final-look","text":"Service owners can use a self-service tools for rules management. Rules are stored in the database. On the service host we have rules retriever that stores retrieved rules in the local cache. When request comes, rate limiter client builds client identifier and passes it to the rate limiter to make a decision. Rate limiter communicates with a message broadcaster, that talks to other hosts in the cluster. We wanted to build a solution that is highly scalable, fast and accurate. And at this point I would really like to say that the solution we have built meets all the requirements. But this is not completely true. And the correct answer is \"it depends\". Depends on the number of hosts in the cluster, depends on the number of rules, depends on the request rate. For majority of clusters out there, where cluster size is less then several thousands of nodes and number of active buckets per second is less then tens of thousands, gossip communication over UDP will work really fast and is quite accurate. In case of a really large clusters, like tens of thousands of hosts, we may no longer rely on host-to-host communication in the service cluster as it becomes costly. And we need a separate cluster for making a throttling decision. This is a distributed cache option we discussed above. But the drawback of this approach is that it increases latency and operational cost. It would be good to have these tradeoff discussions with your interviewer. As it demonstrates both breadth and depth of your knowledge and critical thinking.","title":"One Final Look"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/","text":"Step By Step Guide Problem Statement / Gathering Requirements Today we discuss how to count things at a large scale. This could be count number of views on Youtube. More often, the problem will be stated in a general matter. For ex, we need to calculate application performance metrics. Even more generic, analyze data in real time. What does data analysis mean? Who sends us data? Who uses results of this analysis? What does real time really mean? This and many other questions need to be clarified. Even if problem seems clear to you, there are 2 big reasons why you still need to ask questions. First, interviewer wants to see how you deal with ambiguity, whether you can identify key pieces of the system and design/scope the problem. This is why questions are open ended. We should be clear what functional pieces of the problem we will focus on for rest of the interview. There may be many solutions to the problem asked. Only when we understand what features of the system we need to design, we can come up with proper technologies. Focus questions on 4 big categories: Users/Customers Who will use the system? All Youtube users? Video owner only? Used by some ML models to generate recommendations. How will the system be used? Used by marketing department only to generate monthly reports? If so, data is retrived not often. Or data is sent to recommendation service in real time. Data is reterived very often . Immediately, we have some more ideas about our system. Scale (read/write) Interviewer will help us define these numbers or maybe we have to calculate these on our own. We may be given daily # of users and given # actions per user/day and have to go from there. Performance What is expected write-to-read data delay? If time does not matter, we can use batch data processing. Otherwise, its too slow. How fast must data be retrieved from the system. If it must be as fast as possible, its a hint we must count views as we write data. Minimal to no counting as we read data. In other words, data must already be aggregated. Cost Should help us choose technology stack. For ex, if asked to minimize dev cost, we should lean towards open source frameworks. If maintanence cost is primary concern, we should use public cloud services. Functional Requirements After figuring out what the system should do, write down in a few sentences what the system has to do. These should include actions the system will make. We can expand from initial idea to make API more extensible. Non Functional Requirements Durability can be the 4th requirement after Scalable, Peformant, and Highly Available. Do we favor stale data over no data at all? Question of consistency. High Level Architecture Start with something simple. Database to store data. Web service to processes incoming view events and stores data in the database. To retrieve view count from the database, lets introduce another web service. At this point, we don't have a clear vision of the design so we throw a few high level componenets on the board. High chances interviewer is an expert in the field and knows the Q very well. We may not feel comfortable discussing any componenet just yet. Unless you are an expert, you may not be ready to answer their questions. That's why we need to start with something simple such as Data . What data do we want to store and how? Data Storage Defining A Data Model Which option should we store? Raw events or aggregate data in real time? This is where we need interviewer to help us make a decision. We should ask about expected data delay. Time between when event happened and when it was processed. If it should be no more than several minutes, we must aggregate data on the fly. If few hours is ok, we can store raw events and process them in the background. Former approach is known as stream data processing while latter is known as batch data processing . For sake of this guide, lets choose both options. Btw, combining both approaches makes alot of sense for many systems. We will store raw events and because there are so many of them we will store them for several days or weeks before purging data and sending to longer term storage systems if needed. We will calculate and store numbers in real time so that statistics are available for users right away. By storing both raw events and aggregated data, we get the bost of both worlds. Fast reads, ability to aggregate data differently, and recalculate statistics if there were bugs in real time path. There is a price to pay for this flexibility. Complexity and cost. Discuss with interviewer. Where we store data Interviewer may ask specific database type / name. We should evaluate both SQL and NoSQL options. This is where Non-Functional requirements come in. Scalability, Availability, and Performance. We should evaluate databases against these requirements. Lets add more requirements along the way. SQL database (MySQL) Things are simple when we can store all data on a single machine. When its not enough, we need to shard or horizontal partitioning . Now that we have several machines, services that talk to the database need to know how many machines exist and which one to pick to store and retrieve data. Instead of making both services call databases directly, we introduce a Cluster Proxy that knows about all database machines and routes traffic to the correct shard. How does cluster proxy know when some shard dies or becomes unavailable due to network partition? Or when new shard is added to the database cluster. We introduce a new componenet called Configuration Service which maintains a health connection to all shards so that it knows which machines are available. ZooKeeper allows Configuration Service to know this. See ZooKeeper . Finally we have Shard Proxy which sits infront of a database. It can cache query results, monitor database instance health, and publish metrics. This setup helps us address Scalability and Performance. But Availability is not yet addressed? What if shard dies? How do we ensure data is not lost? Replication . Replicate the data in other datacenters so if whole DC goes down, we still have data available. So when store data request comes, based on information returned by Configuration Service , Cluster Proxy sends data to a shard. Data is synchronously or asynchronously sent to the replicas. Writes go to leader while reads to go to the followers. Great, we now know how to scale SQL databases. But this solution doesn't seem simple. We have all this proxies, configuration services, leaders, replicas. Maybe we can simply things. Lets look at what NoSQL can offer us, specifically Apache Cassandra. NoSQL Database (Cassandra) In NoSQL world we also split data into chunks/shards. Instead of having leaders/followers we say that each shard is equal. We no longer need configuration service to monitor health of each shard. Let's instead allow shards to talk to each other and exchange information about their state. To reduce network load, we don't need each shard to talk to every other shard. Let's instead use a gossip protocl where nodes only talk to few nodes (<3) every second. This is a big deal. Previously we used Cluster Proxy to figure out which shard has which data. Now, all nodes know about each other. Clients may simply call any node in the cluster and the node itself will decide where to forward the request. Processing Service makes a call to store views count for some video B. Node 4 is selected to serve this request. Choose a node that is closest to teh client in terms of network distance. 4 is now a Coordinator Node . Coordinator Node needs to decide which node stores data for the requested video. We can use Consistent Hashing algorithm to pick the node. Node 1 stores the data for the video B. CN will also replicate data in next 2-3 nodes in consistent hashing ring. Consider idea of Quorum Read and Quorum Write to determine number of nodes we need for read/write requests to be successful. For high availability, we also create replicas in other data centers. Look at Distributed Cache for Consistent Hashing 4 Types of NoSQL Databases. Column, Document, Key-Value, and Graph. We chose Cassandra because its fault tolerant, scalable, both read and write throughput increases lineraly as new machines are added. It supports multi datacenter replication and works well with time series data. It is a wide column database that supports asynchronous masterless replication but other NoSQL databases have different architecutres. MongoDB is document oriented with leader base replication. HBase is another column oriented similar to Cassandra with a leader based architecture aswell. How we store data In relational database, data is normalized. We minimize data duplication. NoSQL databases promote different paradigm. We think in terms of queries. Instead of adding rows for every next hour, we add columns. Great, we have coveraged the storage portion of our design. Processing Service Let's define what processing really means. When YouTube user opens some video, we want total views count for this video to be displayed immediately. It means we need to calculate views count on the fly, in real time. When video owner opens statistics, we need to show per hour counts. So processing basically means we get a video view event and we update 2 counters, total and per hour. Where to start? As usual, Start with the Requirements . We want the processing service to scale together with increase in video views. Partitioning We don't want to lose data incase of failures. Replicate We want to process events quickly. In Memory. Minimize disk reads Before diving into Processing Service detailed design, lets agree on some basics. Data Aggregation Basics Should we increment data in database for each event or accumulate data in processing service memory and add accumulate value to database counter? Aggregate data in memory is better. Less database writes. Push or pull? Should something send events synchronously to Processing Service or should Processing Service pull data from temporary storage? Although both work, Pull option has more advantages as it provides more fault tolerance support and its easier to scale. Messages can be kept in queue where Processing Service deletes item from storage once it updates in memory counters. If machine crashes, it will never ACK message so it stays in storage. We can use distributed queue such as Kafka or SQS. This part is optional as Kafka or SQS should provide this functionality for us. When events arrive, we put items in order to storage. Fixed order allows us to assign an offset for each item in storage. Events are always consumed sequentially. Every time event is read, offset moves forward. After we process several events and store them in database, we write checkpoint to persistent storage. If PS fails, we will replace it and it will resume from checkpoint offset. Most likely we dont need FIFO order. Instead of putting all events into single queue, we create multiple. Assign items to queues based on a hash. Partitioning allows us to parallelize event processing . We need to ensure certain hosts only read from certain queues. All \"A\" videos go to \"A\" queue which is processed from \"A\" PS. Processing Service Detailed Design We discussed so far that processing service reads events from partition one by one, counts events in memory, and flushes this counted values to the database periodically. So, we need a component to read events. Lets say consumer is a single thread. We could use multiple threads but we need to use concurrent hashmaps. The consumer establishes and maintains TCP connection with the partition to fetch data. When consumer reads event it deserializes it. Meaning it converts byte array into the actual object. Consumer does one more important thing - helps to eliminate duplicate events. Usually queues support atleast once delivery. To achieve this we use a distributed cache that stores unique event identifiers for let's say last 10 minutes. Event then comes to the component that does in-memory counting. Let's call it aggregator. Think of it as a hash table that accumulates data for some period of time. Periodically, we stop writing to the current hash table and create a new one. A new hash table keeps accumulating incoming data. While old hash table is no longer counting any data and each counter from the old hash table is sent to the internal queue for further processing. Why do we need this internal queue? Why can't we send data directly to the database. Glad you asked. Remember, we have a single thread that reads events from the partition. But nothing stops us from processing these events by multiple threads, to speed up processing. Especially if processing takes time. However, if we have many threads for readers, we don't need this. By sending data to the internal queue we decouple consumption and processing . You may argue whether we should put internal queue before Aggregator component. Both options are fine. Ok, we now ready to send pre-aggregated values to the database. So, we need a component responsible for this. Database writer is either a single-threaded or a multi-threaded component. Each thread takes a message from the internal queue and stores pre-aggregated views count in the database. Single-threaded version makes checkpointing easier. But multi-threaded version increases throughput. Meanwhile, I would like to point out two more important features of the database writer. The first concept is called a dead letter queue . The dead-letter queue is the queue to which messages are sent if they cannot be routed to their correct destination. Why do you think we may need one? To protect ourselves from database performance or availability issues. If database becomes slow or we cannot reach database due to network issues, we simply push messages to the dead letter queue. And there is a separate process that reads messages from this queue and sends them to the database. This concept is widely used when you need to preserve data in case of downstream services degradation. So, you may apply it in many system designs. Another viable option is to store undelivered messages on a local disk of the processing service machine. The second concept is data enrichment. Remember how we store data in Cassandra? We store it the way data is queried, right? If we want for example to show video title in the report, we need to store video title together with views count. The same is true for the channel name and many other attributes that we may want to display. But all these attributes do not come to the processing service with every video view event. Event contains minimum information, like video identifier and timestamp. It does not need to contain video title or channel name or video creation date. So, these information comes from somewhere else, right? Some database. But the trick here is that this database lives on the same machine as the processing service. All these additional attributes should be retrieved from this database really quickly. Thus, having it on the same machine eliminates a need for remote calls. Such databases are called embedded databases. One last concept I would like to mention is state management. We keep counters in memory for some period of time. Either in in-memory store or internal queue. And every time we keep anything in memory we need to understand what to do when machine fails and this in-memory state is lost . But that is easy, you may say. We have events stored in the partition, let's just re-create the state from the point where we failed. In other words we just re-process events one more time. This is a good idea. And it will work well if we store data in-memory for a relatively short period of time and state is small. Sometimes it may be hard to re-create the state from raw events from scratch. The solution in this case is to periodically save the entire in-memory data to a durable storage. And new machine just re-loads this state into its memory when started. Data Ingestion Path We know already that we have a set of partitions and processing service reads events from them, count data in memory for some short period of time and stores total count in the database. Someone needs to distribute data across partitions, right? Let's have a component called Partitioner service . Let's also have a load balancer component in front of our partitioner service to evenly distribute events across partitioner service machines. When user opens a video, request goes through API Gateway , component that represents a single-entry point into a video content delivery system. API Gateway routes client requests to backend services. Our counting system may be one of such backend services.And one more important component to mention is the partitioner service client . We talked about database and processing service in details. Now let's cover remaining 3 components of the data ingestion path: partitioner service client, load balancer and partitioner service. Partitioner Service Client Blocking vs Non-Blocking I/O When client makes a request to a server, server processes the request and sends back a response. The client initiates the connection by using sockets .When a client makes a request, the socket that handles that connection on the server side is blocked. This happens within a single execution thread. So, the thread that handles that connection is blocked as well. And when another client sends a request at the same time, we need to create one more thread to process that request. This is how blocking systems work. They create one thread per connection. Modern multi-core machines can handle hundreds of concurrent connections each. But let's say server starts to experience a slow down and number of active connections and threads increases. When this happens, machines can go into a death spiral and the whole cluster of machines may die. Remember we designed a rate limiter in one of the previous videos. That is exactly why we need rate limiting solutions, to help keep systems stable during traffic peeks. Alternative to blocking I/O is non-blocking I/O. When we can use a single thread on the server side to handle multiple concurrent connections. Server just queues the request and the actual I/O is then processed at some later point. Piling up requests in the queue are far less expensive than piling up threads. Non-blocking systems are more efficient and as a result has higher throughput. You may be wondering that if non-blocking systems are so great, why we still have so many blocking systems out there? The price of non-blocking systems is increased complexity of operations. Blocking systems are easy to debug. In blocking systems we have a thread per request and we can easily track progress of the request by looking into the thread's stack. Exceptions pop up the stack and it is easy to catch and handle them. We can use thread local variables in blocking systems. All these familiar concepts either do not work at all or work differently in the non-blocking world. Buffering and Batching There are thousands of video view events happening on Youtube every second. To process all these requests, API Gateway cluster has to be big in size. If we then pass each individual event to the partitioner service, partitioner service cluster of machines has to be big as well. This is not efficient. We should somehow combine events together and send several of them in a single request to the partitioner service. This is what batching is about. Instead of sending each event individually, we first put events into a buffer. We then wait up to several seconds before sending buffer's content or until batch fills up, whichever comes first. There are many benefits of batching: it increases throughput, it helps to save on cost, request compression is more effective. But there are drawbacks as well. It introduces some complexity both on the client and the server side. For example think of a scenario when partitioner service processes a batch request and several events from the batch fail, while other succeed. Should we re-send the whole batch? Timeouts Timeouts define how much time a client is willing to wait for a response from a server. We have two types of timeouts: connection timeout and request timeout. Connection timeout defines how much time a client is willing to wait for a connection to establish. Usually this value is relatively small, tens of milliseconds. Because we only try to establish a connection, no heavy request processing is happening just yet. Request timeout happens when request processing takes too much time, and a client is not willing to wait any longer. To choose a request timeout value we need to analyze latency percentiles. For example we measure latency of 1% of the slowest requests in the system. And set this value as a request timeout. It means that about 1% of requests in the system will timeout. And what should we do with these failed requests? Let's retry them. May be we just hit a bad server machine with the first request. And the second attempt may hit a different server machine, increasing our chances to succeed. But we should be smart when retry. Because if all clients retry at the same time or do it aggressively, we may create a so-called retry storm event and overload sever with too many requests. To prevent this, we should use exponential backoff and jitter algorithms. Exponential backoff algorithm increases the waiting time between retries up to a maximum backoff time. We retry requests several times, but wait a bit longer with every retry attempt. And jitter adds randomness to retry intervals to spread out the load. If we do not add jitter, backoff algorithm will retry requests at the same time. And jitter helps to separate retries. Even with exponential backoff and jitter we may still be in danger of too many retries. For example when partitioner service is down or degraded. And majority of requests are retried. Circuit Breaker The Circuit Breaker pattern stops a client from repeatedly trying to execute an operation that's likely to fail. We simply calculate how many requests have failed recently and if error threshold is exceeded we stop calling a downstream service. Some time later, limited number of requests from the client are allowed to pass through and invoke the operation. If these requests are successful, it's assumed that the fault that was previously causing the failure has been fixed. We allow all requests at this point and start counting failed requests from scratch. The Circuit Breaker pattern also has drawbacks. For example, it makes the system more difficult to test. And it may be hard to properly set error threshold and timers. By the way, have you noticed that everything we discussed so far has tradeoffs? None of these concepts is a silver bullet. This is true for almost all concepts in distributed systems. We should always know and remember about tradeoffs. Load Balancer Software vs Hardware Load Balancing Hardware load balancers Network devices we buy from known organizations. Theses are powerful machines with many CPU cores, memory and they are optimized to handle very high throughput. Millions of requests per second. Software load balancer Software that we install on hardware we choose. We do not need big fancy machines, and many software load balancers are open source. Load balancers provided by public clouds (for example ELB from AWS ) are examples of software load balancer type as well. Networking Protocols TCP Load Balancers TCP load balancers simply forward network packets without inspecting the content of the packets. This allows TCP load balancers to be super fast and handle millions of requests per second. Think of it as if we established a single end-to-end TCP connection between a client and a server. HTTP load balancers On contrast, terminate the connection. Load balancer gets an HTTP request from a client, establishes a connection to a server and sends request to this server. HTTP load balancer can look inside a message and make a load\u2011balancing decision based on the content of the message. For example based on a cookie information or a header. Load Balancing Algorithms Round robin Distributes requests in order across the list of servers. Least connections Sends requests to the server with the lowest number of active connections. Least response time Sends requests to the server with the fastest response time. Hash-based Distribute requests based on a key we define, such as the client IP address or the request URL. DNS Ok, you got it, there are many benefits of using load balancers. Let's return to our original system design problem and address several very specific questions. Such as How does our partitioner service client know about load balancer? How does load balancer know about partitioner service machines? How does load balancer guarantee high availability? Because it looks like a single point of failure, right? Here is where we should recall DNS, Domain Name System . DNS is like a phone book for the internet. It maintains a directory of domain names and translate them to IP addresses. We register our partitioner service in DNS, specify domain name, for example partitionerservice.domain.com and associate it with IP address of the load balancer device. So, when clients hit domain name, requests are forwarded to the load balancer device. For the load balancer to know about partitioner service machines, we need to explicitly tell the load balancer the IP address of each machine. Both software and hardware load balancers provides API to register and unregister servers. Load balancers need to know which server from the registered list are healthy and which are unavailable at the moment. This way load balancers ensure that traffic is routed to healthy servers only. Load balancer pings each server periodically and if unhealthy server is identified, load balancer stops to send traffic to it. It will then resume routing traffic to that server when it detects that the server is healthy again. As for high availability of load balancers, they utilize a concept of primary and secondary nodes. The primary load balancer accepts connections and serves requests, while the secondary load balancer monitors the primary. If, for any reason, the primary load balancer is unable to accept connections, the secondary one takes over. Primary and secondary also live in different data centers, in case one data center goes down. Partitioner Service and Partitions Partitioner service is a web service that gets requests from clients, looks inside each request to retrieve individual video view events (because remember we batch events on the client side), and routs each such event (we can also use the word message) to some partition. But what partitions are? Partitions is also a web service, that gets messages and stores them on disk in the form of the append-only log file. So, we have a totally-ordered sequence of messages ordered by time. This is not a single very large log file, but a set of log files of the predefined size. Partition Strategy Partitioner service has to use some rule, partition strategy, that defines which partition gets what messages. A simple strategy is to calculate a hash function based on some key, let's say video identifier and chose a machine based on this hash. This simple strategy does not work very well with large scale. As it may lead to so called \"hot partitions\". For example when we have a very popular video or set of videos and all view events for them go to the same partition. One approach to deal with hot partitions is to include event time, for example in minutes, into partition key. All video events within the current minute interval are forwarded to some partition. Next minute, all events go to a different partition. Within one minute interval a single partition gets a lot of data, but over several minutes data is spread more evenly among partitions. Another solution to hot partitions problem is to split hot partition into two new partitions . To get an idea how this approach might work, remember consistent hashing algorithm and how adding a new node to the consistent hashing ring splits a range of keys into two new ranges. And if to push this idea of partition split even further, we may explicitly allocate dedicated partitions for some popular video channels. All video view events from such channels go to their allocated partitions. And view events from all other channels never go to those partitions. These are the powerful techniques and there is little information on the \"hot partitions\" topic on the internet. Service Discovery To send messages to partitions, partitioner service needs to know about every partition. This is where the concept of service discovery comes on stage. In the world of microservices there are two main service discovery patterns: server-side discovery and client-side discovery. We already looked at server-side discovery when talked about load balancers. Clients know about load balancer, load balancer knows about server-side instances. Easy. But we do not need a load balancer between partitioner service and partitions. Partitioner service itself acts like a load balancer by distributing events over partitions. This is a perfect match for the client-side discovery pattern. With client-side discovery every server instance registers itself in some common place, named service registry . Service registry is another highly available web service, which can perform health checks to determine health of each registered instance. Clients then query service registry and obtain a list of available servers. Example of such registry service is Zookeeper . In our case each partition registers itself in Zookeeper, while every partitioner service instance queries Zookeeper for the list of partitions. Zookeeper allows watch flag to receive updates in case of changes using ephemeral nodes. We also discussed client-side service discovery pattern in Distributed Cache Design although we did not name it there that way When cache client needs to pick a cache shard that stores the data. There we discussed several other options as well. One more option for service discovery is similar to what Cassandra does. Remember we mentioned before that Cassandra nodes talk to each other? So, every node in the cluster knows about other nodes. It means clients only need to contact one node from the server cluster to figure out information about the whole cluster. Replication We must not lose events when store them in partitions. So, when event is persisted in a partition, we need to replicate it. If this partition machine goes down, events are not lost. There are three main approaches to replication: single leader replication, multi leader replication and leaderless replication. Do you remember where we used single leader replication? When we discussed how to scale a SQL database . Do you remember when we talked about leaderless replication? When we discussed how Cassandra works, right? So far we did not talk about multi leader replication. Let me make a separate deep dive into replication topic. And only mention right now that multi leader replication is mostly used to replicate between several data centers. So, which approach should we chose for partitions? Let's go with single leader replication. Each partition will have a leader and several followers. We always write events and read them from the leader only. While a leader stays alive, all followers copy events from their leader. And if the leader dies, we choose a new leader from its followers. The leader keeps track of its followers: checks whether the followers are alive and whether any of the followers is too far behind. If a follower dies, gets stuck, or falls behind, the leader will remove it from the list of its followers. Remember a concept of a quorum write in Cassandra? We consider a write to be successful, when predefined number of replicas acknowledge the write. Similar concept applies to partitions. When partitioner service makes a call to a partition, we may send response back as soon as leader partition persisted the message, or only when message was replicated to a specified number of replicas. When we write to a leader only, we may still lose data if leader goes down before replication really happened. When we wait for the replication to complete, we increase durability of the system, but latency will increase. Plus, if required number of replicas is not available at the moment, availability will suffer. Message Formats We can use either textual or binary formats for messages. Popular textual formats are XML, CSV, JSON . Popular binary formats are Thrift, Protocol Buffers and Avro . What's great about textual formats - they are human-readable. They are well-known, widely supported and heavily used by many distributed systems. But for the large scale real-time processing systems binary formats provide much more benefits. Messages in binary format are more compact and faster to parse. And why is this? As mentioned before, messages contain several attributes, such as video identifier, timestamp, user related information. When represented in JSON format, for example, every message contains field names, which greatly increases total message size. Binary formats are smarter. Formats we mentioned before require a schema. And when schema is defined we no longer need to keep field names. For example Apache Thrift and Protocol Buffers use field tags instead of field names. Tags are just numbers and they act like aliases for fields. Tags occupy less space when encoded. Schemas are crucial for binary formats. Message producers (or clients) need to know the schema to serialize the data. Message consumers (processing service in our case) require the schema to deserialize the message. So, schemas are usually stored in some shared database where both producers and consumers can retrieve them. Schemas may and will change over time. We may want to add more attributes into messages and use them later for counting or filtering. Apache Avro is a good choice for our counting system. Data Retrieval When users open a video on Youtube, we need to show total views count for this video. To build a video web page, several web services are called. A web service that retrieves information about the video, a web service that retrieves comments, another one for recommendations. Among them there is our Query web service that is responsible for video statistics. All these web services are typically hidden behind an API Gateway service, a single-entry point. API Gateway routes client requests to backend services. So, get total views count request comes to the Query service. We can retrieve the total count number directly from the database. Remember we discussed before how both SQL and NoSQL databases scale for reads. But total views count scenario is probably the simplest one. This is just a single value in the database per video. The more interesting use case is when users retrieve time-series data, which is a sequence of data points ordered in time. For example, when channel owner wants to see statistics for her videos. As discussed before, we aggregate data in the database per some time interval, let's say per hour. Every hour for every video. That is a lot of data, right? And it grows over time. Fortunately, this is not a new problem and solution is known. Monitoring systems, for example, aggregate data for every 1 minute interval or even 1 second. You can imaging how huge those data sets can be. So, we cannot afford storing time series data at this low granularity for a long period of time. The solution to this problem is to rollup the data . For example, we store per minute count for several days. After let's say one week, per minute data is aggregated into per hour data. And we store per hour count for several months. Then we rollup counts even further and data that is older than let's say 3 months, is stored with 1 day granularity. And the trick here is that we do not need to store old data in the database. We keep data for the last several days in the database, but the older data can be stored somewhere else, for example, object storage like AWS S3. In the industry, you may also hear terms like a hot storage and a cold storage. Hot storage Frequently used data that must be accessed fast. Cold storage Doesn\u2019t require fast access. It mostly represents archived and infrequently accessed data. When request comes to the Query service, it does so-called data federation, when it may need to call several storages to fulfill the request. Most recent statistics is retrieved from the database, while older statistics is retrieved from the Object Storage. Query service then stitches the data. And this is ideal use case for the cache. We should store query results in a distributed cache. This helps to further improve performance of queries and scale them. Data Flow Simulation Three users opened some video A. And API Gateway got 3 requests. Partitioner service client batches all three events and sends them in a single request to the partitioner service. This request hits the load balancer first. And load balancer routes it to one of the partitioner service machines. Partitioner service gets all three events from the request and sends them to some partition All three events end up in the same partition, as we partition data based on the video identifier. Here is where processing service appears on the stage. Partition consumer reads all three messages from the partition one by one and sends them to the aggregator. Aggregator counts messages for a one minute period and flushes calculated values to the internal queue at the end of that minute. Database writer picks count from the internal queue and sends it to the database. In the database we store count per hour and the total number of views for each video. So, we just add a one minute value to the current hour count as well as the total count. Total count was 7 prior to this minute and we add 3 for the current minute. And during data retrieval, when user opens video A, API Gateway sends request to the Query service. Query service checks the cache. And if data is not found in the cache, or cache value has expired, we call the database. Total count value is then stored in the cache and Query service returns the total count back to the user. We may need cache invalidation but if we keep low TTL it may be ok to show slightly stale data. Other Interview Aspects Another important aspect of an interview and system design in general is a technology stack. When we design some system, we usually do not need to reinvent the wheel. We rely on some well-regarded technologies. Either open source or commercial. Public cloud services. During the interview do not forget to discuss these technologies. You may do this along the way or at the end of the interview. So, let's see what technologies we may use for our solution. Client Side Netty is a high-performance non-blocking IO framework for developing network applications, both clients and servers. Frameworks such as Hystrix from Netflix and Polly simplify implementation of many client-side concepts we discussed before: timeouts, retries, circuit breaker pattern. Load Balancers Citrix Netscaler is probably the most famous hardware load balancer. NGINX is a very popular software load balancer. And if we run our counting system in the cloud, for example Amazon cloud, then Elastic Load Balancer is a good pick. Messaging Systems Instead of using our custom Partitioner service and partitions, we could use Apache Kafka instead. Or Kafka's public cloud counterpart, like Amazon Kinesis . To process events and aggregate them in memory we can use stream-processing frameworks such as Apache Spark or Flink . Or cloud-based solutions, such as Kinesis Data Analytics . Storage We already talked about Apache Cassandra. Another popular choice for storing time-series data is Apache HBase database. These are wide column databases. There are also databases optimized for handling time series data, like InfluxDB . We also mentioned before that we may need to store raw events to recalculate counts in case of any error or if customers need to run ad-hoc queries. We can store raw events in Apache Hadoop or in a cloud data warehouse, such as AWS Redshift . And when we roll up the data and need to archive it, AWS S3 is a natural choice. Other Vitess is a database solution for scaling and managing large clusters of MySQL instances Vitess has been serving all Youtube database traffic since 2011. In several places of our design we rely on a distributed cache : for message deduplication and to scale read data queries. Redis is a good option. For a dead-letter queue mechanism, when we need to temporarily queue undelivered messages, we may use an open-source message-broker such as RabbitMQ . Or public cloud alternative, such as Amazon SQS . For data enrichment, when we store video and channel related information locally on the machine and inject this information in real-time, we may use RocksDB , a high performance embedded database for key-value data. To do leader election for partitions and to manage service discovery, we may rely on Apache Zookeeper , which is a distributed configuration service. For the service discovery piece we actually have an alternative, Eureka web service from Netflix. To monitor each of our system design components we may rely on monitoring solutions provided by public cloud services, such as AWS CloudWatch . Or use a popular stack of open source frameworks: Elasticsearch, Logstash, Kibana. Or ELK for short. We discussed before that binary message format is preferred for our system. Popular choices are Thrift, Protobuf and Avro . For Partitioner service to partition the data, we should use a good hashing function, for example a MurmurHash . Follow Up Questions We are done with the detailed system design. And here is where our interviewer will start challenging us with the choices we have made. We discussed many tradeoffs of individual components throughout this video. Let's see what else the interviewer may want to discuss with us. Bottlenecks To identify bottlenecks in the system we need to test it under a heavy load. This is what performance testing is about. There are several types of performance testing. We have: Load Testing when we measure behavior of a system under a specific expected load. Stress Testing when we test beyond normal operational capacity, often to a breaking point. Soak Testing when we test a system with a typical production load for an extended period of time. With load testing we want to understand that our system is indeed scalable and can handle a load we expect. For example, a two or three times increase in traffic. With stress testing we want to identify a breaking point in the system. Which component will start to suffer first. And what resource it will be: memory, CPU, network, disk IO . And with soak testing we want to find leaks in resources. For example, memory leaks.So, generating high load is the key. Tools like Apache JMeter can be used to generate a desired load. Health monitoring. Health Monitoring All the components of our system must be instrumented with monitoring of their health. Metrics, dashboards and alerts should be our friends all the time. Metric is a variable that we measure, like error count or processing time. Dashboard provides a summary view of a service\u2019s core metrics. And alert is a notification sent to service owners in a reaction to some issue happening in the service. Remember about the four golden signals of monitoring, which are latency , traffic , errors , and saturation . Ensure Accurate Results Ok, we designed a system and deployed all the components. We know it is running healthy and can handle a high load. But how to make sure it counts things correctly? This becomes critical when we not just count video views, but, for example, number of times some ad was played in a video. As we need to properly charge an ad owner and pay money to a video owner. This problem is typically addressed by building an audit system. There can be two flavors of audit systems. Let's call them weak and strong. Weak audit system is a continuosly running end-to-ed test. When let's say once a minute we generate several video view events in the system, call query service and validate that returned value equals to the expected count. This simple test gives us a high confidence that the system counts correctly. And it is easy to implement and maintain such test. But unfortunately, this test is not 100% reliable. What if our system loses events in some rare scenarios? And weak audit test may not identify this issue for a long period of time. That is why we may need a better approach. Strong audit system calculates video views using a completely different path then out main system. For example we store raw events in Hadoop and use MapReduce to count events. And then compare results of both systems. Having two different systems doing almost the same may seem like an overkill, right? You may be surprised but this is not so uncommon in practice. Not such a long time ago it was quite a popular idea. And it even has a name - Lambda Architecture . The key idea is to send events to a batch system and a stream processing system in parallel. And stitch together the results from both systems at query time. See Top K Heavy Hitters Ideally, we should have a single system. Let me share with you advice from Jay Kreps, who is one of the authors of Apache Kafka. Use a batch processing framework like MapReduce if we aren\u2019t latency sensitive Use a stream processing framework if we are, but not to try to do both at the same time unless we absolutely must . And please note that out today's problem can indeed be solved with MapReduce. But MapReduce-based system would have a much higher latency. Avoid Bottleneck with Extremely Popular Videos We already discussed the problem with popular videos. I will just reiterate the key idea. We have to spread events coming for a popular video across several partitions. Otherwise, a single consumer of a single \"hot\" partition may not be able to keep up with the load. And will fall behind. Situation when the processing service cannot keep up with the load Maybe because number of events is huge, maybe because processing of a single event is complicated and time consuming.I will not dive too much into details, but describe the main idea of the solution. We batch events and store them in the Object Storage service , for example AWS S3. Every time we persist a batch of events, we send a message to a message broker. For example SQS. Then we have a big cluster of machines, for example EC2, that retrieve messages from SQS, read a corresponding batch of events from S3 and process each event. This approach is a bit slower than stream processing, but faster than batch processing. Summary We start with requirements clarification. And more specifically, we need to define Functional Requirements / APIs, what exactly our system is supposed to do. We then discuss non-functional requirements with the interviewer and figure out what qualities of the system she is most interested in. We can now outline a high-level architecture of the system. Draw some key components on the whiteboard. At the next stage we should dive deep into several of those components. Our interviewer will help us understand what components we should focus on. And the last important step is to discuss bottlenecks and how to deal with them. Functional Requirements / API To define APIs, we discuss with the interviewer what specific behaviors or functions of the system we need to design. We write down verbs characterizing these functions and start thinking about input parameters and return values. We then can make several iterations to brush up the APIs. After this step we should be clear on what the scope of the design is. NonFunctional Requirements To define non-functional requirements, just know what your options are. Open a list of non-functional requirements on wiki and read the list. There are many of them. I recommend to focus on scalability , availability and performance . Among other popular choices we have consistency , durability , maintainability and cost. Try to pick not more than 3 qualities. HLD To outline a high-level design, think about how data gets into the system, how it gets out of the system and where data is stored inside the system. Draw these components on the whiteboard. It is ok to be rather generic at this stage. Details will follow later. And although it is not easy, try to drive the conversation . Our goal here is to get understanding of what components to focus on next. And the interviewer will help us. Detailed Design While designing specific components, start with data . How it is stored, transferred and processed. Here is where our knowledge and experience becomes critical. By using fundamental concepts of system design and by knowing how to combine these concepts together, we can make small incremental improvements. And apply relevant technologies along the way. After technical details are discussed, we can move to discussing other important aspects of the system. Listen carefully to the interviewer. She sees bottlenecks of our design and in her questions there will be hints what those bottlenecks are. And what can really help us here is the knowledge of different tradeoffs in system design. We just need to pick and apply a proper one. Although we talked about a specific problem today, like video views counting, the same ideas can be applied to other problems, for example counting likes, shares, reposts, ad impressions and clicks. The same ideas can be applied to designing monitoring systems, when we count metrics. When we design a fraud prevention system we need to count number of times each credit card was used recently. When we design recommendation service we may use counts as input to machine learning models. When we design \"what's trending\" service, we count all sorts of different reactions: views, re-tweets, comments, likes. And many other applications.","title":"Step By Step Guide"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#step-by-step-guide","text":"","title":"Step By Step Guide"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#problem-statement-gathering-requirements","text":"Today we discuss how to count things at a large scale. This could be count number of views on Youtube. More often, the problem will be stated in a general matter. For ex, we need to calculate application performance metrics. Even more generic, analyze data in real time. What does data analysis mean? Who sends us data? Who uses results of this analysis? What does real time really mean? This and many other questions need to be clarified. Even if problem seems clear to you, there are 2 big reasons why you still need to ask questions. First, interviewer wants to see how you deal with ambiguity, whether you can identify key pieces of the system and design/scope the problem. This is why questions are open ended. We should be clear what functional pieces of the problem we will focus on for rest of the interview. There may be many solutions to the problem asked. Only when we understand what features of the system we need to design, we can come up with proper technologies. Focus questions on 4 big categories: Users/Customers Who will use the system? All Youtube users? Video owner only? Used by some ML models to generate recommendations. How will the system be used? Used by marketing department only to generate monthly reports? If so, data is retrived not often. Or data is sent to recommendation service in real time. Data is reterived very often . Immediately, we have some more ideas about our system. Scale (read/write) Interviewer will help us define these numbers or maybe we have to calculate these on our own. We may be given daily # of users and given # actions per user/day and have to go from there. Performance What is expected write-to-read data delay? If time does not matter, we can use batch data processing. Otherwise, its too slow. How fast must data be retrieved from the system. If it must be as fast as possible, its a hint we must count views as we write data. Minimal to no counting as we read data. In other words, data must already be aggregated. Cost Should help us choose technology stack. For ex, if asked to minimize dev cost, we should lean towards open source frameworks. If maintanence cost is primary concern, we should use public cloud services.","title":"Problem Statement / Gathering Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#functional-requirements","text":"After figuring out what the system should do, write down in a few sentences what the system has to do. These should include actions the system will make. We can expand from initial idea to make API more extensible.","title":"Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#non-functional-requirements","text":"Durability can be the 4th requirement after Scalable, Peformant, and Highly Available. Do we favor stale data over no data at all? Question of consistency.","title":"Non Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#high-level-architecture","text":"Start with something simple. Database to store data. Web service to processes incoming view events and stores data in the database. To retrieve view count from the database, lets introduce another web service. At this point, we don't have a clear vision of the design so we throw a few high level componenets on the board. High chances interviewer is an expert in the field and knows the Q very well. We may not feel comfortable discussing any componenet just yet. Unless you are an expert, you may not be ready to answer their questions. That's why we need to start with something simple such as Data . What data do we want to store and how?","title":"High Level Architecture"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#data-storage","text":"","title":"Data Storage"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#defining-a-data-model","text":"Which option should we store? Raw events or aggregate data in real time? This is where we need interviewer to help us make a decision. We should ask about expected data delay. Time between when event happened and when it was processed. If it should be no more than several minutes, we must aggregate data on the fly. If few hours is ok, we can store raw events and process them in the background. Former approach is known as stream data processing while latter is known as batch data processing . For sake of this guide, lets choose both options. Btw, combining both approaches makes alot of sense for many systems. We will store raw events and because there are so many of them we will store them for several days or weeks before purging data and sending to longer term storage systems if needed. We will calculate and store numbers in real time so that statistics are available for users right away. By storing both raw events and aggregated data, we get the bost of both worlds. Fast reads, ability to aggregate data differently, and recalculate statistics if there were bugs in real time path. There is a price to pay for this flexibility. Complexity and cost. Discuss with interviewer.","title":"Defining A Data Model"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#where-we-store-data","text":"Interviewer may ask specific database type / name. We should evaluate both SQL and NoSQL options. This is where Non-Functional requirements come in. Scalability, Availability, and Performance. We should evaluate databases against these requirements. Lets add more requirements along the way.","title":"Where we store data"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#how-we-store-data","text":"In relational database, data is normalized. We minimize data duplication. NoSQL databases promote different paradigm. We think in terms of queries. Instead of adding rows for every next hour, we add columns. Great, we have coveraged the storage portion of our design.","title":"How we store data"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#processing-service","text":"Let's define what processing really means. When YouTube user opens some video, we want total views count for this video to be displayed immediately. It means we need to calculate views count on the fly, in real time. When video owner opens statistics, we need to show per hour counts. So processing basically means we get a video view event and we update 2 counters, total and per hour. Where to start? As usual, Start with the Requirements . We want the processing service to scale together with increase in video views. Partitioning We don't want to lose data incase of failures. Replicate We want to process events quickly. In Memory. Minimize disk reads Before diving into Processing Service detailed design, lets agree on some basics.","title":"Processing Service"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#data-aggregation-basics","text":"Should we increment data in database for each event or accumulate data in processing service memory and add accumulate value to database counter? Aggregate data in memory is better. Less database writes. Push or pull? Should something send events synchronously to Processing Service or should Processing Service pull data from temporary storage? Although both work, Pull option has more advantages as it provides more fault tolerance support and its easier to scale. Messages can be kept in queue where Processing Service deletes item from storage once it updates in memory counters. If machine crashes, it will never ACK message so it stays in storage. We can use distributed queue such as Kafka or SQS. This part is optional as Kafka or SQS should provide this functionality for us. When events arrive, we put items in order to storage. Fixed order allows us to assign an offset for each item in storage. Events are always consumed sequentially. Every time event is read, offset moves forward. After we process several events and store them in database, we write checkpoint to persistent storage. If PS fails, we will replace it and it will resume from checkpoint offset. Most likely we dont need FIFO order. Instead of putting all events into single queue, we create multiple. Assign items to queues based on a hash. Partitioning allows us to parallelize event processing . We need to ensure certain hosts only read from certain queues. All \"A\" videos go to \"A\" queue which is processed from \"A\" PS.","title":"Data Aggregation Basics"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#processing-service-detailed-design","text":"We discussed so far that processing service reads events from partition one by one, counts events in memory, and flushes this counted values to the database periodically. So, we need a component to read events. Lets say consumer is a single thread. We could use multiple threads but we need to use concurrent hashmaps. The consumer establishes and maintains TCP connection with the partition to fetch data. When consumer reads event it deserializes it. Meaning it converts byte array into the actual object. Consumer does one more important thing - helps to eliminate duplicate events. Usually queues support atleast once delivery. To achieve this we use a distributed cache that stores unique event identifiers for let's say last 10 minutes. Event then comes to the component that does in-memory counting. Let's call it aggregator. Think of it as a hash table that accumulates data for some period of time. Periodically, we stop writing to the current hash table and create a new one. A new hash table keeps accumulating incoming data. While old hash table is no longer counting any data and each counter from the old hash table is sent to the internal queue for further processing. Why do we need this internal queue? Why can't we send data directly to the database. Glad you asked. Remember, we have a single thread that reads events from the partition. But nothing stops us from processing these events by multiple threads, to speed up processing. Especially if processing takes time. However, if we have many threads for readers, we don't need this. By sending data to the internal queue we decouple consumption and processing . You may argue whether we should put internal queue before Aggregator component. Both options are fine. Ok, we now ready to send pre-aggregated values to the database. So, we need a component responsible for this. Database writer is either a single-threaded or a multi-threaded component. Each thread takes a message from the internal queue and stores pre-aggregated views count in the database. Single-threaded version makes checkpointing easier. But multi-threaded version increases throughput. Meanwhile, I would like to point out two more important features of the database writer. The first concept is called a dead letter queue . The dead-letter queue is the queue to which messages are sent if they cannot be routed to their correct destination. Why do you think we may need one? To protect ourselves from database performance or availability issues. If database becomes slow or we cannot reach database due to network issues, we simply push messages to the dead letter queue. And there is a separate process that reads messages from this queue and sends them to the database. This concept is widely used when you need to preserve data in case of downstream services degradation. So, you may apply it in many system designs. Another viable option is to store undelivered messages on a local disk of the processing service machine. The second concept is data enrichment. Remember how we store data in Cassandra? We store it the way data is queried, right? If we want for example to show video title in the report, we need to store video title together with views count. The same is true for the channel name and many other attributes that we may want to display. But all these attributes do not come to the processing service with every video view event. Event contains minimum information, like video identifier and timestamp. It does not need to contain video title or channel name or video creation date. So, these information comes from somewhere else, right? Some database. But the trick here is that this database lives on the same machine as the processing service. All these additional attributes should be retrieved from this database really quickly. Thus, having it on the same machine eliminates a need for remote calls. Such databases are called embedded databases. One last concept I would like to mention is state management. We keep counters in memory for some period of time. Either in in-memory store or internal queue. And every time we keep anything in memory we need to understand what to do when machine fails and this in-memory state is lost . But that is easy, you may say. We have events stored in the partition, let's just re-create the state from the point where we failed. In other words we just re-process events one more time. This is a good idea. And it will work well if we store data in-memory for a relatively short period of time and state is small. Sometimes it may be hard to re-create the state from raw events from scratch. The solution in this case is to periodically save the entire in-memory data to a durable storage. And new machine just re-loads this state into its memory when started.","title":"Processing Service Detailed Design"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#data-ingestion-path","text":"We know already that we have a set of partitions and processing service reads events from them, count data in memory for some short period of time and stores total count in the database. Someone needs to distribute data across partitions, right? Let's have a component called Partitioner service . Let's also have a load balancer component in front of our partitioner service to evenly distribute events across partitioner service machines. When user opens a video, request goes through API Gateway , component that represents a single-entry point into a video content delivery system. API Gateway routes client requests to backend services. Our counting system may be one of such backend services.And one more important component to mention is the partitioner service client . We talked about database and processing service in details. Now let's cover remaining 3 components of the data ingestion path: partitioner service client, load balancer and partitioner service.","title":"Data Ingestion Path"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#partitioner-service-client","text":"","title":"Partitioner Service Client"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#load-balancer","text":"","title":"Load Balancer"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#partitioner-service-and-partitions","text":"Partitioner service is a web service that gets requests from clients, looks inside each request to retrieve individual video view events (because remember we batch events on the client side), and routs each such event (we can also use the word message) to some partition. But what partitions are? Partitions is also a web service, that gets messages and stores them on disk in the form of the append-only log file. So, we have a totally-ordered sequence of messages ordered by time. This is not a single very large log file, but a set of log files of the predefined size.","title":"Partitioner Service and Partitions"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#data-retrieval","text":"When users open a video on Youtube, we need to show total views count for this video. To build a video web page, several web services are called. A web service that retrieves information about the video, a web service that retrieves comments, another one for recommendations. Among them there is our Query web service that is responsible for video statistics. All these web services are typically hidden behind an API Gateway service, a single-entry point. API Gateway routes client requests to backend services. So, get total views count request comes to the Query service. We can retrieve the total count number directly from the database. Remember we discussed before how both SQL and NoSQL databases scale for reads. But total views count scenario is probably the simplest one. This is just a single value in the database per video. The more interesting use case is when users retrieve time-series data, which is a sequence of data points ordered in time. For example, when channel owner wants to see statistics for her videos. As discussed before, we aggregate data in the database per some time interval, let's say per hour. Every hour for every video. That is a lot of data, right? And it grows over time. Fortunately, this is not a new problem and solution is known. Monitoring systems, for example, aggregate data for every 1 minute interval or even 1 second. You can imaging how huge those data sets can be. So, we cannot afford storing time series data at this low granularity for a long period of time. The solution to this problem is to rollup the data . For example, we store per minute count for several days. After let's say one week, per minute data is aggregated into per hour data. And we store per hour count for several months. Then we rollup counts even further and data that is older than let's say 3 months, is stored with 1 day granularity. And the trick here is that we do not need to store old data in the database. We keep data for the last several days in the database, but the older data can be stored somewhere else, for example, object storage like AWS S3. In the industry, you may also hear terms like a hot storage and a cold storage. Hot storage Frequently used data that must be accessed fast. Cold storage Doesn\u2019t require fast access. It mostly represents archived and infrequently accessed data. When request comes to the Query service, it does so-called data federation, when it may need to call several storages to fulfill the request. Most recent statistics is retrieved from the database, while older statistics is retrieved from the Object Storage. Query service then stitches the data. And this is ideal use case for the cache. We should store query results in a distributed cache. This helps to further improve performance of queries and scale them.","title":"Data Retrieval"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#data-flow-simulation","text":"Three users opened some video A. And API Gateway got 3 requests. Partitioner service client batches all three events and sends them in a single request to the partitioner service. This request hits the load balancer first. And load balancer routes it to one of the partitioner service machines. Partitioner service gets all three events from the request and sends them to some partition All three events end up in the same partition, as we partition data based on the video identifier. Here is where processing service appears on the stage. Partition consumer reads all three messages from the partition one by one and sends them to the aggregator. Aggregator counts messages for a one minute period and flushes calculated values to the internal queue at the end of that minute. Database writer picks count from the internal queue and sends it to the database. In the database we store count per hour and the total number of views for each video. So, we just add a one minute value to the current hour count as well as the total count. Total count was 7 prior to this minute and we add 3 for the current minute. And during data retrieval, when user opens video A, API Gateway sends request to the Query service. Query service checks the cache. And if data is not found in the cache, or cache value has expired, we call the database. Total count value is then stored in the cache and Query service returns the total count back to the user. We may need cache invalidation but if we keep low TTL it may be ok to show slightly stale data.","title":"Data Flow Simulation"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#other-interview-aspects","text":"Another important aspect of an interview and system design in general is a technology stack. When we design some system, we usually do not need to reinvent the wheel. We rely on some well-regarded technologies. Either open source or commercial. Public cloud services. During the interview do not forget to discuss these technologies. You may do this along the way or at the end of the interview. So, let's see what technologies we may use for our solution.","title":"Other Interview Aspects"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#client-side","text":"Netty is a high-performance non-blocking IO framework for developing network applications, both clients and servers. Frameworks such as Hystrix from Netflix and Polly simplify implementation of many client-side concepts we discussed before: timeouts, retries, circuit breaker pattern.","title":"Client Side"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#messaging-systems","text":"Instead of using our custom Partitioner service and partitions, we could use Apache Kafka instead. Or Kafka's public cloud counterpart, like Amazon Kinesis . To process events and aggregate them in memory we can use stream-processing frameworks such as Apache Spark or Flink . Or cloud-based solutions, such as Kinesis Data Analytics .","title":"Messaging Systems"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#storage","text":"We already talked about Apache Cassandra. Another popular choice for storing time-series data is Apache HBase database. These are wide column databases. There are also databases optimized for handling time series data, like InfluxDB . We also mentioned before that we may need to store raw events to recalculate counts in case of any error or if customers need to run ad-hoc queries. We can store raw events in Apache Hadoop or in a cloud data warehouse, such as AWS Redshift . And when we roll up the data and need to archive it, AWS S3 is a natural choice.","title":"Storage"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#other","text":"Vitess is a database solution for scaling and managing large clusters of MySQL instances Vitess has been serving all Youtube database traffic since 2011. In several places of our design we rely on a distributed cache : for message deduplication and to scale read data queries. Redis is a good option. For a dead-letter queue mechanism, when we need to temporarily queue undelivered messages, we may use an open-source message-broker such as RabbitMQ . Or public cloud alternative, such as Amazon SQS . For data enrichment, when we store video and channel related information locally on the machine and inject this information in real-time, we may use RocksDB , a high performance embedded database for key-value data. To do leader election for partitions and to manage service discovery, we may rely on Apache Zookeeper , which is a distributed configuration service. For the service discovery piece we actually have an alternative, Eureka web service from Netflix. To monitor each of our system design components we may rely on monitoring solutions provided by public cloud services, such as AWS CloudWatch . Or use a popular stack of open source frameworks: Elasticsearch, Logstash, Kibana. Or ELK for short. We discussed before that binary message format is preferred for our system. Popular choices are Thrift, Protobuf and Avro . For Partitioner service to partition the data, we should use a good hashing function, for example a MurmurHash .","title":"Other"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#follow-up-questions","text":"We are done with the detailed system design. And here is where our interviewer will start challenging us with the choices we have made. We discussed many tradeoffs of individual components throughout this video. Let's see what else the interviewer may want to discuss with us.","title":"Follow Up Questions"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#bottlenecks","text":"To identify bottlenecks in the system we need to test it under a heavy load. This is what performance testing is about. There are several types of performance testing. We have: Load Testing when we measure behavior of a system under a specific expected load. Stress Testing when we test beyond normal operational capacity, often to a breaking point. Soak Testing when we test a system with a typical production load for an extended period of time. With load testing we want to understand that our system is indeed scalable and can handle a load we expect. For example, a two or three times increase in traffic. With stress testing we want to identify a breaking point in the system. Which component will start to suffer first. And what resource it will be: memory, CPU, network, disk IO . And with soak testing we want to find leaks in resources. For example, memory leaks.So, generating high load is the key. Tools like Apache JMeter can be used to generate a desired load. Health monitoring.","title":"Bottlenecks"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#health-monitoring","text":"All the components of our system must be instrumented with monitoring of their health. Metrics, dashboards and alerts should be our friends all the time. Metric is a variable that we measure, like error count or processing time. Dashboard provides a summary view of a service\u2019s core metrics. And alert is a notification sent to service owners in a reaction to some issue happening in the service. Remember about the four golden signals of monitoring, which are latency , traffic , errors , and saturation .","title":"Health Monitoring"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#ensure-accurate-results","text":"Ok, we designed a system and deployed all the components. We know it is running healthy and can handle a high load. But how to make sure it counts things correctly? This becomes critical when we not just count video views, but, for example, number of times some ad was played in a video. As we need to properly charge an ad owner and pay money to a video owner. This problem is typically addressed by building an audit system. There can be two flavors of audit systems. Let's call them weak and strong. Weak audit system is a continuosly running end-to-ed test. When let's say once a minute we generate several video view events in the system, call query service and validate that returned value equals to the expected count. This simple test gives us a high confidence that the system counts correctly. And it is easy to implement and maintain such test. But unfortunately, this test is not 100% reliable. What if our system loses events in some rare scenarios? And weak audit test may not identify this issue for a long period of time. That is why we may need a better approach. Strong audit system calculates video views using a completely different path then out main system. For example we store raw events in Hadoop and use MapReduce to count events. And then compare results of both systems. Having two different systems doing almost the same may seem like an overkill, right? You may be surprised but this is not so uncommon in practice. Not such a long time ago it was quite a popular idea. And it even has a name - Lambda Architecture . The key idea is to send events to a batch system and a stream processing system in parallel. And stitch together the results from both systems at query time. See Top K Heavy Hitters Ideally, we should have a single system. Let me share with you advice from Jay Kreps, who is one of the authors of Apache Kafka. Use a batch processing framework like MapReduce if we aren\u2019t latency sensitive Use a stream processing framework if we are, but not to try to do both at the same time unless we absolutely must . And please note that out today's problem can indeed be solved with MapReduce. But MapReduce-based system would have a much higher latency.","title":"Ensure Accurate Results"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#avoid-bottleneck-with-extremely-popular-videos","text":"We already discussed the problem with popular videos. I will just reiterate the key idea. We have to spread events coming for a popular video across several partitions. Otherwise, a single consumer of a single \"hot\" partition may not be able to keep up with the load. And will fall behind.","title":"Avoid Bottleneck with Extremely Popular Videos"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#situation-when-the-processing-service-cannot-keep-up-with-the-load","text":"Maybe because number of events is huge, maybe because processing of a single event is complicated and time consuming.I will not dive too much into details, but describe the main idea of the solution. We batch events and store them in the Object Storage service , for example AWS S3. Every time we persist a batch of events, we send a message to a message broker. For example SQS. Then we have a big cluster of machines, for example EC2, that retrieve messages from SQS, read a corresponding batch of events from S3 and process each event. This approach is a bit slower than stream processing, but faster than batch processing.","title":"Situation when the processing service cannot keep up with the load"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#summary","text":"We start with requirements clarification. And more specifically, we need to define Functional Requirements / APIs, what exactly our system is supposed to do. We then discuss non-functional requirements with the interviewer and figure out what qualities of the system she is most interested in. We can now outline a high-level architecture of the system. Draw some key components on the whiteboard. At the next stage we should dive deep into several of those components. Our interviewer will help us understand what components we should focus on. And the last important step is to discuss bottlenecks and how to deal with them.","title":"Summary"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#functional-requirements-api","text":"To define APIs, we discuss with the interviewer what specific behaviors or functions of the system we need to design. We write down verbs characterizing these functions and start thinking about input parameters and return values. We then can make several iterations to brush up the APIs. After this step we should be clear on what the scope of the design is.","title":"Functional Requirements / API"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#nonfunctional-requirements","text":"To define non-functional requirements, just know what your options are. Open a list of non-functional requirements on wiki and read the list. There are many of them. I recommend to focus on scalability , availability and performance . Among other popular choices we have consistency , durability , maintainability and cost. Try to pick not more than 3 qualities.","title":"NonFunctional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#hld","text":"To outline a high-level design, think about how data gets into the system, how it gets out of the system and where data is stored inside the system. Draw these components on the whiteboard. It is ok to be rather generic at this stage. Details will follow later. And although it is not easy, try to drive the conversation . Our goal here is to get understanding of what components to focus on next. And the interviewer will help us.","title":"HLD"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/stepByStepGuide/#detailed-design","text":"While designing specific components, start with data . How it is stored, transferred and processed. Here is where our knowledge and experience becomes critical. By using fundamental concepts of system design and by knowing how to combine these concepts together, we can make small incremental improvements. And apply relevant technologies along the way. After technical details are discussed, we can move to discussing other important aspects of the system. Listen carefully to the interviewer. She sees bottlenecks of our design and in her questions there will be hints what those bottlenecks are. And what can really help us here is the knowledge of different tradeoffs in system design. We just need to pick and apply a proper one. Although we talked about a specific problem today, like video views counting, the same ideas can be applied to other problems, for example counting likes, shares, reposts, ad impressions and clicks. The same ideas can be applied to designing monitoring systems, when we count metrics. When we design a fraud prevention system we need to count number of times each credit card was used recently. When we design recommendation service we may use counts as input to machine learning models. When we design \"what's trending\" service, we count all sorts of different reactions: views, re-tweets, comments, likes. And many other applications.","title":"Detailed Design"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/","text":"Top K Heavy Hitters Introduction In many applications of practical interest, we wish to identify only the \u201cmost important\u201d or largest values in a data stream. This idea is captured by the heavy hitters problem - we supposethat there are only a few large or \u201cheavy hitting\u201d elements in the stream, and we want to have a streaming algorithm that can identify them. It's a popular system design interview question as it may take many different forms. \"what's trending\" service popular products based on product page views most actively traded stocks design a system that protects from DDoS attack You probably have a feeling already that database or distributed cache, when applied directly, is not a good option for services of such scale. We are talking about hundreds of thousands requests per second, even millions at peak. Databases today can handle millions of requests per second. But even if we count all the views for Youtube videos in the database for some period of time, how do we calculate the top 100? We need to scan over entries in the database and order entries by view counts. For small scale such approach is fine, but for services of Youtube scale this is very ineffective: both expensive and slow. Ok, sounds like a typical big data processing problem and MapReduce might help. MapReduce will indeed help us to solve this problem, but MapReduce alone is not enough. We need our solution to return list of heavy hitters as close to real time as possible. For example, we need to return a list of most viewed videos for the last several minutes. Which moves this problem to the category of stream processing problems. Functional Requirements topK(k, startTime, endTime) : We want our system to return a list of k most frequent items. And because this list changes over time, we also need to provide a time interval, and more specifically, start and end time of that interval. Non-Functional Requirements Scalable (scales out together with increasing amount of data : videos, tweets, posts, etc) : We want to design a solution that can scale together with the data increase. Highly Available (tolerates network / hardware failures, no single point of failure) : Make data available in case of hardware failures or network partitions. Highly Performant (few tens of miliseconds to return top 100 list) : Fast, so that we can retrieve a list of heavy hitters in a matter of tens of milliseconds. Performance requirement should give you a hint that the final top k list should be pre-calculated and we should avoid heavy calculations while calling the topK API. Accuracy (as accurate as we can get) : Without this requirement, we may design a solution that is scalable, highly available and performant, but never produces accurate results. For example, by using data sampling we may not count every event, but only a small fraction of all events. And even though this is a good option to discuss with the interviewer, let's try to design a solution that can produce accurate results. Approaches Hash table, single host Let's assume the whole data set can be loaded into a memory of that single host. For example, we have a list of events (video views). Every time user opens a video, we log such event by adding video identifier to the list of events. A, B, C, D represent unique video identifiers. Given a list of events, how do we calculate the k most frequent elements? First, we calculate how many times each element appears in the list. So, we create a hash table that contains frequency counts. And to get top k elements we can either sort the hash table based on frequency counts. Or we can add all elements into a heap data structure. We make sure heap contains only k elements every time we add new element to the heap. Heap approach is faster. When we sort all elements in the hash table, the time complexity of such algorithm is NLog(N) , where n is the number of elements in the hash table. By using heap, we can reduce time complexity to be NLog(K) . The first and the most obvious problem with this solution - it is not scalable. If events are coming with a high rate, single host will quickly become a bottleneck. So, we may want to start processing events in parallel. How do we achieve this? Hash table, multiple hosts One of the options is to introduce a load balancer. This may be a classic load balancer or a distributed queue. Each event then goes to one of the hosts in a cluster. Let's call them Processor hosts. And because the same video identifier may appear on different Processor hosts, each Processor needs to flush accumulated data to a single Storage host. It is a bit better now, we can process events in parallel. Total throughput of the system has increased. But another evident problem of this solution is memory. We may use too much memory on each Processor host as well as Storage host. There are billions of videos on Youtube. Even if we store a fraction of this number in memory, hash table will become huge. What can we do? Hash table, multiple hosts, partitioning If you recall our previous video where we designed a distributed cache, you will notice that we solved a similar problem there. And the way we did it is by partitioning data into smaller chunks. Let's apply the same idea here. We introduce a new component, called Data Partitioner . And this component is responsible for routing each individual video identifier to its own Processor host. So, each Processor host only stores a subset of all the data. And we follow the same procedure as we did for a single host. We build a hash table, create a heap and add all elements from the hash table to the heap. Now, each Processor host contains its own list of k heavy hitters. And each such list is sorted. How do we create a final list that combines information from every Processor host? And I hope you recognize another classic coding interview question that asks to merge n sorted lists. It is important to note that Processor hosts only pass a list of size k to the Storage host. We cannot pass all the data, meaning that we cannot pass each Processor hash table to the Storage host, as one combined hash table may be too big to fit in memory. That was the whole point of data partitioning after all, to not accumulate all the data on a single host. Ok, we took one more step in improving our solution. By partitioning the data, we increased both scalability and throughput. This architecture is not bad, but it has some serious drawbacks and needs further improvement. All this time we were talking about bounded data sets or data sets of limited size. Such data sets can indeed be split into chunks, and after finding top k heavy hitters for each chunk we just merge results together. But streaming data is unbounded, essentially infinite. Users keep clicking on videos every second. In these circumstances, Processor hosts can accumulate data only for some period of time, let's say 1-minute, and will flush 1-minute data to the Storage host. So, the Storage host stores a list of heavy hitters for every minute. And remember that this is only top k heavy hitters. Information about all other elements, that did not make to the top k list is lost. We cannot afford storing information about every video in memory. But if we want to know heavy hitters for 1-hour or 1-day period, how can we build 1-hour list from 60 1-minute lists? If you think about this for a moment, you will see that there is no way to solve this problem precisely. We need the whole data set for that hour or that day to calculate a list of top k heavy hitters. So, on one hand we need the whole data set for a particular time period, let's say 1-day. And on the other hand, we cannot accumulate data in memory for the whole day. What should we do? Let's store all the data on disk and use batch processing framework to calculate a top k list. This is where MapReduce comes into play. Yes. Another problem with this architecture, is that although it may seem simple, it is not. Every time we introduce data partitioning, we need to deal with data replication, so that copies of each partition are stored on multiple nodes. We need to think about rebalancing, when a new node is added to the cluster or removed from it. We need to deal with hot partitions. Count-min sketch All these problems are not critical and solutions are known. But before diving into all these complexities let's ask ourselves: is there a simpler solution to the top k problem. And the answer is yes, there is a simpler solution. But we will need to make some sacrifices along the way. And the main sacrifice is accuracy. Let's take a look at some amazing data structure that will help identify a list of heavy hitters using fixed size memory. But results may not be 100% accurate. Bear with me, we are very close to outlining the final architecture. This data structure is called count-min sketch . Count-min sketch is really simple. You can think of it as a two-dimensional array. We define width of this array and its height. Width is usually in thousands, while height is small and represents a number of hash functions, for example 5. When new element comes, we calculate each hash function value and add 1 to the correspondent cell. For example, video A comes, we calculate five hash functions based on video A identifier and put 1 to each of 5 cells. When another A comes we increment each cell value. And repeat the same for one more A. Then B arrives and we add 1 to each of B cells. Hash function H1 produced a collision and we incremented A's value as well. Then C arrives. And produces collisions with both A and B. Here is how we add data to the count-min sketch. But how do we retrieve data? Logic is simple, among all the cells for A we take the minimum value. Because of collisions, some cells will contain overestimated values. And by taking minimum we decrease a chance of overestimation. And, hopefully, it makes total sense right now why we need several hash functions, and not a single function. If it was just a single hash function (for example H1), meaning that our array had only a single row, value of A would be 5, instead of 3. By using more functions, we decrease the error. - How we choose width and height parameters? Count-min sketch is a very well-studied data structure. There are formulas that help to calculate both parameters based on the accuracy we want to achieve and probability with which we reach the accuracy. How do we apply count-min sketch to our original problem? Roughly, think of a count-min sketch as a replacement of the hash table we had in all our previous discussions. We still need a heap to store a list of heavy hitters. But we replace a hash table, that could grow big, with a count-min sketch that always have a predefined size and never grow in size, even when data set size increases. High-level Architecture Every time user clicks on a video, request goes through API Gateway, component that represents a single-entry point into a video content delivery system. API Gateway routes client requests to backend services. Nowadays majority of public distributed systems use API Gateways, it's a widely spread practice. For our use case, we are interested in one specific function of API Gateways, log generation, when every call to API is logged. Usually these logs are used for monitoring, audit, billing. We will use these logs for counting how many times each video was viewed. We may implement a background process that reads data from logs, does some initial aggregation, and sends this data for further processing. We allocate a buffer in memory on the API Gateway host, read every log entry and build a frequency count hash table we discussed before. This buffer should have a limited size, and when buffer is full, data is flushed. If buffer is not full for a specified period of time, we can flush based on time. There are also other options, like aggregating data on the fly, without even writing to logs. Or completely skip all the aggregation on the API Gateway side and send information about every individual video view further down for processing. There are pros and cons for every option. Will return to this topic a bit later. Last but not least, we better serialize data into a compact binary format. This way we save on network IO utilization if request rate is very high. And let CPU pay the price. Once again, all these considerations depend on what resources are available on the API Gateway host: memory, CPU, network or disk IO. Great stuff to discuss with the interviewer. Initially aggregated data is then sent to a distributed messaging system, like Apache Kafka. Internally Kafka splits messages across several partitions, where each partition can be placed on a separate machine in a cluster. At this point we do not have any preferences how messages are partitioned. Default random partitioning will help to distribute messages uniformly across partitions. Now the more interesting part starts. We will split our data processing pipeline into two parts: fast path and slow path. On the fast path, we will calculate a list of k heavy hitters approximately. And results will be available within seconds. On the slow path, we will calculate a list of k heavy hitters precisely. And results will be available within minutes or hours, depending on the data volume. Let's first look at the fast path, it is much shorter. We have a service, let's call it fast processor, that creates count-min sketch for some short time interval and aggregates data. And because memory is no longer a problem, we do not need to partition the data. Any message from Kafka can be processed by any Fast Processor host. Every time we keep data in memory, even for a short period of time, we need to think about data replication. Otherwise, we cannot claim high availability for a service, as data may be lost due to hardware failures. Because count-min sketch already produces approximate results, meaning we lose data already in some way, it may be ok if we lose data in some rare cases when host dies. As long as slow path does not lose data, we will have accurate results several hours later. Absence of replication greatly simplifies the system. A good trade-off to discuss with your interviewer. Every several seconds Fast Processor flushes data to the Storage. Remember that count-min sketch has a predefined size, it does not grow over time. Nothing stops us from aggregating for longer than several seconds, we may wait for minutes. But this will delay final results. Another trade-off to discuss. The Storage component is a service in front of a database. And it stores the final top k list for some time interval, for example every 1 minute or 5 minutes. Have you noticed how our data processing pipeline gradually decreases request rate? Let me elaborate on this. There may be millions of users clicking on videos in the current minute. All those requests end up on API Gateway hosts. This cluster may be big, it may contain thousands of hosts. Then we pre-aggregate data on each host, may be just for several seconds. But it helps to decrease number of messages that go into Kafka. Next is Fast Aggregator cluster. And it will be much smaller in size than API Gateway cluster. It aggregates data further. For another several seconds. And when we arrive to the Storage, we only deal with a small fraction of requests that landed initially on API Gateway hosts. This is an important concept when you deal with stream processing and data aggregation. Please have it in mind for other system designs. Now, let's define components of the slow path. On the slow path we also need to aggregate data, but we want to count everything precisely. There are several options how to do this. One option is to let MapReduce do the trick. We dump all the data to the distributed file system, for example HDFS or object storage, for example S3. And run two MapReduce jobs, one job to calculate frequency counts and another job to calculate the actual top k list. Data is then stored in the Storage service. I feel like something is missing here. Count-min sketch is great, it helped us to create a very simple and cost-effective architecture. But results produced by our fast path are approximate. MapReduce jobs we introduced on the slow path will help to produce accurate results. But this data processing path is indeed slow. I wish we could have something faster than MapReduce jobs. It may be slower than count-min sketch solution, but still accurate. Let's return to the idea of data partitioning we discussed in the past. Let's Data Partitioner read batches of events and parse them into individual events. Partitioner will take each video identifier and send information to a correspondent partition in another Kafka cluster. And as we mentioned before, every time we partition data, we need to think about possibility of hot partitions. And our partitioner service should take care of it. Now, each partition in Kafka or shard in Kinesis, depending on what we use, stores a subset of data. Both Kafka and Kinesis will take care of data replication. And we need a component that will read data from each partition and aggregate it further. Let's call it a Partition Processor. It will aggregate data in memory over the course of several minutes, batch this information into files of the predefined size and send it to the distributed file system, where it will be further processed by MapReduce jobs. Partition Processor may also send aggregated information to the Storage service. Wait, wait, wait. Send data to the Storage service from three different places? I may not clearly explain it yet, but I feel that there is an overlap somewhere. Ideally, we should have a single data processing path. I just wanted to show you that depending on the requirements and more specifically how soon top k heavy hitters should be identified, we may choose a path that suits us the best. For example, if we can tolerate approximate results, forget about the slow path and its components. Fast path is simple and should be cheap to build. If accuracy is important and results should be calculated in a matter of minutes, we need to partition the data and aggregate in memory. And if time is not an issue but we still need accurate results and data set is big, Hadoop MapReduce should be our choice. Low Level Architecture Fast path Users click on videos and each view video request is sent to one of the API Gateway hosts. Let's say user 1 opened videos A, B and C. While User 2 opened videos A and C. Video A was opened multiple times by both users. Each request landed on one of the API Gateway hosts and information about this was captured in log files. Information about views is then aggregated on the host and sent to the Distributed Messaging System. And when I say aggregated I mean we just build a hash table on each host. For each video we count how many times it was viewed in the last several seconds. And remember that this hash table is relatively small. Whenever size of the hash table reaches some limit or specified time elapses, we send data out and free the memory for the next hash table. So, the first API Gateway host will send out information about videos A and B. The second host, information about videos A and C. And the third host, information about video A. The first Fast Processor host will pick up and process the first message. And this is how count-min sketch will look like. The second Fast Processor host will pick up messages 2 and 3 and add information to its own count-min sketch. And after aggregating information for several seconds, each Fast Processor host sends count-min sketch information to some Storage host, where we build a single count-min sketch. Merging several count-min sketches together is as simple as summing up values in corresponding cells. Each Fast Processor host can keep candidates for top K using heap and pass along aswell to storage. Slow path We have Data Partitioner , a component that reads each message and sends information about each video to its own partition. Here, each blue cylinder represents a partition of the Distributed Messaging System. For example, the first message is scattered around partitions 1 and 2. The second message goes to partitions 2 and 3. The third message goes to the second partition. We have Partition Processors aggregate data for each partition. Each processor accumulates data in memory for let's say 5 minutes and writes this data to files, which are stored in the Distributed File System. Frequency Count MapReduce job reads all such 5-minute files and creates a final list per some time interval, let's say 1 hour. Top K MapReduce job then uses this information to calculate a list of k heavy hitters for that hour. MapReduce jobs In MapReduce data processing is split into phases. The input to a MapReduce job is a set of files split into independent chunks which are processed by the map tasks in a parallel manner. First, the record reader parses the data into records and passes the data to the mapper, in the form of a key/value pair. In the mapper, each key/value pair from the record reader is transformed into zero or more new key/value pairs, called the intermediate pairs. In the next phase, the partitioner takes the intermediate key/value pairs from the mapper and assigns the partition. Partitioning specifies that all the values for each key are grouped together and make sure that all the values of a single key go to the same reducer. The partitioned data is written to the local disk for each mapper. During the shuffling phase, Hadoop MapReduce takes the output files produced by partitioners and downloads them to the reducer machine, via HTTP. And the purpose of the sort is to group keys together. This way reducer task can easily iterate over values. Reducer then takes each key and iterates over all of the values associated with that key. In our case we just sum up all the values for the key. And the output of the reduce task is written to the file system. The Top K MapReduce job takes this data, splits it into chunks and sends each chunk to a mapper that calculates local top k list. Then all the individual top k lists are sent to the single reducer that calculates the final top k list. As you may see, the Top K MapReduce is based on exactly the same idea we discussed before: partition the data, find top k list for each partition and merge individual lists into the one final list. Data Retrieval We have talked about data ingestion or how data gets into the system. Let's now talk about data retrieval. First of all API Gateway should expose topK operation. API Gateway will route data retrieval calls to the Storage service. And the Storage service just retrieves the data from its underlying database. This requires some more explanation. Let's say our fast path creates approximate top k list for every 1-minute interval. And our slow path creates precise top k list for every 1-hour interval, 24 lists total for a day. When user asks for the top k list for the last 5 minutes, we just merge together 5 1-minute lists. And get the final list, which is also approximate. When user asks for the top k list for some 1-hour interval, for example from 1 pm till 2 pm, it is easy, we have precise lists calculated for each hour of the day. But when user asks for the top k list for some 2-hour interval, for example from 1 pm till 3 pm, we cannot give back a precise list. The best we can do is to merge together 2 1-hour lists. And results may no longer be 100% correct. This is a bit tricky, I agree. Hopefully, it makes sense. What else API Gateway does a lot of work. Authentication, SSL termination, rate limiting, request routing, response caching are among the few. And API Gateway hosts indeed may not have enough capacity to run any data aggregation processes in the background. But no matter how busy these hosts are, there will be a process that offloads log files from these hosts. Log files are usually sent to some storage or to a separate cluster where they can be further processed. So, we can run our log parsing and aggregation process on that cluster. The rest of the data processing pipeline stays the same. There are several alternatives to count-min sketch algorithm. There are counter-based algorithms, like Lossy Counting, Space Saving and Sticky Sampling. There are also modifications of the count-min sketch algorithm. As you may see, the value of k plays its role in several places. On the fast path we merge top k lists together on data retrieval. On the slow path, in Top K MapReduce job, all mappers send local top k lists to a single reducer. And what it tells us is that k cannot be arbitrary large. Several thousands are probably ok, but tens of thousands may start causing performance degradation. For example, during data retrieval when we merge such long lists on the fly. We should also remember about network bandwidth and storage space if we need to support larger values of k. The architecture we built with you is not a novel idea. It is also known as Lambda Architecture. Please do not confuse with AWS Lambdas. Lambda Architecture is an approach to building stream processing applications on top of MapReduce and stream processing engines. We send events to a batch system and a stream processing system in parallel. And we stitch together the results from both systems at query time. In 2011, Nathan Marz, the creator of Apache Storm wrote a famous article called \"How to beat the CAP theorem\" where he introduced the idea of Lambda Architecture. There are many benefits of Lambda Architecture, but there are drawbacks as well. In 2014, Jay Kreps, one of the authors of Apache Kafka wrote an article where he questioned Lambda Architecture. And he mentioned complexity as one of the main drawbacks of this approach. And indeed, if you look at the fast path once again, you will see that it is very simple. But by introducing the slow path, we greatly increased overall complexity of the system. Yes, we did this to have accurate results and to aggregate data over long time intervals. But these benefits come with a price. And this price is complexity. As always, everything is a trade-off in the world of system design. And I bet you are wondering right now, why on earth I want to make your life more complicated. Couldn't we just use Kafka plus some stream processing framework, for example Apache Spark and solve this problem having a simpler architecture. Yes and no. Depending on the data volume we can solve the top k problem precisely and having a single data processing path. And Kafka + Spark can do the trick. But I wanted to show you stuff under the hood. At the end of the day Spark internally relies on the same ideas we discussed: data partitioning and in-memory aggregation. Spark internally partitions the data and calculates a top k list for each partition using a heap data structure. And all these lists are merged together in the reduce operation. Today we solved a tough problem. Problem that requires knowledge (remember count-min sketch data structure, MapReduce paradigm, principles of data aggregation). But we did even more than this, we designed a solution for a whole class of problems. So, in your interview, when you are asked explicitly about identifying heavy hitters, you now know where to start, right? But an interview question may be stated differently and solution we built today still applies. For example, you may be asked to design \"what's trending\" service. Google trends, Twitter trends, Youtube trends, etc. In reality these services are more complicated than just a list of heavy hitters, but a simpler version of such service may purely rely on our today's design. We may compute a list of popular products based on product page views or product purchases. If asked about identifying the most actively traded stocks, this should also ring a bell in your head. Basically, we have a stream of stock trades as an input. And we need to find the top k stocks with the most number of trades. You may be asked to design a system that protects from DDoS attack. And if you think about this problem you will see that this is also a problem of identifying heavy hitters, where heavy hitter is some IP address that floods the victim service with superfluous requests. We just need to find the top k IP addresses that generates the most number of requests. And block those IPs. So, knowing how to solve the top k problem will help you better deal with several other system design questions.","title":"Top K Heavy Hitters"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#top-k-heavy-hitters","text":"","title":"Top K Heavy Hitters"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#introduction","text":"In many applications of practical interest, we wish to identify only the \u201cmost important\u201d or largest values in a data stream. This idea is captured by the heavy hitters problem - we supposethat there are only a few large or \u201cheavy hitting\u201d elements in the stream, and we want to have a streaming algorithm that can identify them. It's a popular system design interview question as it may take many different forms. \"what's trending\" service popular products based on product page views most actively traded stocks design a system that protects from DDoS attack You probably have a feeling already that database or distributed cache, when applied directly, is not a good option for services of such scale. We are talking about hundreds of thousands requests per second, even millions at peak. Databases today can handle millions of requests per second. But even if we count all the views for Youtube videos in the database for some period of time, how do we calculate the top 100? We need to scan over entries in the database and order entries by view counts. For small scale such approach is fine, but for services of Youtube scale this is very ineffective: both expensive and slow. Ok, sounds like a typical big data processing problem and MapReduce might help. MapReduce will indeed help us to solve this problem, but MapReduce alone is not enough. We need our solution to return list of heavy hitters as close to real time as possible. For example, we need to return a list of most viewed videos for the last several minutes. Which moves this problem to the category of stream processing problems.","title":"Introduction"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#functional-requirements","text":"topK(k, startTime, endTime) : We want our system to return a list of k most frequent items. And because this list changes over time, we also need to provide a time interval, and more specifically, start and end time of that interval.","title":"Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#non-functional-requirements","text":"Scalable (scales out together with increasing amount of data : videos, tweets, posts, etc) : We want to design a solution that can scale together with the data increase. Highly Available (tolerates network / hardware failures, no single point of failure) : Make data available in case of hardware failures or network partitions. Highly Performant (few tens of miliseconds to return top 100 list) : Fast, so that we can retrieve a list of heavy hitters in a matter of tens of milliseconds. Performance requirement should give you a hint that the final top k list should be pre-calculated and we should avoid heavy calculations while calling the topK API. Accuracy (as accurate as we can get) : Without this requirement, we may design a solution that is scalable, highly available and performant, but never produces accurate results. For example, by using data sampling we may not count every event, but only a small fraction of all events. And even though this is a good option to discuss with the interviewer, let's try to design a solution that can produce accurate results.","title":"Non-Functional Requirements"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#approaches","text":"","title":"Approaches"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#hash-table-single-host","text":"Let's assume the whole data set can be loaded into a memory of that single host. For example, we have a list of events (video views). Every time user opens a video, we log such event by adding video identifier to the list of events. A, B, C, D represent unique video identifiers. Given a list of events, how do we calculate the k most frequent elements? First, we calculate how many times each element appears in the list. So, we create a hash table that contains frequency counts. And to get top k elements we can either sort the hash table based on frequency counts. Or we can add all elements into a heap data structure. We make sure heap contains only k elements every time we add new element to the heap. Heap approach is faster. When we sort all elements in the hash table, the time complexity of such algorithm is NLog(N) , where n is the number of elements in the hash table. By using heap, we can reduce time complexity to be NLog(K) . The first and the most obvious problem with this solution - it is not scalable. If events are coming with a high rate, single host will quickly become a bottleneck. So, we may want to start processing events in parallel. How do we achieve this?","title":"Hash table, single host"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#hash-table-multiple-hosts","text":"One of the options is to introduce a load balancer. This may be a classic load balancer or a distributed queue. Each event then goes to one of the hosts in a cluster. Let's call them Processor hosts. And because the same video identifier may appear on different Processor hosts, each Processor needs to flush accumulated data to a single Storage host. It is a bit better now, we can process events in parallel. Total throughput of the system has increased. But another evident problem of this solution is memory. We may use too much memory on each Processor host as well as Storage host. There are billions of videos on Youtube. Even if we store a fraction of this number in memory, hash table will become huge. What can we do?","title":"Hash table, multiple hosts"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#hash-table-multiple-hosts-partitioning","text":"If you recall our previous video where we designed a distributed cache, you will notice that we solved a similar problem there. And the way we did it is by partitioning data into smaller chunks. Let's apply the same idea here. We introduce a new component, called Data Partitioner . And this component is responsible for routing each individual video identifier to its own Processor host. So, each Processor host only stores a subset of all the data. And we follow the same procedure as we did for a single host. We build a hash table, create a heap and add all elements from the hash table to the heap. Now, each Processor host contains its own list of k heavy hitters. And each such list is sorted. How do we create a final list that combines information from every Processor host? And I hope you recognize another classic coding interview question that asks to merge n sorted lists. It is important to note that Processor hosts only pass a list of size k to the Storage host. We cannot pass all the data, meaning that we cannot pass each Processor hash table to the Storage host, as one combined hash table may be too big to fit in memory. That was the whole point of data partitioning after all, to not accumulate all the data on a single host. Ok, we took one more step in improving our solution. By partitioning the data, we increased both scalability and throughput. This architecture is not bad, but it has some serious drawbacks and needs further improvement. All this time we were talking about bounded data sets or data sets of limited size. Such data sets can indeed be split into chunks, and after finding top k heavy hitters for each chunk we just merge results together. But streaming data is unbounded, essentially infinite. Users keep clicking on videos every second. In these circumstances, Processor hosts can accumulate data only for some period of time, let's say 1-minute, and will flush 1-minute data to the Storage host. So, the Storage host stores a list of heavy hitters for every minute. And remember that this is only top k heavy hitters. Information about all other elements, that did not make to the top k list is lost. We cannot afford storing information about every video in memory. But if we want to know heavy hitters for 1-hour or 1-day period, how can we build 1-hour list from 60 1-minute lists? If you think about this for a moment, you will see that there is no way to solve this problem precisely. We need the whole data set for that hour or that day to calculate a list of top k heavy hitters. So, on one hand we need the whole data set for a particular time period, let's say 1-day. And on the other hand, we cannot accumulate data in memory for the whole day. What should we do? Let's store all the data on disk and use batch processing framework to calculate a top k list. This is where MapReduce comes into play. Yes. Another problem with this architecture, is that although it may seem simple, it is not. Every time we introduce data partitioning, we need to deal with data replication, so that copies of each partition are stored on multiple nodes. We need to think about rebalancing, when a new node is added to the cluster or removed from it. We need to deal with hot partitions.","title":"Hash table, multiple hosts, partitioning"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#count-min-sketch","text":"All these problems are not critical and solutions are known. But before diving into all these complexities let's ask ourselves: is there a simpler solution to the top k problem. And the answer is yes, there is a simpler solution. But we will need to make some sacrifices along the way. And the main sacrifice is accuracy. Let's take a look at some amazing data structure that will help identify a list of heavy hitters using fixed size memory. But results may not be 100% accurate. Bear with me, we are very close to outlining the final architecture. This data structure is called count-min sketch . Count-min sketch is really simple. You can think of it as a two-dimensional array. We define width of this array and its height. Width is usually in thousands, while height is small and represents a number of hash functions, for example 5. When new element comes, we calculate each hash function value and add 1 to the correspondent cell. For example, video A comes, we calculate five hash functions based on video A identifier and put 1 to each of 5 cells. When another A comes we increment each cell value. And repeat the same for one more A. Then B arrives and we add 1 to each of B cells. Hash function H1 produced a collision and we incremented A's value as well. Then C arrives. And produces collisions with both A and B. Here is how we add data to the count-min sketch. But how do we retrieve data? Logic is simple, among all the cells for A we take the minimum value. Because of collisions, some cells will contain overestimated values. And by taking minimum we decrease a chance of overestimation. And, hopefully, it makes total sense right now why we need several hash functions, and not a single function. If it was just a single hash function (for example H1), meaning that our array had only a single row, value of A would be 5, instead of 3. By using more functions, we decrease the error. - How we choose width and height parameters? Count-min sketch is a very well-studied data structure. There are formulas that help to calculate both parameters based on the accuracy we want to achieve and probability with which we reach the accuracy. How do we apply count-min sketch to our original problem? Roughly, think of a count-min sketch as a replacement of the hash table we had in all our previous discussions. We still need a heap to store a list of heavy hitters. But we replace a hash table, that could grow big, with a count-min sketch that always have a predefined size and never grow in size, even when data set size increases.","title":"Count-min sketch"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#high-level-architecture","text":"Every time user clicks on a video, request goes through API Gateway, component that represents a single-entry point into a video content delivery system. API Gateway routes client requests to backend services. Nowadays majority of public distributed systems use API Gateways, it's a widely spread practice. For our use case, we are interested in one specific function of API Gateways, log generation, when every call to API is logged. Usually these logs are used for monitoring, audit, billing. We will use these logs for counting how many times each video was viewed. We may implement a background process that reads data from logs, does some initial aggregation, and sends this data for further processing. We allocate a buffer in memory on the API Gateway host, read every log entry and build a frequency count hash table we discussed before. This buffer should have a limited size, and when buffer is full, data is flushed. If buffer is not full for a specified period of time, we can flush based on time. There are also other options, like aggregating data on the fly, without even writing to logs. Or completely skip all the aggregation on the API Gateway side and send information about every individual video view further down for processing. There are pros and cons for every option. Will return to this topic a bit later. Last but not least, we better serialize data into a compact binary format. This way we save on network IO utilization if request rate is very high. And let CPU pay the price. Once again, all these considerations depend on what resources are available on the API Gateway host: memory, CPU, network or disk IO. Great stuff to discuss with the interviewer. Initially aggregated data is then sent to a distributed messaging system, like Apache Kafka. Internally Kafka splits messages across several partitions, where each partition can be placed on a separate machine in a cluster. At this point we do not have any preferences how messages are partitioned. Default random partitioning will help to distribute messages uniformly across partitions. Now the more interesting part starts. We will split our data processing pipeline into two parts: fast path and slow path. On the fast path, we will calculate a list of k heavy hitters approximately. And results will be available within seconds. On the slow path, we will calculate a list of k heavy hitters precisely. And results will be available within minutes or hours, depending on the data volume. Let's first look at the fast path, it is much shorter. We have a service, let's call it fast processor, that creates count-min sketch for some short time interval and aggregates data. And because memory is no longer a problem, we do not need to partition the data. Any message from Kafka can be processed by any Fast Processor host. Every time we keep data in memory, even for a short period of time, we need to think about data replication. Otherwise, we cannot claim high availability for a service, as data may be lost due to hardware failures. Because count-min sketch already produces approximate results, meaning we lose data already in some way, it may be ok if we lose data in some rare cases when host dies. As long as slow path does not lose data, we will have accurate results several hours later. Absence of replication greatly simplifies the system. A good trade-off to discuss with your interviewer. Every several seconds Fast Processor flushes data to the Storage. Remember that count-min sketch has a predefined size, it does not grow over time. Nothing stops us from aggregating for longer than several seconds, we may wait for minutes. But this will delay final results. Another trade-off to discuss. The Storage component is a service in front of a database. And it stores the final top k list for some time interval, for example every 1 minute or 5 minutes. Have you noticed how our data processing pipeline gradually decreases request rate? Let me elaborate on this. There may be millions of users clicking on videos in the current minute. All those requests end up on API Gateway hosts. This cluster may be big, it may contain thousands of hosts. Then we pre-aggregate data on each host, may be just for several seconds. But it helps to decrease number of messages that go into Kafka. Next is Fast Aggregator cluster. And it will be much smaller in size than API Gateway cluster. It aggregates data further. For another several seconds. And when we arrive to the Storage, we only deal with a small fraction of requests that landed initially on API Gateway hosts. This is an important concept when you deal with stream processing and data aggregation. Please have it in mind for other system designs. Now, let's define components of the slow path. On the slow path we also need to aggregate data, but we want to count everything precisely. There are several options how to do this. One option is to let MapReduce do the trick. We dump all the data to the distributed file system, for example HDFS or object storage, for example S3. And run two MapReduce jobs, one job to calculate frequency counts and another job to calculate the actual top k list. Data is then stored in the Storage service. I feel like something is missing here. Count-min sketch is great, it helped us to create a very simple and cost-effective architecture. But results produced by our fast path are approximate. MapReduce jobs we introduced on the slow path will help to produce accurate results. But this data processing path is indeed slow. I wish we could have something faster than MapReduce jobs. It may be slower than count-min sketch solution, but still accurate. Let's return to the idea of data partitioning we discussed in the past. Let's Data Partitioner read batches of events and parse them into individual events. Partitioner will take each video identifier and send information to a correspondent partition in another Kafka cluster. And as we mentioned before, every time we partition data, we need to think about possibility of hot partitions. And our partitioner service should take care of it. Now, each partition in Kafka or shard in Kinesis, depending on what we use, stores a subset of data. Both Kafka and Kinesis will take care of data replication. And we need a component that will read data from each partition and aggregate it further. Let's call it a Partition Processor. It will aggregate data in memory over the course of several minutes, batch this information into files of the predefined size and send it to the distributed file system, where it will be further processed by MapReduce jobs. Partition Processor may also send aggregated information to the Storage service. Wait, wait, wait. Send data to the Storage service from three different places? I may not clearly explain it yet, but I feel that there is an overlap somewhere. Ideally, we should have a single data processing path. I just wanted to show you that depending on the requirements and more specifically how soon top k heavy hitters should be identified, we may choose a path that suits us the best. For example, if we can tolerate approximate results, forget about the slow path and its components. Fast path is simple and should be cheap to build. If accuracy is important and results should be calculated in a matter of minutes, we need to partition the data and aggregate in memory. And if time is not an issue but we still need accurate results and data set is big, Hadoop MapReduce should be our choice.","title":"High-level Architecture"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#low-level-architecture","text":"","title":"Low Level Architecture"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#fast-path","text":"Users click on videos and each view video request is sent to one of the API Gateway hosts. Let's say user 1 opened videos A, B and C. While User 2 opened videos A and C. Video A was opened multiple times by both users. Each request landed on one of the API Gateway hosts and information about this was captured in log files. Information about views is then aggregated on the host and sent to the Distributed Messaging System. And when I say aggregated I mean we just build a hash table on each host. For each video we count how many times it was viewed in the last several seconds. And remember that this hash table is relatively small. Whenever size of the hash table reaches some limit or specified time elapses, we send data out and free the memory for the next hash table. So, the first API Gateway host will send out information about videos A and B. The second host, information about videos A and C. And the third host, information about video A. The first Fast Processor host will pick up and process the first message. And this is how count-min sketch will look like. The second Fast Processor host will pick up messages 2 and 3 and add information to its own count-min sketch. And after aggregating information for several seconds, each Fast Processor host sends count-min sketch information to some Storage host, where we build a single count-min sketch. Merging several count-min sketches together is as simple as summing up values in corresponding cells. Each Fast Processor host can keep candidates for top K using heap and pass along aswell to storage.","title":"Fast path"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#slow-path","text":"We have Data Partitioner , a component that reads each message and sends information about each video to its own partition. Here, each blue cylinder represents a partition of the Distributed Messaging System. For example, the first message is scattered around partitions 1 and 2. The second message goes to partitions 2 and 3. The third message goes to the second partition. We have Partition Processors aggregate data for each partition. Each processor accumulates data in memory for let's say 5 minutes and writes this data to files, which are stored in the Distributed File System. Frequency Count MapReduce job reads all such 5-minute files and creates a final list per some time interval, let's say 1 hour. Top K MapReduce job then uses this information to calculate a list of k heavy hitters for that hour.","title":"Slow path"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#data-retrieval","text":"We have talked about data ingestion or how data gets into the system. Let's now talk about data retrieval. First of all API Gateway should expose topK operation. API Gateway will route data retrieval calls to the Storage service. And the Storage service just retrieves the data from its underlying database. This requires some more explanation. Let's say our fast path creates approximate top k list for every 1-minute interval. And our slow path creates precise top k list for every 1-hour interval, 24 lists total for a day. When user asks for the top k list for the last 5 minutes, we just merge together 5 1-minute lists. And get the final list, which is also approximate. When user asks for the top k list for some 1-hour interval, for example from 1 pm till 2 pm, it is easy, we have precise lists calculated for each hour of the day. But when user asks for the top k list for some 2-hour interval, for example from 1 pm till 3 pm, we cannot give back a precise list. The best we can do is to merge together 2 1-hour lists. And results may no longer be 100% correct. This is a bit tricky, I agree. Hopefully, it makes sense.","title":"Data Retrieval"},{"location":"System%20Design/System%20Design%20Interview%20YT%20Channel/topKHeavyHitters/#what-else","text":"API Gateway does a lot of work. Authentication, SSL termination, rate limiting, request routing, response caching are among the few. And API Gateway hosts indeed may not have enough capacity to run any data aggregation processes in the background. But no matter how busy these hosts are, there will be a process that offloads log files from these hosts. Log files are usually sent to some storage or to a separate cluster where they can be further processed. So, we can run our log parsing and aggregation process on that cluster. The rest of the data processing pipeline stays the same. There are several alternatives to count-min sketch algorithm. There are counter-based algorithms, like Lossy Counting, Space Saving and Sticky Sampling. There are also modifications of the count-min sketch algorithm. As you may see, the value of k plays its role in several places. On the fast path we merge top k lists together on data retrieval. On the slow path, in Top K MapReduce job, all mappers send local top k lists to a single reducer. And what it tells us is that k cannot be arbitrary large. Several thousands are probably ok, but tens of thousands may start causing performance degradation. For example, during data retrieval when we merge such long lists on the fly. We should also remember about network bandwidth and storage space if we need to support larger values of k. The architecture we built with you is not a novel idea. It is also known as Lambda Architecture. Please do not confuse with AWS Lambdas. Lambda Architecture is an approach to building stream processing applications on top of MapReduce and stream processing engines. We send events to a batch system and a stream processing system in parallel. And we stitch together the results from both systems at query time. In 2011, Nathan Marz, the creator of Apache Storm wrote a famous article called \"How to beat the CAP theorem\" where he introduced the idea of Lambda Architecture. There are many benefits of Lambda Architecture, but there are drawbacks as well. In 2014, Jay Kreps, one of the authors of Apache Kafka wrote an article where he questioned Lambda Architecture. And he mentioned complexity as one of the main drawbacks of this approach. And indeed, if you look at the fast path once again, you will see that it is very simple. But by introducing the slow path, we greatly increased overall complexity of the system. Yes, we did this to have accurate results and to aggregate data over long time intervals. But these benefits come with a price. And this price is complexity. As always, everything is a trade-off in the world of system design. And I bet you are wondering right now, why on earth I want to make your life more complicated. Couldn't we just use Kafka plus some stream processing framework, for example Apache Spark and solve this problem having a simpler architecture. Yes and no. Depending on the data volume we can solve the top k problem precisely and having a single data processing path. And Kafka + Spark can do the trick. But I wanted to show you stuff under the hood. At the end of the day Spark internally relies on the same ideas we discussed: data partitioning and in-memory aggregation. Spark internally partitions the data and calculates a top k list for each partition using a heap data structure. And all these lists are merged together in the reduce operation. Today we solved a tough problem. Problem that requires knowledge (remember count-min sketch data structure, MapReduce paradigm, principles of data aggregation). But we did even more than this, we designed a solution for a whole class of problems. So, in your interview, when you are asked explicitly about identifying heavy hitters, you now know where to start, right? But an interview question may be stated differently and solution we built today still applies. For example, you may be asked to design \"what's trending\" service. Google trends, Twitter trends, Youtube trends, etc. In reality these services are more complicated than just a list of heavy hitters, but a simpler version of such service may purely rely on our today's design. We may compute a list of popular products based on product page views or product purchases. If asked about identifying the most actively traded stocks, this should also ring a bell in your head. Basically, we have a stream of stock trades as an input. And we need to find the top k stocks with the most number of trades. You may be asked to design a system that protects from DDoS attack. And if you think about this problem you will see that this is also a problem of identifying heavy hitters, where heavy hitter is some IP address that floods the victim service with superfluous requests. We just need to find the top k IP addresses that generates the most number of requests. And block those IPs. So, knowing how to solve the top k problem will help you better deal with several other system design questions.","title":"What else"}]}