{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"About the Site This site contains my notes when reading papers. As I only write down important or interesting parts for me, they may not be good summaries. Fields of Interest I'm mainly interested in large-scale data systems including Ethereum and block-chain based systems. \"A future is not given to you. It is something you must take for yourself.\"","title":"Home"},{"location":"index.html#about-the-site","text":"This site contains my notes when reading papers. As I only write down important or interesting parts for me, they may not be good summaries.","title":"About the Site"},{"location":"index.html#fields-of-interest","text":"I'm mainly interested in large-scale data systems including Ethereum and block-chain based systems. \"A future is not given to you. It is something you must take for yourself.\"","title":"Fields of Interest"},{"location":"aurora/aurora.html","text":"*# Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases A cloud-native relational database service for OLTP workloads. MIT Notes FAQ 1. Overview We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora uses a novel architecture with a fleet of database instances and storage service. Several database functions(redo logging, crash recovery, etc) are offloaded to the storage service, which is like a virtualized segmented redo log (shared-disk architecture). Aurora uses two big ideas : Quorum writes for better fault-tolerance without too much waiting Storage servers understand how to apply DB's log to data pages, so only need to send (small) log entries, not (big) dirty pages. Sending to many replicas, but not much data. The log is the database; any page that the storage system materializes are simply a cache of log application. Three advantages over traditional approaches to traditional distributed databases First, by building storage as an independent faulttolerant and self-healing service across multiple data-centers, we protect the database from performance variance and transient or permanent failures at either the networking or storage tiers. Second, by only writing redo log records to storage, we are able to reduce network IOPS by an order of magnitude Third, we move some of the most complex and critical functions (backup and redo recovery) from one-time expensive operations in the database engine to continuous asynchronous operations amortized across a large distributed fleet. This yields near-instant crash recovery without checkpointing as well as inexpensive backups that do not interfere with foreground processing. 2. HLD of Architecture To tolerate AZ failure, Aurora replicates each data item 6 ways across 3AZs with 2 copies in each AZ. Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. The only writes that cross the network are redo log records, so network load is drastically reduced despite amplifying write for replication. Storage nodes gossips with peers to fill gaps in received log records. Durable log application happens at the storage nodes continuously and asynchronously. Each log record has a monotonically-increasing Log Sequence Number(LSN). Instead of 2PC protocol, Aurora maintains points of consistency and durability and advances them when receiving acknowledgements for storage requests. Durability: the highest LSN at which all prior log records are available. Consistency: each transaction is broken up to mini-transactions, and the final log record in a mini-transaction is a consistency point. Normally read quorum is not needed since the database feeds log records to storage nodes and tracks progress. : 3. Durability V nodes, read quorum V_r , write quorum V_w To ensure each write is aware of the most recent write: V_w > V/2 Read = max_version(all nodes), so V_r + V_w > V , it can ensure the request must be accepted by a node with most update data. Because each log entry have an index/version, we only pick the date with the most updated log. AZ (availability zone) level failure tolerance Losing an entire AZ and one additional node (AZ+1) without losing data Losing an entire AZ without impacting the ability to write data AZ = 3, V = 6, V_w = 4, V_r = 3 Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. Advantages of storage server based on quorum mechanism : Smoother handling of server failures, slow execution or network partition problems, because each operation does not require a response from all replica servers (for example, as opposed to chain replication, chain replication needs to wait for write operations to be completed on all replicas) Under the premise of satisfying W+R>V , W and R can be adjusted. For different read and write load conditions, if the read load is relatively large, R can be reduced, and vice versa Raft also uses the quorum mechanism: leader will submit the log entry only after most copies are written to the log entry , but Raft is more powerful: it can handle more complex operations (due to its sequential operations); leader can be automatically re-elected when a split-brain problem occurs 4. The Log Is The Database 4.1. Database execution process Paper assumes you know how DB works, how it uses storage. Let's describe the execution process of the write operation of a single-machine general transaction database. The data is stored in the B-Tree of the hard disk, and there are cached data pages in the database. Take the transaction x=x+10 y=y-10 as an example: First lock x and y DB server modifies only cached data pages as transaction runs and appends update info to Write-Ahead Log (redo log) At this time log entry can be expressed as: LSID TID Key old new Notes 101 7 x 500 510 x=x+10 102 7 y 750 740 y=y-10 103 7 commit transaction finished Release the locks of x and y after WAL is written to the hard disk, and reply to client Log Applicator acts on the modification of the log entry on the before image of the cached data page, which will generate an after image. Delayed writing can optimize performance because the data page is large Crash Recovery Replay all committed transactions in log ( redo ) Roll back all uncommitted transactions in log ( undo ) 4.2 Aurora Log Processing In Aurora , Log Procesing is pushed down to the storage layer to generate data pages in the background or on-demand. The write data transmitted through the network is only REDO logs, thus reducing the network load , And provides considerable performance and durability. Extra Notes In shared-disk architecture, all data is shared by all nodes. In shared-nothing architecture, each node manages a subset of data. It is hard to use change buffer in shared-disk architecture, so writes are penalized when there are secondary indexes.*","title":"aurora"},{"location":"aurora/aurora.html#1-overview","text":"We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora uses a novel architecture with a fleet of database instances and storage service. Several database functions(redo logging, crash recovery, etc) are offloaded to the storage service, which is like a virtualized segmented redo log (shared-disk architecture). Aurora uses two big ideas : Quorum writes for better fault-tolerance without too much waiting Storage servers understand how to apply DB's log to data pages, so only need to send (small) log entries, not (big) dirty pages. Sending to many replicas, but not much data. The log is the database; any page that the storage system materializes are simply a cache of log application. Three advantages over traditional approaches to traditional distributed databases First, by building storage as an independent faulttolerant and self-healing service across multiple data-centers, we protect the database from performance variance and transient or permanent failures at either the networking or storage tiers. Second, by only writing redo log records to storage, we are able to reduce network IOPS by an order of magnitude Third, we move some of the most complex and critical functions (backup and redo recovery) from one-time expensive operations in the database engine to continuous asynchronous operations amortized across a large distributed fleet. This yields near-instant crash recovery without checkpointing as well as inexpensive backups that do not interfere with foreground processing.","title":"1. Overview"},{"location":"aurora/aurora.html#2-hld-of-architecture","text":"To tolerate AZ failure, Aurora replicates each data item 6 ways across 3AZs with 2 copies in each AZ. Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. The only writes that cross the network are redo log records, so network load is drastically reduced despite amplifying write for replication. Storage nodes gossips with peers to fill gaps in received log records. Durable log application happens at the storage nodes continuously and asynchronously. Each log record has a monotonically-increasing Log Sequence Number(LSN). Instead of 2PC protocol, Aurora maintains points of consistency and durability and advances them when receiving acknowledgements for storage requests. Durability: the highest LSN at which all prior log records are available. Consistency: each transaction is broken up to mini-transactions, and the final log record in a mini-transaction is a consistency point. Normally read quorum is not needed since the database feeds log records to storage nodes and tracks progress. :","title":"2. HLD of Architecture"},{"location":"aurora/aurora.html#3-durability","text":"V nodes, read quorum V_r , write quorum V_w To ensure each write is aware of the most recent write: V_w > V/2 Read = max_version(all nodes), so V_r + V_w > V , it can ensure the request must be accepted by a node with most update data. Because each log entry have an index/version, we only pick the date with the most updated log. AZ (availability zone) level failure tolerance Losing an entire AZ and one additional node (AZ+1) without losing data Losing an entire AZ without impacting the ability to write data AZ = 3, V = 6, V_w = 4, V_r = 3 Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. Advantages of storage server based on quorum mechanism : Smoother handling of server failures, slow execution or network partition problems, because each operation does not require a response from all replica servers (for example, as opposed to chain replication, chain replication needs to wait for write operations to be completed on all replicas) Under the premise of satisfying W+R>V , W and R can be adjusted. For different read and write load conditions, if the read load is relatively large, R can be reduced, and vice versa Raft also uses the quorum mechanism: leader will submit the log entry only after most copies are written to the log entry , but Raft is more powerful: it can handle more complex operations (due to its sequential operations); leader can be automatically re-elected when a split-brain problem occurs","title":"3. Durability"},{"location":"aurora/aurora.html#4-the-log-is-the-database","text":"","title":"4. The Log Is The Database"},{"location":"aurora/aurora.html#41-database-execution-process","text":"Paper assumes you know how DB works, how it uses storage. Let's describe the execution process of the write operation of a single-machine general transaction database. The data is stored in the B-Tree of the hard disk, and there are cached data pages in the database. Take the transaction x=x+10 y=y-10 as an example: First lock x and y DB server modifies only cached data pages as transaction runs and appends update info to Write-Ahead Log (redo log) At this time log entry can be expressed as: LSID TID Key old new Notes 101 7 x 500 510 x=x+10 102 7 y 750 740 y=y-10 103 7 commit transaction finished Release the locks of x and y after WAL is written to the hard disk, and reply to client Log Applicator acts on the modification of the log entry on the before image of the cached data page, which will generate an after image. Delayed writing can optimize performance because the data page is large Crash Recovery Replay all committed transactions in log ( redo ) Roll back all uncommitted transactions in log ( undo )","title":"4.1. Database execution process"},{"location":"aurora/aurora.html#42-aurora-log-processing","text":"In Aurora , Log Procesing is pushed down to the storage layer to generate data pages in the background or on-demand. The write data transmitted through the network is only REDO logs, thus reducing the network load , And provides considerable performance and durability.","title":"4.2 Aurora Log Processing"},{"location":"aurora/aurora.html#extra-notes","text":"In shared-disk architecture, all data is shared by all nodes. In shared-nothing architecture, each node manages a subset of data. It is hard to use change buffer in shared-disk architecture, so writes are penalized when there are secondary indexes.*","title":"Extra Notes"},{"location":"craq/craq.html","text":"Object Storage on CRAQ High-throughput chain replication for read-mostly workloads. MIT Notes , FAQ 1. Introduction Chain Replication with Apportioned Queries (CRAQ) is an improvement to chain replication. It distributes the load on all object copies to greatly improve read throughput while maintaining strong consistency. This article mainly summarizes the chain replication, the principle of CRAQ , and the consistency model of CRAQ . 1.1 Chain Replication Chain Replication (CR) is a method of replicating data across multiple nodes: The nodes form a chain of length C The head node of the chain handles all write operations from the client When a node receives a write operation, it will propagate to every node in the chain Once the write reaches the tail node, it is applied to all copies in the chain and is considered committed When the tail node submits a write operation, it will notify up the chain and head will respond to the client The tail node handles all read operations, so only the submitted value can be returned by the read operation Chain replication achieves strong consistency : Since all read operations are performed at the tail, and all write operations are committed at the tail, the chain tail can simply apply a total sequence to all operations. Tradeoffs vs Raft Both CRAQ and Raft/Paxos are replicated state machines. They can be used to replicate any service that can be fit into a state machine mold (basically, processes a stream of requests one at a time). One application for Raft/Paxos is object storage. CR and CRAQ are likely to be faster than protocols like Raft that provide strong consistency because the CR head does less work than the Raft leader: the CR head sends writes to just one replica, while the Raft leader must send all operations to all followers. CR has a performance advantage for reads as well, since it serves them from the tail (not the head), while the Raft leader must serve all client requests. However, Raft/Paxos and CR/CRAQ differ significantly in their failure properties. Raft (and Paxos and ZooKeeper) can continue operating (with no pauses at all) even if a minority of nodes are crashed, slow, unreliable, or partitioned. A CRAQ or CR chain must stop if something like that goes wrong, and wait for a configuration manager to decide how to proceed. On the other hand the post-failure situation is significantly simpler in CR/CRAQ; recall Figures 7 and 8 in the Raft paper. Failure recovery for chain replication: When the head node fails: the subsequent node replaces it as the head node, and there is no missing committed write operation When the tail node fails: the previous node replaces it as the tail node, and there is no lost write operation When the intermediate node fails: removed from the chain, the previous node needs to resend the most recent write operation Limitations : All reads of an object must go to the tail node, resulting in heavy load. 2. CRAQ 2.1 CRAQ Principles CRAQ is an improvement of chain replication which allows any node in the chain to perform read operations: CRAQ Each node can store multiple versions of an object, and each version contains a monotonically increasing version number and an additional attribute (identifying clean or dirty ) When a node receives a new version of an object (via a write operation that propagates down), the node appends this latest version to the list of the object If the node is not the tail node, mark the version as dirty and pass the write operation to subsequent nodes If the node is the tail node, the version is marked as clean , at this time the write operation is committed . Then, the tail node sends ACK back in the chain to notify other nodes to submit When the ACK of the object version arrives at the node, the node will mark the object version as clean . The node can then delete all previous versions of the object When the node receives a read request from the object: -If the latest known version of the requested object is clean, the node will return this value -Otherwise, the node will contact the tail node and ask for the last submitted version number of the object on the tail node, and then the node will return this version of the object 2.2 Performance Improvement CRAQ 's throughput improvement over CR occurs in two different situations: Read-intensive workload : Read operations can be performed on all nodes, so throughput is linearly proportional to chain length Write-intensive workload : In workloads with a large number of write operations, it is easier to read dirty data, so there are more query requests for tail nodes. However, the workload of querying the tail node is much lower than that of all read requests performed by the tail node, so the throughput of CRAQ is higher than that of CR . 2.3 Consistency Model For read operations, CRAQ supports three consistency models: Strong Consistency : The read operation described in 4.1 enables the latest written data to be read for each read, thus providing strong consistency Eventual consistency : Allow nodes to return uncommitted new data, that is, allow client to read inconsistent object versions from different nodes. But for a client , since it establishes a session with the node, its read operation is guaranteed to be monotonous and consistent. Eventual consistency with maximum inconsistency boundary : Nodes are allowed to return uncommitted new data, but there is a limit of inconsistency. This limit can be based on version or time. For example, it is allowed to return newly written but uncommitted data within a period of time. 2.4 ZooKeeper Coordination Service If the network connection between two adjacent nodes is disconnected, the subsequent node will want to become the head node, which will result in two head nodes. CRAQ itself will not solve such a problem, so an external distributed coordination service is needed to solve this problem, such as using ZooKeeper . ZooKeeper determines the composition of the chain, determines which node is the head and tail, and monitors which node has failed. When a network failure occurs, ZooKeeper determines the new composition of the chain, not based on each node's own perception of the network situation. Through the use of Zookeper watch flags , CRAQ nodes are guaranteed to receive a notification when nodes are added to or removed from a group. Similarly, a node can be notified when metadata in which it has expressed interest changes. During initialization, a CRAQ node creates an ephemeral file in /nodes/dc_name/node_id . CRAQ nodes can query /nodes/dc_name to determine the membership list for its datacenter, but instead of having to periodically check the list for changes, ZooKeeper provides processes with the ability to create a watch on a file. A CRAQ node, after creating an ephemeral file to notify other nodes it has joined the system, creates a watch on the children list of /nodes/dc_name , thereby guaranteeing that it receives a notification when a node is added or removed. 3.1 Alternate Approaches To Improving Chain Replication A data center will probably have lots of distinct CR chains, each serving a fraction (shard) of the objects. Suppose you have three servers (S1, S2, and S3) and three chains (C1, C2, C3). Then you can have the three chains be: C1: S1 S2 S3 C2: S2 S3 S1 C3: S3 S1 S2 Now, assuming activity on the three chains is roughly equal, the load on the three servers will also be roughly equal. In particular the load of serving client requests (head and tail) will be roughly equally divided among the three servers. This is a pretty reasonable arrangement; CRAQ is only better if it turns out that some chains see more load than others . 4.1 Summary In Chain Replication, only the head and tail directly serve client requests; the other replicas help fault tolerance but not performance. Since the load on the head and tail is thus likely to be higher than the load on intermediate nodes, you could get into a situation where performance is bottlenecked by head/tail, yet there is plenty of idle CPU available in the intermediate nodes. CRAQ exploits that idle CPU by moving the read work to them.","title":"craq"},{"location":"craq/craq.html#object-storage-on-craq","text":"High-throughput chain replication for read-mostly workloads. MIT Notes , FAQ","title":"Object Storage on CRAQ"},{"location":"craq/craq.html#1-introduction","text":"Chain Replication with Apportioned Queries (CRAQ) is an improvement to chain replication. It distributes the load on all object copies to greatly improve read throughput while maintaining strong consistency. This article mainly summarizes the chain replication, the principle of CRAQ , and the consistency model of CRAQ .","title":"1. Introduction"},{"location":"craq/craq.html#11-chain-replication","text":"Chain Replication (CR) is a method of replicating data across multiple nodes: The nodes form a chain of length C The head node of the chain handles all write operations from the client When a node receives a write operation, it will propagate to every node in the chain Once the write reaches the tail node, it is applied to all copies in the chain and is considered committed When the tail node submits a write operation, it will notify up the chain and head will respond to the client The tail node handles all read operations, so only the submitted value can be returned by the read operation Chain replication achieves strong consistency : Since all read operations are performed at the tail, and all write operations are committed at the tail, the chain tail can simply apply a total sequence to all operations. Tradeoffs vs Raft Both CRAQ and Raft/Paxos are replicated state machines. They can be used to replicate any service that can be fit into a state machine mold (basically, processes a stream of requests one at a time). One application for Raft/Paxos is object storage. CR and CRAQ are likely to be faster than protocols like Raft that provide strong consistency because the CR head does less work than the Raft leader: the CR head sends writes to just one replica, while the Raft leader must send all operations to all followers. CR has a performance advantage for reads as well, since it serves them from the tail (not the head), while the Raft leader must serve all client requests. However, Raft/Paxos and CR/CRAQ differ significantly in their failure properties. Raft (and Paxos and ZooKeeper) can continue operating (with no pauses at all) even if a minority of nodes are crashed, slow, unreliable, or partitioned. A CRAQ or CR chain must stop if something like that goes wrong, and wait for a configuration manager to decide how to proceed. On the other hand the post-failure situation is significantly simpler in CR/CRAQ; recall Figures 7 and 8 in the Raft paper. Failure recovery for chain replication: When the head node fails: the subsequent node replaces it as the head node, and there is no missing committed write operation When the tail node fails: the previous node replaces it as the tail node, and there is no lost write operation When the intermediate node fails: removed from the chain, the previous node needs to resend the most recent write operation Limitations : All reads of an object must go to the tail node, resulting in heavy load.","title":"1.1 Chain Replication"},{"location":"craq/craq.html#2-craq","text":"","title":"2. CRAQ"},{"location":"craq/craq.html#21-craq-principles","text":"CRAQ is an improvement of chain replication which allows any node in the chain to perform read operations: CRAQ Each node can store multiple versions of an object, and each version contains a monotonically increasing version number and an additional attribute (identifying clean or dirty ) When a node receives a new version of an object (via a write operation that propagates down), the node appends this latest version to the list of the object If the node is not the tail node, mark the version as dirty and pass the write operation to subsequent nodes If the node is the tail node, the version is marked as clean , at this time the write operation is committed . Then, the tail node sends ACK back in the chain to notify other nodes to submit When the ACK of the object version arrives at the node, the node will mark the object version as clean . The node can then delete all previous versions of the object When the node receives a read request from the object: -If the latest known version of the requested object is clean, the node will return this value -Otherwise, the node will contact the tail node and ask for the last submitted version number of the object on the tail node, and then the node will return this version of the object","title":"2.1 CRAQ Principles"},{"location":"craq/craq.html#22-performance-improvement","text":"CRAQ 's throughput improvement over CR occurs in two different situations: Read-intensive workload : Read operations can be performed on all nodes, so throughput is linearly proportional to chain length Write-intensive workload : In workloads with a large number of write operations, it is easier to read dirty data, so there are more query requests for tail nodes. However, the workload of querying the tail node is much lower than that of all read requests performed by the tail node, so the throughput of CRAQ is higher than that of CR .","title":"2.2 Performance Improvement"},{"location":"craq/craq.html#23-consistency-model","text":"For read operations, CRAQ supports three consistency models: Strong Consistency : The read operation described in 4.1 enables the latest written data to be read for each read, thus providing strong consistency Eventual consistency : Allow nodes to return uncommitted new data, that is, allow client to read inconsistent object versions from different nodes. But for a client , since it establishes a session with the node, its read operation is guaranteed to be monotonous and consistent. Eventual consistency with maximum inconsistency boundary : Nodes are allowed to return uncommitted new data, but there is a limit of inconsistency. This limit can be based on version or time. For example, it is allowed to return newly written but uncommitted data within a period of time.","title":"2.3 Consistency Model"},{"location":"craq/craq.html#24-zookeeper-coordination-service","text":"If the network connection between two adjacent nodes is disconnected, the subsequent node will want to become the head node, which will result in two head nodes. CRAQ itself will not solve such a problem, so an external distributed coordination service is needed to solve this problem, such as using ZooKeeper . ZooKeeper determines the composition of the chain, determines which node is the head and tail, and monitors which node has failed. When a network failure occurs, ZooKeeper determines the new composition of the chain, not based on each node's own perception of the network situation. Through the use of Zookeper watch flags , CRAQ nodes are guaranteed to receive a notification when nodes are added to or removed from a group. Similarly, a node can be notified when metadata in which it has expressed interest changes. During initialization, a CRAQ node creates an ephemeral file in /nodes/dc_name/node_id . CRAQ nodes can query /nodes/dc_name to determine the membership list for its datacenter, but instead of having to periodically check the list for changes, ZooKeeper provides processes with the ability to create a watch on a file. A CRAQ node, after creating an ephemeral file to notify other nodes it has joined the system, creates a watch on the children list of /nodes/dc_name , thereby guaranteeing that it receives a notification when a node is added or removed.","title":"2.4 ZooKeeper Coordination Service"},{"location":"craq/craq.html#31-alternate-approaches-to-improving-chain-replication","text":"A data center will probably have lots of distinct CR chains, each serving a fraction (shard) of the objects. Suppose you have three servers (S1, S2, and S3) and three chains (C1, C2, C3). Then you can have the three chains be: C1: S1 S2 S3 C2: S2 S3 S1 C3: S3 S1 S2 Now, assuming activity on the three chains is roughly equal, the load on the three servers will also be roughly equal. In particular the load of serving client requests (head and tail) will be roughly equally divided among the three servers. This is a pretty reasonable arrangement; CRAQ is only better if it turns out that some chains see more load than others .","title":"3.1 Alternate Approaches To Improving Chain Replication"},{"location":"craq/craq.html#41-summary","text":"In Chain Replication, only the head and tail directly serve client requests; the other replicas help fault tolerance but not performance. Since the load on the head and tail is thus likely to be higher than the load on intermediate nodes, you could get into a situation where performance is bottlenecked by head/tail, yet there is plenty of idle CPU available in the intermediate nodes. CRAQ exploits that idle CPU by moving the read work to them.","title":"4.1 Summary"},{"location":"distributedTransactions/distributedTransactions.html","text":"Distributed Transactions MIT Notes FAQ Background Problem: lots of data records, sharded on multiple servers, lots of clients Correct behavior of a xactions: ACID A: Atomicity, all writes or none, despite failures C: obeys application-specific invariants I: Isolation, no interference between xactions -- serializable D: Durability, committed writes are permanent How to test whether the xactions is serializable example transactions x and y are bank balances -- records in database tables x and y are on different servers (maybe at different banks) x and y start out as $10 T1 and T2 are transactions T1: transfer $1 from x to y T2: audit, to check that no money is lost T1: T2: begin_xaction begin_xaction add(x, 1) tmp1 = get(x) add(y, -1) tmp2 = get(y) end_xaction print tmp1, tmp2 end_xaction execute concurrent transactions T1 and T2 T1; T2 : x=11 y=9 \"11,9\" T2; T1 : x=11 y=9 \"10,10\" the results for the two differ; either is OK Concurrency Control Distributed transactions have two big components: concurrency control (to provide isolation/serializability) atomic commit (to provide atomicity despite failure) Two classes of concurrency control for transactions: pessimistic: lock records before use conflicts cause delays (waiting for locks) optimistic: use records without locking commit checks if reads/writes were serializable conflict causes abort+retry called Optimistic Concurrency Control (OCC) pessimistic is faster if conflicts are frequent optimistic is faster if conflicts are rare \"Two-phase locking\" is pessimistic: a transaction must acquire a record's lock before using it a transaction must hold its locks until after commit or abort Process: TC A B |----put------>|(lock data) |-----------get---------->|(lock data) | | |---prepare--->| |----------prepare-------->| | |<---yes/no----| |<-----------yes/no--------| | | |-commit/abort->|(commit/abort and release lock) |---------commit/abort---->|(commit/abort and release lock) |<----ack-----| |<-------------ack---------| Failure tolerance: What if B crashes and restarts? If B sent YES before crash, B must remember (despite crash)! Because A might have received a COMMIT and committed. So B must be able to commit (or not) even after a reboot. write log to disk What if TC crashes and restarts? If TC might have sent COMMIT before crash, TC must remember! Since one worker may already have committed. write log to disk before sending COMMIT msgs. repeat COMMIT if it crashes and reboots, or if a participants asks. What if TC never gets a YES/NO from B? Perhaps B crashed and didn't recover; perhaps network is broken. TC can time out, and abort (since has not sent any COMMIT msgs). What if B times out or crashes while waiting for PREPARE from TC? B has not yet responded to PREPARE, so TC can't have decided commit B can unilaterally abort, and release locks respond NO to future PREPARE What if B replied YES to PREPARE, but doesn't receive COMMIT or ABORT? B cannot decide to abort it, because TC might have gotten YES from both, and sent out COMMIT to A, but crashed before sending to B. cannot do anything, just wait for TC came back Two-phase commit perspective: Used in sharded DBs when a transaction uses data on multiple shards slow: multiple rounds of messages slow: disk writes locks are held over the prepare/commit exchanges; blocks other xactions TC crash can cause indefinite blocking, with locks held Consolidate with Raft Difference between Raft: Use Raft to get high availability by replicating, Raft does not ensure that all servers do something. Only a majority have to be alive. Use 2PC when each participant does something different, 2PC does not help availability Consolidate with Raft, achieve both high availability and atomic commit: The TC and servers should each be replicated with Raft It is the basic theory of Spanner","title":"distributedTransactions"},{"location":"distributedTransactions/distributedTransactions.html#distributed-transactions","text":"MIT Notes FAQ","title":"Distributed Transactions"},{"location":"distributedTransactions/distributedTransactions.html#background","text":"Problem: lots of data records, sharded on multiple servers, lots of clients Correct behavior of a xactions: ACID A: Atomicity, all writes or none, despite failures C: obeys application-specific invariants I: Isolation, no interference between xactions -- serializable D: Durability, committed writes are permanent How to test whether the xactions is serializable example transactions x and y are bank balances -- records in database tables x and y are on different servers (maybe at different banks) x and y start out as $10 T1 and T2 are transactions T1: transfer $1 from x to y T2: audit, to check that no money is lost T1: T2: begin_xaction begin_xaction add(x, 1) tmp1 = get(x) add(y, -1) tmp2 = get(y) end_xaction print tmp1, tmp2 end_xaction execute concurrent transactions T1 and T2 T1; T2 : x=11 y=9 \"11,9\" T2; T1 : x=11 y=9 \"10,10\" the results for the two differ; either is OK","title":"Background"},{"location":"distributedTransactions/distributedTransactions.html#concurrency-control","text":"Distributed transactions have two big components: concurrency control (to provide isolation/serializability) atomic commit (to provide atomicity despite failure) Two classes of concurrency control for transactions: pessimistic: lock records before use conflicts cause delays (waiting for locks) optimistic: use records without locking commit checks if reads/writes were serializable conflict causes abort+retry called Optimistic Concurrency Control (OCC) pessimistic is faster if conflicts are frequent optimistic is faster if conflicts are rare \"Two-phase locking\" is pessimistic: a transaction must acquire a record's lock before using it a transaction must hold its locks until after commit or abort Process: TC A B |----put------>|(lock data) |-----------get---------->|(lock data) | | |---prepare--->| |----------prepare-------->| | |<---yes/no----| |<-----------yes/no--------| | | |-commit/abort->|(commit/abort and release lock) |---------commit/abort---->|(commit/abort and release lock) |<----ack-----| |<-------------ack---------| Failure tolerance: What if B crashes and restarts? If B sent YES before crash, B must remember (despite crash)! Because A might have received a COMMIT and committed. So B must be able to commit (or not) even after a reboot. write log to disk What if TC crashes and restarts? If TC might have sent COMMIT before crash, TC must remember! Since one worker may already have committed. write log to disk before sending COMMIT msgs. repeat COMMIT if it crashes and reboots, or if a participants asks. What if TC never gets a YES/NO from B? Perhaps B crashed and didn't recover; perhaps network is broken. TC can time out, and abort (since has not sent any COMMIT msgs). What if B times out or crashes while waiting for PREPARE from TC? B has not yet responded to PREPARE, so TC can't have decided commit B can unilaterally abort, and release locks respond NO to future PREPARE What if B replied YES to PREPARE, but doesn't receive COMMIT or ABORT? B cannot decide to abort it, because TC might have gotten YES from both, and sent out COMMIT to A, but crashed before sending to B. cannot do anything, just wait for TC came back Two-phase commit perspective: Used in sharded DBs when a transaction uses data on multiple shards slow: multiple rounds of messages slow: disk writes locks are held over the prepare/commit exchanges; blocks other xactions TC crash can cause indefinite blocking, with locks held","title":"Concurrency Control"},{"location":"distributedTransactions/distributedTransactions.html#consolidate-with-raft","text":"Difference between Raft: Use Raft to get high availability by replicating, Raft does not ensure that all servers do something. Only a majority have to be alive. Use 2PC when each participant does something different, 2PC does not help availability Consolidate with Raft, achieve both high availability and atomic commit: The TC and servers should each be replicated with Raft It is the basic theory of Spanner","title":"Consolidate with Raft"},{"location":"frangipani/frangipani.html","text":"Frangipani: A Scalable Distributed File System i 1990s scalable distributed file system that manages a collection of disks on multiple machines as a single shared pool of storage. MIT Notes FAQ Background What to digest: strong consistency cache coherence distributed transactions distributed crash recovery Overall design: A decentralized file system , cache for performance Petal: block storage service; what is stored: just like an ordinary hard disk file system directores i-node file content blocks free bitmaps Scenario 1: WS2(work station) run ls / or cat /grades while WS1 is modifying the same inode Scenario 2: WS1 and WS2 concurrently try to create /a , /b under same directory Scenario 3: WS1 crashes while creating a file operation: allocate i-node, initialize i-node, update directory Components Petal : block storage service; replicated Lock Server (LS) , with one lock per file/directory , the locks are named by files/directories (really i-numbers) file owner ----------- x WS1 y WS1 Workstation (WS) cache(store the lock status): file/dir lock content ----------------------- x busy ... //using data right now y idle ... //holds lock but not using the cached data right now Solution for Scenario 1: Cache Coherence (revealing writes) It is to guarantee linearizability AND caching Example: WS1 changes file z , then WS2 reads z WS1 changes file z request lock (WS1 -> LS) LS put: owner(z) = WS1 grant lock (LS -> WS1) (WS1: read+cache z data from Petal modify z locally when done, cached lock in state) WS2 try read file z Revoke Lock: Coherence Protocol Msg request lock (WS2 -> LS) grant (LS -> WS2) revoke (LS -> WS1) (write update log of metadata to Petal) (write modified z to Petal) release lock (WS1 -> LS) WS2 get the lock, and read z from Petal notes: locks and rules force reads to see last write one optimization: Frangipani has shared read locks, as well as exclusive write locks Solution for Scenario 2: Atomic Transactions (concealing writes) There is two operation for create/rename/delete a file, 1. create/rename initializes i-node, adds to directory entry. 2. rename. The challenge is to guarantee the atomicity. Transactional file-system operations: operation corresponds to a system call (create file, remove file, rename, &c) WS acquires locks on all file system data that it will modify performs operation with all locks held only releases when finished no other WS can see partially-completed operations Solution for Scenario 3: Crash Recovery (write-ahead logging) What if a Frangipani workstation dies while holding locks? eg: dead WS had modified data in its cache eg: dead WS had started to write back modified data to Petal Solution: Before writing any of op's cached blocks to Petal, first write log to Petal Log entry stored in Petal for recovery Before writing any of op's cached blocks to Petal, first write log to Petal if a crashed workstation has done some Petal writes for an operation, not not all. the writes can be completed from the log in Petal What an log entry include: note : it is just for file metadata, not for file data log sequence number array of updates: block #, new version #, addr, new bytes When WS receive lock revoke: Play log in the Petal (Petal already store it) (P) WS send the cached updated blocks to Petal (WS1-P) Release the lock (WS1-LS) Why version number is necessary: for linearizability of the metadata update WS1: delete(d/f)(v1) crash WS2: create(d/f)(v2) WS3: recover WS1 WS3 is recovering WS1's log -- but it doesn't look at WS2's log When WS1 delete the file, it add a log entry with v1 when WS2 create a file with same name, the system will give it a version number v2 so when replay the log of ws1, the v1<v2. so ignore it.","title":"frangipani"},{"location":"frangipani/frangipani.html#frangipani-a-scalable-distributed-file-systemi","text":"1990s scalable distributed file system that manages a collection of disks on multiple machines as a single shared pool of storage. MIT Notes FAQ","title":"Frangipani: A Scalable Distributed File Systemi"},{"location":"frangipani/frangipani.html#background","text":"What to digest: strong consistency cache coherence distributed transactions distributed crash recovery Overall design: A decentralized file system , cache for performance Petal: block storage service; what is stored: just like an ordinary hard disk file system directores i-node file content blocks free bitmaps Scenario 1: WS2(work station) run ls / or cat /grades while WS1 is modifying the same inode Scenario 2: WS1 and WS2 concurrently try to create /a , /b under same directory Scenario 3: WS1 crashes while creating a file operation: allocate i-node, initialize i-node, update directory","title":"Background"},{"location":"frangipani/frangipani.html#components","text":"Petal : block storage service; replicated Lock Server (LS) , with one lock per file/directory , the locks are named by files/directories (really i-numbers) file owner ----------- x WS1 y WS1 Workstation (WS) cache(store the lock status): file/dir lock content ----------------------- x busy ... //using data right now y idle ... //holds lock but not using the cached data right now","title":"Components"},{"location":"frangipani/frangipani.html#solution-for-scenario-1-cache-coherence-revealing-writes","text":"It is to guarantee linearizability AND caching Example: WS1 changes file z , then WS2 reads z WS1 changes file z request lock (WS1 -> LS) LS put: owner(z) = WS1 grant lock (LS -> WS1) (WS1: read+cache z data from Petal modify z locally when done, cached lock in state) WS2 try read file z Revoke Lock: Coherence Protocol Msg request lock (WS2 -> LS) grant (LS -> WS2) revoke (LS -> WS1) (write update log of metadata to Petal) (write modified z to Petal) release lock (WS1 -> LS) WS2 get the lock, and read z from Petal notes: locks and rules force reads to see last write one optimization: Frangipani has shared read locks, as well as exclusive write locks","title":"Solution for Scenario 1: Cache Coherence (revealing writes)"},{"location":"frangipani/frangipani.html#solution-for-scenario-2-atomic-transactions-concealing-writes","text":"There is two operation for create/rename/delete a file, 1. create/rename initializes i-node, adds to directory entry. 2. rename. The challenge is to guarantee the atomicity. Transactional file-system operations: operation corresponds to a system call (create file, remove file, rename, &c) WS acquires locks on all file system data that it will modify performs operation with all locks held only releases when finished no other WS can see partially-completed operations","title":"Solution for Scenario 2: Atomic Transactions (concealing writes)"},{"location":"frangipani/frangipani.html#solution-for-scenario-3-crash-recovery-write-ahead-logging","text":"What if a Frangipani workstation dies while holding locks? eg: dead WS had modified data in its cache eg: dead WS had started to write back modified data to Petal Solution: Before writing any of op's cached blocks to Petal, first write log to Petal Log entry stored in Petal for recovery Before writing any of op's cached blocks to Petal, first write log to Petal if a crashed workstation has done some Petal writes for an operation, not not all. the writes can be completed from the log in Petal What an log entry include: note : it is just for file metadata, not for file data log sequence number array of updates: block #, new version #, addr, new bytes When WS receive lock revoke: Play log in the Petal (Petal already store it) (P) WS send the cached updated blocks to Petal (WS1-P) Release the lock (WS1-LS) Why version number is necessary: for linearizability of the metadata update WS1: delete(d/f)(v1) crash WS2: create(d/f)(v2) WS3: recover WS1 WS3 is recovering WS1's log -- but it doesn't look at WS2's log When WS1 delete the file, it add a log entry with v1 when WS2 create a file with same name, the system will give it a version number v2 so when replay the log of ws1, the v1<v2. so ignore it.","title":"Solution for Scenario 3: Crash Recovery (write-ahead logging)"},{"location":"gfs/gfs.html","text":"The Google File System A scalable distributed file system for large distributed data-intensive applications. Introduction The design is driven by key observations different from earlier file system assumptions: frequent component failures, huge files (GB+), most files are mutated by appending rather than overwriting. GFS provides a POSIX-like file system interface. It supports snapshot and record append operations(useful for multi-way merge and consumer/producer). Architecture Figure 1 A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients. Files are divided into fixed-sized chunks(64 MB, larger than typical) and each chunk has an immutable and globally unique id. Chunkservers store chunks as local Linux files and the master maintains all file system metadata. The master stores in memory three majors types of metadata: the file and chunk namespaces, the mapping from files to chunks and the locations of each chunk's replicas. The first two are also persisted in a replicated operation log(with checkpoints); the last one is polled from chunkservers at start time and kept up-to-date via heartbeat messages. Reads Using the fixed chunk size, the client translates the file name and byte offset into a chunk index within the file. Sends the master a request containing the file name and chunk index. The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the file name and chunk index as the key. The client then sends a request to one of the replicas,most likely the closest one. The request specifies the chunk handle and a byte range within that chunk. Further reads of the same chunk require no more client-master interaction until the cached information expires. Chunk Size Key design parameters, Large chunk chosen : 64MB Advantages Reduces clients' need to interact with the master Operations are more likely to target at the same chunk, can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time Reduces metadata size, easier for master to store in memory Disadvantages Small files -> small # of chunks -> the chunkserver becomes a hot spot Operation Log Contains a historical record of critical metadata changes. Replicated on multiple remote machines. Respond to a client operation only after flushing the corresponding log record to disk both locally and remotely. Recovers file system by replaying the log. Master checkpoints its state in a compact B-tree like form whenever the log grows beyond a certain size. Only keeps latest complete checkpoint and subsequent log files. Figure 2 System Interactions Leases and Mutation Order For mutations to a chunk, the master grants an extendable chunk lease to one of the replicas. The primary will pick a serial order for all mutations to the chunk and all replicas will follow this order. Control and data flows are decoupled to use the network efficiently. Control flows from the client to the primary and then to all secodaries. Data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. GFS provides at-least-once semantics. A client retries when mutation fails, so replicas of the same chunk may contain different data including duplicates or paddings(not bytewise identical). Writes Client asks the master which chunkserver holds the current lease for the chunk and replica locations. Master replies with primary and secondaries locations. Caches this until primary unavailable/no longer has lease. Client pushes the data to all the replicas. Each chunkserver will store the data in an internal LRU buffer cache. After all the replicas ack, client sends a write request to the primary. Primary assigns write order to all mutations it receives. Primary forwards the write request to all secondary replicas with mutation order. Secondaries ack to primary on completion. Primary replies to client. Failure -> client retry. Master Operations GFS namespace is represented as a lookup table(with prefix compression) from full pathnames to metadata. Each file or directory has a read-write lock to ensure proper searialization. The master create chunk replicas for initial creation, re-replication and rebalancing. It considers various factors when placing the replica, to maximizing data reliability/availability/network bandwidth utilization. When a file is deleted, the master logs the deletion immediately but deletes metadata after some delay(3 days, etc). Each chunkserver reports a subset of chunks in heartbeat messages, and the master replies whether these chunks can be deleted. This lazy approach is simple and reliable in a large-scale distributed system. The master maintains a version number for each chunk. Whenever the master grants a new lease on the chunk, it increases the version number and informs all the up-to-date replicas. For high availability, we have (1) monitoring infrastructure outside GFS to start a master and (2) \"shadow\" masters for read-only operations. Fault Tolerance High Availability Fast Recovery - Both the master and the chunkserver are designed to restore their state and start in seconds. Chunk Replication - default 3 replications per chunk. Master Replication - Operation log and checkpoints replicated on multiple machines. \"Shadow\" masters above. Data Integrity Breaks each 64 MB chunk into blocks of 64 KB, each with its own 32-bit checksum stored in memory and written to the log. For reads, the chunkserver verifies the checksum of datablocks that overlap the read range before returning any data to the requester If a block does not match the recorded checksum, the chunkserver returns an error and and reports the mismatch to the master. In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica. Summary Good Ideas High Availability through fast recovery of master and chunkserver. Separation of naming (master) from storage (chunkserver). Sharding (chunk replication) for parallel throughput. Primary to sequence writes. Leases to prevent split-brain chunkserver primaries. Not So Good Single master performance : Ran out of RAM and CPU as file count increased 1000x Chunkservers not very efficient for small files Lack of automatic fail-over to master replica","title":"gfs"},{"location":"gfs/gfs.html#the-google-file-system","text":"A scalable distributed file system for large distributed data-intensive applications.","title":"The Google File System"},{"location":"gfs/gfs.html#introduction","text":"The design is driven by key observations different from earlier file system assumptions: frequent component failures, huge files (GB+), most files are mutated by appending rather than overwriting. GFS provides a POSIX-like file system interface. It supports snapshot and record append operations(useful for multi-way merge and consumer/producer).","title":"Introduction"},{"location":"gfs/gfs.html#architecture","text":"Figure 1 A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients. Files are divided into fixed-sized chunks(64 MB, larger than typical) and each chunk has an immutable and globally unique id. Chunkservers store chunks as local Linux files and the master maintains all file system metadata. The master stores in memory three majors types of metadata: the file and chunk namespaces, the mapping from files to chunks and the locations of each chunk's replicas. The first two are also persisted in a replicated operation log(with checkpoints); the last one is polled from chunkservers at start time and kept up-to-date via heartbeat messages. Reads Using the fixed chunk size, the client translates the file name and byte offset into a chunk index within the file. Sends the master a request containing the file name and chunk index. The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the file name and chunk index as the key. The client then sends a request to one of the replicas,most likely the closest one. The request specifies the chunk handle and a byte range within that chunk. Further reads of the same chunk require no more client-master interaction until the cached information expires.","title":"Architecture"},{"location":"gfs/gfs.html#chunk-size","text":"Key design parameters, Large chunk chosen : 64MB Advantages Reduces clients' need to interact with the master Operations are more likely to target at the same chunk, can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time Reduces metadata size, easier for master to store in memory Disadvantages Small files -> small # of chunks -> the chunkserver becomes a hot spot","title":"Chunk Size"},{"location":"gfs/gfs.html#operation-log","text":"Contains a historical record of critical metadata changes. Replicated on multiple remote machines. Respond to a client operation only after flushing the corresponding log record to disk both locally and remotely. Recovers file system by replaying the log. Master checkpoints its state in a compact B-tree like form whenever the log grows beyond a certain size. Only keeps latest complete checkpoint and subsequent log files. Figure 2","title":"Operation Log"},{"location":"gfs/gfs.html#system-interactions","text":"","title":"System Interactions"},{"location":"gfs/gfs.html#leases-and-mutation-order","text":"For mutations to a chunk, the master grants an extendable chunk lease to one of the replicas. The primary will pick a serial order for all mutations to the chunk and all replicas will follow this order. Control and data flows are decoupled to use the network efficiently. Control flows from the client to the primary and then to all secodaries. Data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. GFS provides at-least-once semantics. A client retries when mutation fails, so replicas of the same chunk may contain different data including duplicates or paddings(not bytewise identical). Writes Client asks the master which chunkserver holds the current lease for the chunk and replica locations. Master replies with primary and secondaries locations. Caches this until primary unavailable/no longer has lease. Client pushes the data to all the replicas. Each chunkserver will store the data in an internal LRU buffer cache. After all the replicas ack, client sends a write request to the primary. Primary assigns write order to all mutations it receives. Primary forwards the write request to all secondary replicas with mutation order. Secondaries ack to primary on completion. Primary replies to client. Failure -> client retry.","title":"Leases and Mutation Order"},{"location":"gfs/gfs.html#master-operations","text":"GFS namespace is represented as a lookup table(with prefix compression) from full pathnames to metadata. Each file or directory has a read-write lock to ensure proper searialization. The master create chunk replicas for initial creation, re-replication and rebalancing. It considers various factors when placing the replica, to maximizing data reliability/availability/network bandwidth utilization. When a file is deleted, the master logs the deletion immediately but deletes metadata after some delay(3 days, etc). Each chunkserver reports a subset of chunks in heartbeat messages, and the master replies whether these chunks can be deleted. This lazy approach is simple and reliable in a large-scale distributed system. The master maintains a version number for each chunk. Whenever the master grants a new lease on the chunk, it increases the version number and informs all the up-to-date replicas. For high availability, we have (1) monitoring infrastructure outside GFS to start a master and (2) \"shadow\" masters for read-only operations.","title":"Master Operations"},{"location":"gfs/gfs.html#fault-tolerance","text":"High Availability Fast Recovery - Both the master and the chunkserver are designed to restore their state and start in seconds. Chunk Replication - default 3 replications per chunk. Master Replication - Operation log and checkpoints replicated on multiple machines. \"Shadow\" masters above. Data Integrity Breaks each 64 MB chunk into blocks of 64 KB, each with its own 32-bit checksum stored in memory and written to the log. For reads, the chunkserver verifies the checksum of datablocks that overlap the read range before returning any data to the requester If a block does not match the recorded checksum, the chunkserver returns an error and and reports the mismatch to the master. In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica.","title":"Fault Tolerance"},{"location":"gfs/gfs.html#summary","text":"Good Ideas High Availability through fast recovery of master and chunkserver. Separation of naming (master) from storage (chunkserver). Sharding (chunk replication) for parallel throughput. Primary to sequence writes. Leases to prevent split-brain chunkserver primaries. Not So Good Single master performance : Ran out of RAM and CPU as file count increased 1000x Chunkservers not very efficient for small files Lack of automatic fail-over to master replica","title":"Summary"},{"location":"mapreduce/mapreduce.html","text":"MapReduce: Simplified Data Processing on Large Clusters A programming model and an associated implementation for processing and generating large data sets. Introduction Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. map (k1,v1) \u2192 list(k2,v2) reduce (k2,list(v2)) \u2192 list(v2) Implementation Execution A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk partitioned into R regions. The locations of these buffered pairs on disk are passed back to the master, who forwards these locations to the reduce workers. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks. Master Data Structures stores state(idle, in-progress, competed) and identity of the worker machine for each task (non idle). for each completed map task: locations and sizes of the R intermediate file regions. The information is pushed incrementally to in-progress reduce workers. Fault Tolerance For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed and in-progress map tasks are reset to idle and reduce workers executing are notified. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry. Miscellaneous Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers. MR takes 44% longer without backup tasks. Refinements Partitioning Function: Typically hash(key) mod R is used for hashing. However, it is useful to allow custom partitioning function so that for example users can have all URLs from the same host end up in the same output file. Combiner Function: In some cases, there is significant repetition in the intermediate keys produced by each map task. For example, word count map produces <word, 1> for each word. All of these counts will be sent over RPC. Better allow user to specify optional Combiner that does partial merging of data before sent to reducers.","title":"mapreduce"},{"location":"mapreduce/mapreduce.html#mapreduce-simplified-data-processing-on-large-clusters","text":"A programming model and an associated implementation for processing and generating large data sets.","title":"MapReduce: Simplified Data Processing on Large Clusters"},{"location":"mapreduce/mapreduce.html#introduction","text":"Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. map (k1,v1) \u2192 list(k2,v2) reduce (k2,list(v2)) \u2192 list(v2)","title":"Introduction"},{"location":"mapreduce/mapreduce.html#implementation","text":"","title":"Implementation"},{"location":"mapreduce/mapreduce.html#execution","text":"A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk partitioned into R regions. The locations of these buffered pairs on disk are passed back to the master, who forwards these locations to the reduce workers. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks.","title":"Execution"},{"location":"mapreduce/mapreduce.html#master-data-structures","text":"stores state(idle, in-progress, competed) and identity of the worker machine for each task (non idle). for each completed map task: locations and sizes of the R intermediate file regions. The information is pushed incrementally to in-progress reduce workers.","title":"Master Data Structures"},{"location":"mapreduce/mapreduce.html#fault-tolerance","text":"For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed and in-progress map tasks are reset to idle and reduce workers executing are notified. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry.","title":"Fault Tolerance"},{"location":"mapreduce/mapreduce.html#miscellaneous","text":"Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers. MR takes 44% longer without backup tasks.","title":"Miscellaneous"},{"location":"mapreduce/mapreduce.html#refinements","text":"Partitioning Function: Typically hash(key) mod R is used for hashing. However, it is useful to allow custom partitioning function so that for example users can have all URLs from the same host end up in the same output file. Combiner Function: In some cases, there is significant repetition in the intermediate keys produced by each map task. For example, word count map produces <word, 1> for each word. All of these counts will be sent over RPC. Better allow user to specify optional Combiner that does partial merging of data before sent to reducers.","title":"Refinements"},{"location":"raft/raft.html","text":"Raft: In Search of an Understandable Consensus Algorithm A consensus algorithm for managing a replicated log MIT Notes Part1 Part2 MIT FAQ Part1 Part2 Raft Basics Raft decomposes the consensus problem into three relatively independent subproblems Leader Election Log replication Safety Properties Election Safety : at most one leader can be elected in a given term. Leader Append-Only : a leader never overwrites or deletes entries in its log; it only appends new entries. Log Matching : if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. Leader Completeness : if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. State Machine Safety : if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. Raft divides time into terms of arbitrary length. Used extensively in leader election and log matching. 1. Leader Election When servers start up, they begin as followers Leader sends periodic heartbeats (AppendEntries RPC with no log entries) to all followers to maintain authority. If follower receives no communication over election timeout , then it assumes there is no viable leader and begins an election to choose a new leader. Begin an election: follower increments its current term and transitions to candidate state, votes for itself and issue RequestVoteRPCs in parallel to each else Candidate continue in this state until one of three things happens: a. it wins b. other wins c. no one wins Candidate wins if it receives the majority of votes, voting is first-come-first-served. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If its term is larger or equal than itself's, the candidate recognizes the leader as legit and returns to follower state Case : No one wins -> after timeout, new election (150ms - 300ms) - Raft uses randomized election timeouts to ensure that split votes are rare. 2. Log Replication I. Steps Leader appends log entry to its own log Send AppendEntry to every server When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. II. Commit A log entry is committed once the leader that created the entry has replicated it on a majority of the servers Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines This also commits all preceding entries in the leader's log, including entries created by previous leaders Leader keep tracks of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs. Once a follower learns that a log entry is committed, it applies the entry to its local state machine. III. Inconsistency Handling Leader maintains a nextIndex fields for each follower. It initializes nextIndex values to the index just after the last one in its log. If follower's log is inconsistent with the leader's, the AppendEntries RPC consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC 3. Safety Ensure each state machine executes same commands in the same order I. Election Restriction Candidate only elected as leader when its log is at least as up-to-date as any other log in that majority Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. Term1 > Term2 -> Term1 wins Term1 == Term2 -> Log with greater length wins II. Committing entries from previous terms Never commits log entries from previous terms by counting replicas. Only log entries from the leader's current term are committed by counting replicas Once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. Log Compaction InstallSnapshot RPC As Raft log grows without bound, it occupies more space and takes more time to replay on server startup. Snapshotting - saves entire system state to a snapshot on stable storage and the entire log up to that point is discarded. Each server takes snapshots independently covering just the committed entries in its log. Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind. This happens when the leader has already discarded the next log entry that it needs to send to a follower. Summary","title":"raft"},{"location":"raft/raft.html#raft-in-search-of-an-understandable-consensus-algorithm","text":"A consensus algorithm for managing a replicated log MIT Notes Part1 Part2 MIT FAQ Part1 Part2","title":"Raft: In Search of an Understandable Consensus Algorithm"},{"location":"raft/raft.html#raft-basics","text":"Raft decomposes the consensus problem into three relatively independent subproblems Leader Election Log replication Safety Properties Election Safety : at most one leader can be elected in a given term. Leader Append-Only : a leader never overwrites or deletes entries in its log; it only appends new entries. Log Matching : if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. Leader Completeness : if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. State Machine Safety : if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. Raft divides time into terms of arbitrary length. Used extensively in leader election and log matching.","title":"Raft Basics"},{"location":"raft/raft.html#1-leader-election","text":"When servers start up, they begin as followers Leader sends periodic heartbeats (AppendEntries RPC with no log entries) to all followers to maintain authority. If follower receives no communication over election timeout , then it assumes there is no viable leader and begins an election to choose a new leader. Begin an election: follower increments its current term and transitions to candidate state, votes for itself and issue RequestVoteRPCs in parallel to each else Candidate continue in this state until one of three things happens: a. it wins b. other wins c. no one wins Candidate wins if it receives the majority of votes, voting is first-come-first-served. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If its term is larger or equal than itself's, the candidate recognizes the leader as legit and returns to follower state Case : No one wins -> after timeout, new election (150ms - 300ms) - Raft uses randomized election timeouts to ensure that split votes are rare.","title":"1. Leader Election"},{"location":"raft/raft.html#2-log-replication","text":"","title":"2. Log Replication"},{"location":"raft/raft.html#i-steps","text":"Leader appends log entry to its own log Send AppendEntry to every server When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client.","title":"I. Steps"},{"location":"raft/raft.html#ii-commit","text":"A log entry is committed once the leader that created the entry has replicated it on a majority of the servers Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines This also commits all preceding entries in the leader's log, including entries created by previous leaders Leader keep tracks of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs. Once a follower learns that a log entry is committed, it applies the entry to its local state machine.","title":"II. Commit"},{"location":"raft/raft.html#iii-inconsistency-handling","text":"Leader maintains a nextIndex fields for each follower. It initializes nextIndex values to the index just after the last one in its log. If follower's log is inconsistent with the leader's, the AppendEntries RPC consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC","title":"III. Inconsistency Handling"},{"location":"raft/raft.html#3-safety","text":"Ensure each state machine executes same commands in the same order","title":"3. Safety"},{"location":"raft/raft.html#i-election-restriction","text":"Candidate only elected as leader when its log is at least as up-to-date as any other log in that majority Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. Term1 > Term2 -> Term1 wins Term1 == Term2 -> Log with greater length wins","title":"I. Election Restriction"},{"location":"raft/raft.html#ii-committing-entries-from-previous-terms","text":"Never commits log entries from previous terms by counting replicas. Only log entries from the leader's current term are committed by counting replicas Once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property.","title":"II. Committing entries from previous terms"},{"location":"raft/raft.html#log-compaction","text":"","title":"Log Compaction"},{"location":"raft/raft.html#installsnapshot-rpc","text":"As Raft log grows without bound, it occupies more space and takes more time to replay on server startup. Snapshotting - saves entire system state to a snapshot on stable storage and the entire log up to that point is discarded. Each server takes snapshots independently covering just the committed entries in its log. Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind. This happens when the leader has already discarded the next log entry that it needs to send to a follower.","title":"InstallSnapshot RPC"},{"location":"raft/raft.html#summary","text":"","title":"Summary"},{"location":"spanner/spanner.html","text":"Spanner: Google\u2019s Globally-Distributed Database Google\u2019s scalable, multi-version, globally-distributed and synchronously-replicated database. MIT Notes FAQ Introduction Bigtable is hard to use for applications need complex, evolving schemas or strong consistency in wide-area replication, so Spanner evolved into a temporal, multi-version, semi-relational database. With a new TrueTime API that exposes clock uncertainty, Spanner assigns globally-meaningful commit timestamp to even distributed transactions. Spanner shards data across many sets of Paxos state machines in data centers spread all over the world. System Architecture A Spanner zone has one zonemaster and many spanservers. The former assigns data to spanservers; the latter serve data to clients. A spanserver is responsible for 100~1000 tablets. Each tablet contains a bag of mappings (key:string, timestamp:int64)\\rightarrow string from multiple partitions of the row space(data colocation). A tablet's state is stored in a set of B-tree like files and a write-ahead log on Colossus. A spanserver implements a Paxos state machine on each tablet; each Paxos state machine stores metadata and log in corresponding tablet. At every replica that is a leader, each spanserver implements a lock table and a transaction table to support distributed transactions(two-phase commit on mutliple Paxos groups). TrueTime Main method TT.now() returns an interval [earliest, latest] that is guaranteed to contain the absolute time when TT.now() is invoked. TT.after(t) and TT.before(t) are convenience wrappers. TrueTime uses two forms of time references, GPS and atomic clocks, as they have different failure modes. TrueTime is implemented by a set of time master machines per data center and a time slave daemon per machine. Each daemon polls a variety of masters, apply Marzullo's algorithm to detect liars and synchronize the local machine clock to non-liars. Between synchronizations, a daemon advertises a slowly increasing time uncertainty \\epsilon (average 4ms in production) derived from worst-case local clock drift, time master uncertainty and communication delay to the time masters. Concurrency Control Spanner supports read-write transactions, read-only transactions and snapshot reads. Read-only transactions and snapshot reads execute at a specific timestamp without locking. Spanner maintains following invariants: disjointness: * within a Paxos group, each Paxos leader's lease interval is disjoint from every other leader's. monotonicity: within a Paxos group, Paxos writes have monotonically increasing timestamp. external consistency: if the start of a transaction T_2 occurs after the commit of a transaction T_1 , then the commit timestamp of T_2 must be greater than the commit timestamp of T_1 . For RW transactions, the coordinator assigns a commit timestamp S_i no later than TT.now().latest computed after receiving the commit request, and waits until TT.after(S_i) is true to apply the commit(\"commit wait\"). For RO transactions and snapshot reads, every replica tracks t_{safe} for the maximum at which the replica is up-to-date, which depends on the Paxos state machine and if there are prepared but not committed transactions.","title":"spanner"},{"location":"spanner/spanner.html#spanner-googles-globally-distributed-database","text":"Google\u2019s scalable, multi-version, globally-distributed and synchronously-replicated database. MIT Notes FAQ","title":"Spanner: Google\u2019s Globally-Distributed Database"},{"location":"spanner/spanner.html#introduction","text":"Bigtable is hard to use for applications need complex, evolving schemas or strong consistency in wide-area replication, so Spanner evolved into a temporal, multi-version, semi-relational database. With a new TrueTime API that exposes clock uncertainty, Spanner assigns globally-meaningful commit timestamp to even distributed transactions. Spanner shards data across many sets of Paxos state machines in data centers spread all over the world.","title":"Introduction"},{"location":"spanner/spanner.html#system-architecture","text":"A Spanner zone has one zonemaster and many spanservers. The former assigns data to spanservers; the latter serve data to clients. A spanserver is responsible for 100~1000 tablets. Each tablet contains a bag of mappings (key:string, timestamp:int64)\\rightarrow string from multiple partitions of the row space(data colocation). A tablet's state is stored in a set of B-tree like files and a write-ahead log on Colossus. A spanserver implements a Paxos state machine on each tablet; each Paxos state machine stores metadata and log in corresponding tablet. At every replica that is a leader, each spanserver implements a lock table and a transaction table to support distributed transactions(two-phase commit on mutliple Paxos groups).","title":"System Architecture"},{"location":"spanner/spanner.html#truetime","text":"Main method TT.now() returns an interval [earliest, latest] that is guaranteed to contain the absolute time when TT.now() is invoked. TT.after(t) and TT.before(t) are convenience wrappers. TrueTime uses two forms of time references, GPS and atomic clocks, as they have different failure modes. TrueTime is implemented by a set of time master machines per data center and a time slave daemon per machine. Each daemon polls a variety of masters, apply Marzullo's algorithm to detect liars and synchronize the local machine clock to non-liars. Between synchronizations, a daemon advertises a slowly increasing time uncertainty \\epsilon (average 4ms in production) derived from worst-case local clock drift, time master uncertainty and communication delay to the time masters.","title":"TrueTime"},{"location":"spanner/spanner.html#concurrency-control","text":"Spanner supports read-write transactions, read-only transactions and snapshot reads. Read-only transactions and snapshot reads execute at a specific timestamp without locking. Spanner maintains following invariants: disjointness: * within a Paxos group, each Paxos leader's lease interval is disjoint from every other leader's. monotonicity: within a Paxos group, Paxos writes have monotonically increasing timestamp. external consistency: if the start of a transaction T_2 occurs after the commit of a transaction T_1 , then the commit timestamp of T_2 must be greater than the commit timestamp of T_1 . For RW transactions, the coordinator assigns a commit timestamp S_i no later than TT.now().latest computed after receiving the commit request, and waits until TT.after(S_i) is true to apply the commit(\"commit wait\"). For RO transactions and snapshot reads, every replica tracks t_{safe} for the maximum at which the replica is up-to-date, which depends on the Paxos state machine and if there are prepared but not committed transactions.","title":"Concurrency Control"},{"location":"tao/tao.html","text":"TAO: Facebook\u2019s Distributed Data Store for the Social Graph A geographically distributed data store that provides efficient and timely access to the social graph. Background The content Facebook presents to each user is highly tailored, so we have to perform filtering and aggregation when the content is viewed instead of when the content is created. Facebook was originally built with MySQL/PHP/memcache. This look-aside key-value cache architecture has problems like inefficient edge lists, distributed control logic and expensive read-after-write consistency. TAO is designed to handle the workload of read-mostly access to a constantly changing graph. Efficiency and availability are explicitly favored over consistency. Data Model and API Object:(id) -> (otype, (key -> value)) Association: (id1, atype, id2) -> (time, (key -> value)*) Association List: (id1, atype) -> [a1, a2, ..., an] Objects are typed nodes. Each object has a globally unique id. Associations are typed directed edges between objects; at most one association can exist between two objects. Both objects and associations may contain data as key->value pairs. Bidirectional edges are modeled as two associations. TAO keeps associations in sync with their inverses. TAO's association queries are organized around association lists(associations in descending order by the \"time\" field). A per-atype upper bound for each query is enforced( assoc_ranage(id1, atype, pos, limit) ), so the client must issue multiple queries to enumerate a longer association list. Architecture Basics TAO uses MySQL as persistent storage. Data is divided into logical shards and each database server is responsible for one or more shards. Each object id has an embedded shard_id(fixed for entire lifetime). An association is stored on the shard of its id1. Multiple caching servers together form a tier. Shards are mapped onto cache servers with consistent hashing. The TAO in-memory cache contains objects, association lists and association counts. Cache servers understand the semantics of the contents and can use them to answer queries not processed before(example: cache count zero for range query). Client issues requests directly to the appropriate cache server, which will complete the read/write request(contact other cache servers/databases if necessary). For write operation on an association with inverse, the cache server for id1 will contact the cache server for id2 and issue write to database after the inverse write is complete. TAO does not provide atomicity between two updates; hanging associations are repaired by an asynchronous job. Scaling Leader and Follower Tiers Larger tiers are problematic because they are prone to hot spots and have O(N^2) growth in all-to-all connections. We split the cache into two levels: a leader tier and multiple follow tiers. Leaders behave as the basic case, while followers will instead forward read misses and writes to leaders. To keep cache consistent, leaders asynchronously send cache maintenance messages to followers. A version number in message allows it to be ignored when arriving late. Geographically Cluster data center locations into a few regions where the intra-region latency is small. Store one copy of the social graph per region. Followers behave identically in all regions, forwarding read misses and writes to the local region's tier leader. Leaders query the local region's database regardless of whether master or slave. Writes are forwarded by the local leader to the leader in the master region. Master region is controlled separately for each shard and is automatically switched to recover from database failure. We prefer to locate all of the master databases in a single region, otherwise inverse write may introduce inter-region latency. Consistency After a write, TAO guarantees the eventual delivery of an invalidation or refill to all tiers. TAO reads marked as critical will be proxied to the master region for stronger consistency. Fault Tolerance Database failures: When a master database is down, one of its slave is promoted to be the new master. When a slave database is down, cache misses are redirected to leaders in the master region. Leader failures: Followers reroute read misses directly to the database and writes to a random replacement leader. Invalidation and refill failures: If a follower is unreachable, leader persists messages and redeliver them later. If these messages are lost due to permanent leader failure, a bulk invalidation operation is used to invalidate all data of specific shard in followers. Follower failures: Client requests are failed over to followers in other tiers.","title":"tao"},{"location":"tao/tao.html#tao-facebooks-distributed-data-store-for-the-social-graph","text":"A geographically distributed data store that provides efficient and timely access to the social graph.","title":"TAO: Facebook\u2019s Distributed Data Store for the Social Graph"},{"location":"tao/tao.html#background","text":"The content Facebook presents to each user is highly tailored, so we have to perform filtering and aggregation when the content is viewed instead of when the content is created. Facebook was originally built with MySQL/PHP/memcache. This look-aside key-value cache architecture has problems like inefficient edge lists, distributed control logic and expensive read-after-write consistency. TAO is designed to handle the workload of read-mostly access to a constantly changing graph. Efficiency and availability are explicitly favored over consistency.","title":"Background"},{"location":"tao/tao.html#data-model-and-api","text":"Object:(id) -> (otype, (key -> value)) Association: (id1, atype, id2) -> (time, (key -> value)*) Association List: (id1, atype) -> [a1, a2, ..., an] Objects are typed nodes. Each object has a globally unique id. Associations are typed directed edges between objects; at most one association can exist between two objects. Both objects and associations may contain data as key->value pairs. Bidirectional edges are modeled as two associations. TAO keeps associations in sync with their inverses. TAO's association queries are organized around association lists(associations in descending order by the \"time\" field). A per-atype upper bound for each query is enforced( assoc_ranage(id1, atype, pos, limit) ), so the client must issue multiple queries to enumerate a longer association list.","title":"Data Model and API"},{"location":"tao/tao.html#architecture","text":"","title":"Architecture"},{"location":"tao/tao.html#basics","text":"TAO uses MySQL as persistent storage. Data is divided into logical shards and each database server is responsible for one or more shards. Each object id has an embedded shard_id(fixed for entire lifetime). An association is stored on the shard of its id1. Multiple caching servers together form a tier. Shards are mapped onto cache servers with consistent hashing. The TAO in-memory cache contains objects, association lists and association counts. Cache servers understand the semantics of the contents and can use them to answer queries not processed before(example: cache count zero for range query). Client issues requests directly to the appropriate cache server, which will complete the read/write request(contact other cache servers/databases if necessary). For write operation on an association with inverse, the cache server for id1 will contact the cache server for id2 and issue write to database after the inverse write is complete. TAO does not provide atomicity between two updates; hanging associations are repaired by an asynchronous job.","title":"Basics"},{"location":"tao/tao.html#scaling","text":"Leader and Follower Tiers Larger tiers are problematic because they are prone to hot spots and have O(N^2) growth in all-to-all connections. We split the cache into two levels: a leader tier and multiple follow tiers. Leaders behave as the basic case, while followers will instead forward read misses and writes to leaders. To keep cache consistent, leaders asynchronously send cache maintenance messages to followers. A version number in message allows it to be ignored when arriving late. Geographically Cluster data center locations into a few regions where the intra-region latency is small. Store one copy of the social graph per region. Followers behave identically in all regions, forwarding read misses and writes to the local region's tier leader. Leaders query the local region's database regardless of whether master or slave. Writes are forwarded by the local leader to the leader in the master region. Master region is controlled separately for each shard and is automatically switched to recover from database failure. We prefer to locate all of the master databases in a single region, otherwise inverse write may introduce inter-region latency.","title":"Scaling"},{"location":"tao/tao.html#consistency","text":"After a write, TAO guarantees the eventual delivery of an invalidation or refill to all tiers. TAO reads marked as critical will be proxied to the master region for stronger consistency.","title":"Consistency"},{"location":"tao/tao.html#fault-tolerance","text":"Database failures: When a master database is down, one of its slave is promoted to be the new master. When a slave database is down, cache misses are redirected to leaders in the master region. Leader failures: Followers reroute read misses directly to the database and writes to a random replacement leader. Invalidation and refill failures: If a follower is unreachable, leader persists messages and redeliver them later. If these messages are lost due to permanent leader failure, a bulk invalidation operation is used to invalidate all data of specific shard in followers. Follower failures: Client requests are failed over to followers in other tiers.","title":"Fault Tolerance"},{"location":"zookeeper/zookeeper.html","text":"ZooKeeper: Wait-free coordination for Internet-scale systems A service for coordinating processes of distributed applications. MIT Notes , FAQ Basics ZooKeeper provides a coordination kernel for clients to implement primitives for dynamic configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper relaxes the conditions provided by Raft: reads are eventually consistent and can be served by replicas which increases read throughput. Zookeeper guarantees : 1) linearizable writes 2) FIFO client ordering for all operations, all requests from a given client are executed in the order that they were sent by the client. ZooKeeper target workload read to write ratio is 2:1 to 100:1 with data in the MB range. Best for Read Heavy applications such as storing configuration information since many servers will read this data and the data is small. Also, servers will pass a watch flag on reads on the configuration files and will be notified whenever they change. Service Overview znode ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. Three types of znodes: regular ephemeral (automatically removed when corresponding session terminates). sequential (when a file is created with a given name, ZooKeeper appends a number. ZooKeeper guarantees to never repeat a number if several clients try to write.) znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB). Client API ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages. Operations create(path, data, flags(regular, ephemeral, sequential)): returns error if alraedy exists unless sequential create delete(path, version): deletes if version matches exists(path, watch): watch is bool getData(path, watch) setData(path, data, version): sets if version matches getChildren(path, watch) sync(path): waits for all pending writes to complete Implementation ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. Snapshots are fuzzy since ZooKeeper state is not locked when taking the snapshot. After reboot, ZooKeeper constructs a consistent snapshot by replaying all log entries from the point at which the snapshot started. Because updates in Zookeeper are idempotent and delivered in the same order, the application-state will be correct after reboot. Atomic Broadcast Zab is an atomic broadcast protocol, uses simple majority quorums to decide on a proposal. Leader executes the requests and broadcasts the change to the ZooKeeper state through Zab. Zab guarantees the changes broadcast by a leader are delivered in order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes. TCP for transport so message order is maintained by the network. Use log to keep track of proposals as the write-ahead log for the in-memory database. Client Server Interactions Read is handled locally in memory. Each read request is processed and tagged with a zxid that corresponds to the last transaction seen by the server. Zxid defines the partial order of the read requests with respect to the write requests. Drawback: not guaranteeing precedence order for read operations, read may return a stale vlue. Should use sync flag to indicate follower to sync with leader. Sync: place sync operation at the end of the queue of requests between the leader and the server executing the call to sync. If pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction. Heartbeat: send heartbeat after the session has been idle for s/3 ms and switch to a new server if it has not heard from a server for 2s/3 ms. s is session timeout in ms. Use Cases Dynamic Configuration In A Distributed Application. Processes startup with the full pathname of z c , a znode storing dynamic configuration. Set watch flag to true, read config file, upon notified and read new configuration, again set the watch flag to true Rendezvous Client creates rendezvous node, z r and the full pathnameof z r as a startup parameter of the master and worker processes. When the master starts it fills in zr with information about addresses and ports it is using. When workers start, they read zr with watch set to true. Group Membership Designate node, z g to represent the group. When a process member of the group starts, it creates an ephemeral child znode under z g . Processes can obtain group information by simply listing the children of z g . Mini Transactions - Effect is that we can achieve atomic operations. Example of atomic counter: while true : x , v = getData ( \"f\" ) if setData ( \"f\" , x + 1 , v ): break sleep Simple Locks Without Herd Effect (Scalable Locks)","title":"zookeeper"},{"location":"zookeeper/zookeeper.html#zookeeper-wait-free-coordination-for-internet-scale-systems","text":"A service for coordinating processes of distributed applications. MIT Notes , FAQ","title":"ZooKeeper: Wait-free coordination for Internet-scale systems"},{"location":"zookeeper/zookeeper.html#basics","text":"ZooKeeper provides a coordination kernel for clients to implement primitives for dynamic configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper relaxes the conditions provided by Raft: reads are eventually consistent and can be served by replicas which increases read throughput. Zookeeper guarantees : 1) linearizable writes 2) FIFO client ordering for all operations, all requests from a given client are executed in the order that they were sent by the client. ZooKeeper target workload read to write ratio is 2:1 to 100:1 with data in the MB range. Best for Read Heavy applications such as storing configuration information since many servers will read this data and the data is small. Also, servers will pass a watch flag on reads on the configuration files and will be notified whenever they change.","title":"Basics"},{"location":"zookeeper/zookeeper.html#service-overview","text":"","title":"Service Overview"},{"location":"zookeeper/zookeeper.html#znode","text":"ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. Three types of znodes: regular ephemeral (automatically removed when corresponding session terminates). sequential (when a file is created with a given name, ZooKeeper appends a number. ZooKeeper guarantees to never repeat a number if several clients try to write.) znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB).","title":"znode"},{"location":"zookeeper/zookeeper.html#client-api","text":"ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages.","title":"Client API"},{"location":"zookeeper/zookeeper.html#operations","text":"create(path, data, flags(regular, ephemeral, sequential)): returns error if alraedy exists unless sequential create delete(path, version): deletes if version matches exists(path, watch): watch is bool getData(path, watch) setData(path, data, version): sets if version matches getChildren(path, watch) sync(path): waits for all pending writes to complete","title":"Operations"},{"location":"zookeeper/zookeeper.html#implementation","text":"ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. Snapshots are fuzzy since ZooKeeper state is not locked when taking the snapshot. After reboot, ZooKeeper constructs a consistent snapshot by replaying all log entries from the point at which the snapshot started. Because updates in Zookeeper are idempotent and delivered in the same order, the application-state will be correct after reboot.","title":"Implementation"},{"location":"zookeeper/zookeeper.html#atomic-broadcast","text":"Zab is an atomic broadcast protocol, uses simple majority quorums to decide on a proposal. Leader executes the requests and broadcasts the change to the ZooKeeper state through Zab. Zab guarantees the changes broadcast by a leader are delivered in order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes. TCP for transport so message order is maintained by the network. Use log to keep track of proposals as the write-ahead log for the in-memory database.","title":"Atomic Broadcast"},{"location":"zookeeper/zookeeper.html#client-server-interactions","text":"Read is handled locally in memory. Each read request is processed and tagged with a zxid that corresponds to the last transaction seen by the server. Zxid defines the partial order of the read requests with respect to the write requests. Drawback: not guaranteeing precedence order for read operations, read may return a stale vlue. Should use sync flag to indicate follower to sync with leader. Sync: place sync operation at the end of the queue of requests between the leader and the server executing the call to sync. If pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction. Heartbeat: send heartbeat after the session has been idle for s/3 ms and switch to a new server if it has not heard from a server for 2s/3 ms. s is session timeout in ms.","title":"Client Server Interactions"},{"location":"zookeeper/zookeeper.html#use-cases","text":"Dynamic Configuration In A Distributed Application. Processes startup with the full pathname of z c , a znode storing dynamic configuration. Set watch flag to true, read config file, upon notified and read new configuration, again set the watch flag to true Rendezvous Client creates rendezvous node, z r and the full pathnameof z r as a startup parameter of the master and worker processes. When the master starts it fills in zr with information about addresses and ports it is using. When workers start, they read zr with watch set to true. Group Membership Designate node, z g to represent the group. When a process member of the group starts, it creates an ephemeral child znode under z g . Processes can obtain group information by simply listing the children of z g . Mini Transactions - Effect is that we can achieve atomic operations. Example of atomic counter: while true : x , v = getData ( \"f\" ) if setData ( \"f\" , x + 1 , v ): break sleep Simple Locks Without Herd Effect (Scalable Locks)","title":"Use Cases"}]}