{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"About the Site This site contains my notes when reading papers. As I only write down important or interesting parts for me, they may not be good summaries. Fields of Interest I'm mainly interested in large-scale data systems including Ethereum and block-chain based systems. \"A future is not given to you. It is something you must take for yourself.\"","title":"Home"},{"location":"aurora/aurora.html","text":"Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases A cloud-native relational database service for OLTP workloads. MIT Notes FAQ Overview We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora uses a novel architecture with a fleet of database instances and storage service. Several database functions(redo logging, crash recovery, etc) are offloaded to the storage service, which is like a virtualized segmented redo log (shared-disk architecture). Aurora uses two big ideas : Quorum writes for better fault-tolerance without too much waiting Storage servers understand how to apply DB's log to data pages, so only need to send (small) log entries, not (big) dirty pages. Sending to many replicas, but not much data. The log is the database; any page that the storage system materializes are simply a cache of log application. Three advantages over traditional approaches to traditional distributed databases First, by building storage as an independent faulttolerant and self-healing service across multiple data-centers, we protect the database from performance variance and transient or permanent failures at either the networking or storage tiers. Second, by only writing redo log records to storage, we are able to reduce network IOPS by an order of magnitude Third, we move some of the most complex and critical functions (backup and redo recovery) from one-time expensive operations in the database engine to continuous asynchronous operations amortized across a large distributed fleet. This yields near-instant crash recovery without checkpointing as well as inexpensive backups that do not interfere with foreground processing. HLD of Architecture To tolerate AZ failure, Aurora replicates each data item 6 ways across 3AZs with 2 copies in each AZ. Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. The only writes that cross the network are redo log records, so network load is drastically reduced despite amplifying write for replication. Storage nodes gossips with peers to fill gaps in received log records. Durable log application happens at the storage nodes continuously and asynchronously. Each log record has a monotonically-increasing Log Sequence Number(LSN). Instead of 2PC protocol, Aurora maintains points of consistency and durability and advances them when receiving acknowledgements for storage requests. Durability: the highest LSN at which all prior log records are available. Consistency: each transaction is broken up to mini-transactions, and the final log record in a mini-transaction is a consistency point. Normally read quorum is not needed since the database feeds log records to storage nodes and tracks progress. : Durability V nodes, read quorum V_r , write quorum V_w To ensure each write is aware of the most recent write: V_w > V/2 Read = max_version(all nodes), so V_r + V_w > V , it can ensure the request must be accepted by a node with most update data. Because each log entry have an index/version, we only pick the date with the most updated log. AZ (availability zone) level failure tolerance Losing an entire AZ and one additional node (AZ+1) without losing data Losing an entire AZ without impacting the ability to write data AZ = 3, V = 6, V_w = 4, V_r = 3 Database volume is partitioned into 10GB segments. Each segment is replicated 6 times into a Protection Group. Advantages of storage server based on quorum mechanism : Smoother handling of server failures, slow execution or network partition problems, because each operation does not require a response from all replica servers (for example, as opposed to chain replication, chain replication needs to wait for write operations to be completed on all replicas) Under the premise of satisfying W+R>V , W and R can be adjusted. For different read and write load conditions, if the read load is relatively large, R can be reduced, and vice versa Raft also uses the quorum mechanism: leader will submit the log entry only after most copies are written to the log entry , but Raft is more powerful: it can handle more complex operations (due to its sequential operations); leader can be automatically re-elected when a split-brain problem occurs The Log Is The Database Database execution process Paper assumes you know how DB works, how it uses storage. Let's describe the execution process of the write operation of a single-machine general transaction database. The data is stored in the B-Tree of the hard disk, and there are cached data pages in the database. Take the transaction x=x+10 y=y-10 as an example: First lock x and y DB server modifies only cached data pages as transaction runs and appends update info to Write-Ahead Log (redo log) At this time log entry can be expressed as: LSID TID Key old new Notes 101 7 x 500 510 x=x+10 102 7 y 750 740 y=y-10 103 7 commit transaction finished Release the locks of x and y after WAL is written to the hard disk, and reply to client Log Applicator acts on the modification of the log entry on the before image of the cached data page, which will generate an after image. Delayed writing can optimize performance because the data page is large Crash Recovery Replay all committed transactions in log ( redo ) Roll back all uncommitted transactions in log ( undo ) Aurora Log Processing In Aurora , Log Procesing is pushed down to the storage layer to generate data pages in the background or on-demand. The write data transmitted through the network is only REDO logs, thus reducing the network load , And provides considerable performance and durability. Extra Notes In shared-disk architecture, all data is shared by all nodes. In shared-nothing architecture, each node manages a subset of data. It is hard to use change buffer in shared-disk architecture, so writes are penalized when there are secondary indexes.*","title":"aurora"},{"location":"craq/craq.html","text":"Object Storage on CRAQ High-throughput chain replication for read-mostly workloads. MIT Notes , FAQ 1. Introduction Chain Replication with Apportioned Queries (CRAQ) is an improvement to chain replication. It distributes the load on all object copies to greatly improve read throughput while maintaining strong consistency. This article mainly summarizes the chain replication, the principle of CRAQ , and the consistency model of CRAQ . 1.1 Chain Replication Chain Replication (CR) is a method of replicating data across multiple nodes: The nodes form a chain of length C The head node of the chain handles all write operations from the client When a node receives a write operation, it will propagate to every node in the chain Once the write reaches the tail node, it is applied to all copies in the chain and is considered committed When the tail node submits a write operation, it will notify up the chain and head will respond to the client The tail node handles all read operations, so only the submitted value can be returned by the read operation Chain replication achieves strong consistency : Since all read operations are performed at the tail, and all write operations are committed at the tail, the chain tail can simply apply a total sequence to all operations. Tradeoffs vs Raft Both CRAQ and Raft/Paxos are replicated state machines. They can be used to replicate any service that can be fit into a state machine mold (basically, processes a stream of requests one at a time). One application for Raft/Paxos is object storage. CR and CRAQ are likely to be faster than protocols like Raft that provide strong consistency because the CR head does less work than the Raft leader: the CR head sends writes to just one replica, while the Raft leader must send all operations to all followers. CR has a performance advantage for reads as well, since it serves them from the tail (not the head), while the Raft leader must serve all client requests. However, Raft/Paxos and CR/CRAQ differ significantly in their failure properties. Raft (and Paxos and ZooKeeper) can continue operating (with no pauses at all) even if a minority of nodes are crashed, slow, unreliable, or partitioned. A CRAQ or CR chain must stop if something like that goes wrong, and wait for a configuration manager to decide how to proceed. On the other hand the post-failure situation is significantly simpler in CR/CRAQ; recall Figures 7 and 8 in the Raft paper. Failure recovery for chain replication: When the head node fails: the subsequent node replaces it as the head node, and there is no missing committed write operation When the tail node fails: the previous node replaces it as the tail node, and there is no lost write operation When the intermediate node fails: removed from the chain, the previous node needs to resend the most recent write operation Limitations : All reads of an object must go to the tail node, resulting in heavy load. 2. CRAQ 2.1 CRAQ Principles CRAQ is an improvement of chain replication which allows any node in the chain to perform read operations: CRAQ Each node can store multiple versions of an object, and each version contains a monotonically increasing version number and an additional attribute (identifying clean or dirty ) When a node receives a new version of an object (via a write operation that propagates down), the node appends this latest version to the list of the object If the node is not the tail node, mark the version as dirty and pass the write operation to subsequent nodes If the node is the tail node, the version is marked as clean , at this time the write operation is committed . Then, the tail node sends ACK back in the chain to notify other nodes to submit When the ACK of the object version arrives at the node, the node will mark the object version as clean . The node can then delete all previous versions of the object When the node receives a read request from the object: -If the latest known version of the requested object is clean, the node will return this value -Otherwise, the node will contact the tail node and ask for the last submitted version number of the object on the tail node, and then the node will return this version of the object 2.2 Performance Improvement CRAQ 's throughput improvement over CR occurs in two different situations: Read-intensive workload : Read operations can be performed on all nodes, so throughput is linearly proportional to chain length Write-intensive workload : In workloads with a large number of write operations, it is easier to read dirty data, so there are more query requests for tail nodes. However, the workload of querying the tail node is much lower than that of all read requests performed by the tail node, so the throughput of CRAQ is higher than that of CR . 2.3 Consistency Model For read operations, CRAQ supports three consistency models: Strong Consistency : The read operation described in 4.1 enables the latest written data to be read for each read, thus providing strong consistency Eventual consistency : Allow nodes to return uncommitted new data, that is, allow client to read inconsistent object versions from different nodes. But for a client , since it establishes a session with the node, its read operation is guaranteed to be monotonous and consistent. Eventual consistency with maximum inconsistency boundary : Nodes are allowed to return uncommitted new data, but there is a limit of inconsistency. This limit can be based on version or time. For example, it is allowed to return newly written but uncommitted data within a period of time. 2.4 ZooKeeper Coordination Service If the network connection between two adjacent nodes is disconnected, the subsequent node will want to become the head node, which will result in two head nodes. CRAQ itself will not solve such a problem, so an external distributed coordination service is needed to solve this problem, such as using ZooKeeper . ZooKeeper determines the composition of the chain, determines which node is the head and tail, and monitors which node has failed. When a network failure occurs, ZooKeeper determines the new composition of the chain, not based on each node's own perception of the network situation. Through the use of Zookeper watch flags , CRAQ nodes are guaranteed to receive a notification when nodes are added to or removed from a group. Similarly, a node can be notified when metadata in which it has expressed interest changes. During initialization, a CRAQ node creates an ephemeral file in /nodes/dc_name/node_id . CRAQ nodes can query /nodes/dc_name to determine the membership list for its datacenter, but instead of having to periodically check the list for changes, ZooKeeper provides processes with the ability to create a watch on a file. A CRAQ node, after creating an ephemeral file to notify other nodes it has joined the system, creates a watch on the children list of /nodes/dc_name , thereby guaranteeing that it receives a notification when a node is added or removed. 3.1 Alternate Approaches To Improving Chain Replication A data center will probably have lots of distinct CR chains, each serving a fraction (shard) of the objects. Suppose you have three servers (S1, S2, and S3) and three chains (C1, C2, C3). Then you can have the three chains be: C1: S1 S2 S3 C2: S2 S3 S1 C3: S3 S1 S2 Now, assuming activity on the three chains is roughly equal, the load on the three servers will also be roughly equal. In particular the load of serving client requests (head and tail) will be roughly equally divided among the three servers. This is a pretty reasonable arrangement; CRAQ is only better if it turns out that some chains see more load than others . 4.1 Summary In Chain Replication, only the head and tail directly serve client requests; the other replicas help fault tolerance but not performance. Since the load on the head and tail is thus likely to be higher than the load on intermediate nodes, you could get into a situation where performance is bottlenecked by head/tail, yet there is plenty of idle CPU available in the intermediate nodes. CRAQ exploits that idle CPU by moving the read work to them.","title":"craq"},{"location":"distributedTransactions/distributedTransactions.html","text":"Distributed Transactions MIT Notes FAQ Background Problem: lots of data records, sharded on multiple servers, lots of clients Correct behavior of a xactions: ACID A: Atomicity, all writes or none, despite failures C: obeys application-specific invariants I: Isolation, no interference between xactions -- serializable D: Durability, committed writes are permanent Distributed Transactions have two big components: - concurrency control (to provide isolation/serializability) - atomic commit (to provide atomicity despite failure) Serializable Definition : there exists some serial order of those concurrent transactions that would, if followed, lead to the same ending state.* an easy model for programmers: they can write complex transactions while ignoring concurrency It allows parallel execution of transactions on different records example transactions x and y are bank balances -- records in database tables x and y are on different servers (maybe at different banks) x and y start out as $10 T1 and T2 are transactions T1: transfer $1 from x to y T2: audit, to check that no money is lost T1: T2: begin_xaction begin_xaction add(x, 1) tmp1 = get(x) add(y, -1) tmp2 = get(y) end_xaction print tmp1, tmp2 end_xaction execute concurrent transactions T1 and T2 T1; T2 : x=11 y=9 \"11,9\" T2; T1 : x=11 y=9 \"10,10\" the results for the two differ; either is OK Concurrency Control Distributed transactions have two big components: concurrency control (to provide isolation/serializability) atomic commit (to provide atomicity despite failure) Two classes of concurrency control for transactions: pessimistic: lock records before use conflicts cause delays (waiting for locks) optimistic: use records without locking commit checks if reads/writes were serializable conflict causes abort+retry called Optimistic Concurrency Control (OCC) pessimistic is faster if conflicts are frequent optimistic is faster if conflicts are rare Two-Phase locking Two-phase locking is a pessimistic form of concurrency control and one way to achieve serializability: a transaction must acquire a record's lock before using it a transaction must hold its locks until after commit or abort can result in deadlock. Systems must be smart enough to detect and abort. Atomic Commit - Two Phase Commit A bunch of computers are cooperating on some task, each computer has a different role. We want to ensure atomicity: all execute, or none execute. Challenges : failure and performance Process: Data is sharded among multiple servers Transactions run on \"transaction coordinators\" (TCs) For each read/write, TC sends RPC to relevant shard server Each is a \"participant\" who manages locks for its shard of the data There may be many concurrent transactions, many TCs TC assigns unique transaction ID (TID) to each transaction Every message, every table entry tagged with TID to avoid confusion TC A B |----put------>|(lock data) |-----------get---------->|(lock data) | | |---prepare--->| |----------prepare-------->| | |<---yes/no----| |<-----------yes/no--------| | | |-commit/abort->|(commit/abort and release lock) |---------commit/abort---->|(commit/abort and release lock) |<----ack-----| |<-------------ack---------| Why is this correct so far? - Neither A or B can commit unless they both agreed. Failure tolerance What if B crashes and restarts? If B sent YES before crash, B must remember (since it wrote to log despite crash)! Because A might have received a COMMIT and committed. So B must be able to commit (or not) even after a reboot. What if TC crashes and restarts? If TC might have sent COMMIT before crash, TC must remember! Since one worker may already have committed. write log to disk before sending COMMIT msgs. repeat COMMIT if it crashes and reboots, or if a participants asks. What if TC never gets a YES/NO from B? Perhaps B crashed and didn't recover; perhaps network is broken. TC can time out, and abort (since has not sent any COMMIT msgs). What if B times out or crashes while waiting for PREPARE from TC? B has not yet responded to PREPARE, so TC can't have decided commit B can unilaterally abort, and release locks respond NO to future PREPARE What if B replied YES to PREPARE, but doesn't receive COMMIT or ABORT? B cannot decide to abort it, because TC might have gotten YES from both, and sent out COMMIT to A, but crashed before sending to B. cannot do anything, just wait for TC came back Two-phase commit perspective Used in sharded DBs when a transaction uses data on multiple shards Bad reputation - Thus usually used only in a single small domain slow: multiple rounds of messages slow: disk writes locks are held over the prepare/commit exchanges; blocks other xactions TC crash can cause indefinite blocking, with locks held TC crash can cause indefinite blocking, with locks held Raft Comparison Raft and two-phase commit solve different problems! Use Raft to get high availability by replicating i.e. to be able to operate when some servers are crashed the servers all do the same thing Use 2PC when each participant does something different And all of them must do their part 2PC does not help availability since all servers must be up to get anything done Raft does not ensure that all servers do something since only a majority have to be alive What if you want high availability and atomic commit? The TC and servers should each be replicated with Raft Run two-phase commit among the replicated services Then you can tolerate failures and still make progress Whats described is basically Spanner","title":"distributed transactions"},{"location":"farm/farm.html","text":"FaRM: Distributed Transactions With Consistency, Availability, and Performance Design of new transaction, replication, and recovery protocols from first principles to leverage commodity networks with RDMA and a new, inexpensive approach to providing non-volatile DRAM. MIT Notes FAQ Overview FaRM provides distributed ACID transactions with strict serializability, high availability, high throughput and low latency FaRM uses optimistic concurrency control with a four phase commit protocol (lock, validation, commit backup, and commit primary) NVRAM FaRM writes go to RAM, not disk -- eliminates a huge bottleneck RAM write takes 200 ns, hard drive write takes 10 ms, SSD write 100 us, ns = nanosecond, ms = millisecond, us = microsecond RAM is normally volatile so NV achieved by attaching batteries to power supply units and writing the contents of DRAM to SSD when the power fails. What if crash prevents s/w from writing SSD? FaRM copes with single-machine crashes by copying data from RAM of machines' replicas to other machines to ensure always f+1 copies FaRM uses two networking ideas: - Kernel bypass - RDMA Kernel bypass [diagram: FaRM user program, CPU cores, DMA queues, NIC] application directly interacts with NIC -- no system calls, no kernel NIC DMAs into/out of user RAM FaRM s/w polls DMA areas to check for new messages CPU operations is what limits RPC (100,000 bits/), not wire between machines (10 gb/s) RDMA [src host, NIC, switch, NIC, target memory, target CPU] remote NIC directly reads/writes memory Sender provides memory address Remote CPU is not involved! This is \"one-sided RDMA\" Reads an entire cache line, atomically Distributed transactions and replication FaRM uses fewer messages than traditional protocols, and exploits one-sided RDMA reads and writes for CPU efficiency and low latency. FaRM uses primary-backup replication in non-volatile DRAM for both data and transaction logs, and uses unreplicated transaction coordinators that communicate directly with primaries and backups. FaRM uses optimistic concurrency control with read validation. Transactions use one-sided RDMA to read objects and they buffer writes locally. The coordinator also records the addresses and versions of all objects accessed At the end of the execution, FaRM attempts to commit the transaction by executing the following steps: Lock TC writes a LOCK record to the log on each machine that is a primary for any written object, containing versions and new values of all written objects on that primary as well as the list of all regions with written objects. Primaries attempt to lock the objects at the specified versions using compare-and-swap. Locking can fail if : 1. Any object version changed since it was read by the transaction. 2. If the object is currently locked by another transaction. 3. In this case, the coordinator aborts the transaction. It writes an abort record to all primaries and returns an error to the application. Validate TC performs read validation by reading, from their primaries, the versions of all objects that were read but not written by the transaction. If any object has changed, validation fails and the transaction is aborted. Validation uses one-sided RDMA reads by default. For primaries that hold more than t_r objects, validation is done over RPC. The threshold t_r (currently 4) reflects the CPU cost of an RPC relative to an RDMA read. Commit backups TC writes a COMMITBACKUP record to the non-volatile logs at each backup and waits for an ack from the NIC hardware. Commit primaries TC writes a COMMITPRIMARY record to the logs at each primary. Primaries process these records by updating the objects in place, incrementing their versions, and unlocking them, which exposes the writes committed by the transaction. How does FaRM differ from Spanner? both replicate and use two-phase commit (2pc) for transactions Spanner: a deployed system focuses on geographic replication e.g. copies on East and West coasts, in case data centers fail is most innovative for read-only transactions -- TrueTime performance: r/w xaction takes 10 to 100 ms (Tables 3 and 6) FaRM a research prototype, to explore potential of RDMA all replicas are in same data center (wouldn't make sense otherwise) RDMA restricts design options: thus Optimistic Concurrency Control (OCC) performance: 58 microseconds for simple transactions (6.3, Figure 7) i.e. 100 times faster than Spanner performance: throughput of 100 million/second on 90 machines (Figure 7) extremely impressive, particularly for transactions+replication They target different bottlenecks: Spanner: speed of light and network delays FaRM: CPU time on servers","title":"farm"},{"location":"frangipani/frangipani.html","text":"Frangipani: A Scalable Distributed File System i 1990s scalable distributed file system that manages a collection of disks on multiple machines as a single shared pool of storage. MIT Notes FAQ Background What to digest: strong consistency cache coherence distributed transactions distributed crash recovery Overall design: A decentralized file system , cache for performance Petal: block storage service; what is stored: just like an ordinary hard disk file system directores i-node file content blocks free bitmaps Scenario 1: WS2(work station) run ls / or cat /grades while WS1 is modifying the same inode Scenario 2: WS1 and WS2 concurrently try to create /a , /b under same directory Scenario 3: WS1 crashes while creating a file operation: allocate i-node, initialize i-node, update directory Components Petal : block storage service; replicated Lock Server (LS) , with one lock per file/directory , the locks are named by files/directories (really i-numbers) file owner ----------- x WS1 y WS1 Workstation (WS) cache(store the lock status): file/dir lock content ----------------------- x busy ... //using data right now y idle ... //holds lock but not using the cached data right now Solution for Scenario 1: Cache Coherence (revealing writes) It is to guarantee linearizability AND caching Example: WS1 changes file z , then WS2 reads z WS1 changes file z request lock (WS1 -> LS) LS put: owner(z) = WS1 grant lock (LS -> WS1) (WS1: read+cache z data from Petal modify z locally when done, cached lock in state) WS2 try read file z Revoke Lock: Coherence Protocol Msg request lock (WS2 -> LS) grant (LS -> WS2) revoke (LS -> WS1) (write update log of metadata to Petal) (write modified z to Petal) release lock (WS1 -> LS) WS2 get the lock, and read z from Petal notes: locks and rules force reads to see last write one optimization: Frangipani has shared read locks, as well as exclusive write locks Solution for Scenario 2: Atomic Transactions (concealing writes) There is two operation for create/rename/delete a file, 1. create/rename initializes i-node, adds to directory entry. 2. rename. The challenge is to guarantee the atomicity. Transactional file-system operations: operation corresponds to a system call (create file, remove file, rename, &c) WS acquires locks on all file system data that it will modify performs operation with all locks held only releases when finished no other WS can see partially-completed operations Solution for Scenario 3: Crash Recovery (write-ahead logging) What if a Frangipani workstation dies while holding locks? eg: dead WS had modified data in its cache eg: dead WS had started to write back modified data to Petal Solution: Before writing any of op's cached blocks to Petal, first write log to Petal Log entry stored in Petal for recovery Before writing any of op's cached blocks to Petal, first write log to Petal if a crashed workstation has done some Petal writes for an operation, not not all. the writes can be completed from the log in Petal What an log entry include: note : it is just for file metadata, not for file data log sequence number array of updates: block #, new version #, addr, new bytes When WS receive lock revoke: Play log in the Petal (Petal already store it) (P) WS send the cached updated blocks to Petal (WS1-P) Release the lock (WS1-LS) Why version number is necessary: for linearizability of the metadata update WS1: delete(d/f)(v1) crash WS2: create(d/f)(v2) WS3: recover WS1 WS3 is recovering WS1's log -- but it doesn't look at WS2's log When WS1 delete the file, it add a log entry with v1 when WS2 create a file with same name, the system will give it a version number v2 so when replay the log of ws1, the v1<v2. so ignore it.","title":"frangipani"},{"location":"gfs/gfs.html","text":"The Google File System A scalable distributed file system for large distributed data-intensive applications. Introduction The design is driven by key observations different from earlier file system assumptions: frequent component failures, huge files (GB+), most files are mutated by appending rather than overwriting. GFS provides a POSIX-like file system interface. It supports snapshot and record append operations(useful for multi-way merge and consumer/producer). Architecture Figure 1 A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients. Files are divided into fixed-sized chunks(64 MB, larger than typical) and each chunk has an immutable and globally unique id. Chunkservers store chunks as local Linux files and the master maintains all file system metadata. The master stores in memory three majors types of metadata: the file and chunk namespaces, the mapping from files to chunks and the locations of each chunk's replicas. The first two are also persisted in a replicated operation log(with checkpoints); the last one is polled from chunkservers at start time and kept up-to-date via heartbeat messages. Reads Using the fixed chunk size, the client translates the file name and byte offset into a chunk index within the file. Sends the master a request containing the file name and chunk index. The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the file name and chunk index as the key. The client then sends a request to one of the replicas,most likely the closest one. The request specifies the chunk handle and a byte range within that chunk. Further reads of the same chunk require no more client-master interaction until the cached information expires. Chunk Size Key design parameters, Large chunk chosen : 64MB Advantages Reduces clients' need to interact with the master Operations are more likely to target at the same chunk, can reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time Reduces metadata size, easier for master to store in memory Disadvantages Small files -> small # of chunks -> the chunkserver becomes a hot spot Operation Log Contains a historical record of critical metadata changes. Replicated on multiple remote machines. Respond to a client operation only after flushing the corresponding log record to disk both locally and remotely. Recovers file system by replaying the log. Master checkpoints its state in a compact B-tree like form whenever the log grows beyond a certain size. Only keeps latest complete checkpoint and subsequent log files. Figure 2 System Interactions Leases and Mutation Order For mutations to a chunk, the master grants an extendable chunk lease to one of the replicas. The primary will pick a serial order for all mutations to the chunk and all replicas will follow this order. Control and data flows are decoupled to use the network efficiently. Control flows from the client to the primary and then to all secodaries. Data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. GFS provides at-least-once semantics. A client retries when mutation fails, so replicas of the same chunk may contain different data including duplicates or paddings(not bytewise identical). Writes Client asks the master which chunkserver holds the current lease for the chunk and replica locations. Master replies with primary and secondaries locations. Caches this until primary unavailable/no longer has lease. Client pushes the data to all the replicas. Each chunkserver will store the data in an internal LRU buffer cache. After all the replicas ack, client sends a write request to the primary. Primary assigns write order to all mutations it receives. Primary forwards the write request to all secondary replicas with mutation order. Secondaries ack to primary on completion. Primary replies to client. Failure -> client retry. Master Operations GFS namespace is represented as a lookup table(with prefix compression) from full pathnames to metadata. Each file or directory has a read-write lock to ensure proper searialization. The master create chunk replicas for initial creation, re-replication and rebalancing. It considers various factors when placing the replica, to maximizing data reliability/availability/network bandwidth utilization. When a file is deleted, the master logs the deletion immediately but deletes metadata after some delay(3 days, etc). Each chunkserver reports a subset of chunks in heartbeat messages, and the master replies whether these chunks can be deleted. This lazy approach is simple and reliable in a large-scale distributed system. The master maintains a version number for each chunk. Whenever the master grants a new lease on the chunk, it increases the version number and informs all the up-to-date replicas. For high availability, we have (1) monitoring infrastructure outside GFS to start a master and (2) \"shadow\" masters for read-only operations. Fault Tolerance High Availability Fast Recovery - Both the master and the chunkserver are designed to restore their state and start in seconds. Chunk Replication - default 3 replications per chunk. Master Replication - Operation log and checkpoints replicated on multiple machines. \"Shadow\" masters above. Data Integrity Breaks each 64 MB chunk into blocks of 64 KB, each with its own 32-bit checksum stored in memory and written to the log. For reads, the chunkserver verifies the checksum of datablocks that overlap the read range before returning any data to the requester If a block does not match the recorded checksum, the chunkserver returns an error and and reports the mismatch to the master. In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica. Summary Good Ideas High Availability through fast recovery of master and chunkserver. Separation of naming (master) from storage (chunkserver). Sharding (chunk replication) for parallel throughput. Primary to sequence writes. Leases to prevent split-brain chunkserver primaries. Not So Good Single master performance : Ran out of RAM and CPU as file count increased 1000x Chunkservers not very efficient for small files Lack of automatic fail-over to master replica","title":"gfs"},{"location":"mapreduce/mapreduce.html","text":"MapReduce: Simplified Data Processing on Large Clusters A programming model and an associated implementation for processing and generating large data sets. Introduction Many computation tasks are conceptually straightforward, but given the size of input data, the computations have to be distributed across machines to finish in a reasonable amount of time. MapReduce is an abstraction that can express many computation tasks while hiding details of parallelization, fault-tolerance, data distribution and load balancing. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. map (k1,v1) \u2192 list(k2,v2) reduce (k2,list(v2)) \u2192 list(v2) Implementation Execution A single master assigns tasks to workers; there are M map tasks and R reduce tasks in total. For map task, worker reads input, applies user-defined Map function and periodically writes intermediate results buffered in memory to local disk partitioned into R regions. The locations of these buffered pairs on disk are passed back to the master, who forwards these locations to the reduce workers. For reduce task, worker uses rpcs to read intermediate results on map workers' local disks, sorts intermediate results to group occurrences of the same key, applies user-defined Reduce function and writes final results to a global file system. Master is responsible of propagating the locations of intermediate files from map tasks to reduce tasks. Master Data Structures stores state(idle, in-progress, competed) and identity of the worker machine for each task (non idle). for each completed map task: locations and sizes of the R intermediate file regions. The information is pushed incrementally to in-progress reduce workers. Fault Tolerance For worker failure, master periodically pings workers and marks the worker that has no response for a certain amount of time as failed. Completed and in-progress map tasks are reset to idle and reduce workers executing are notified. Completed map tasks and any in-progress tasks on that worker are rescheduled to other workers; no need to re-execute completed reduce tasks because output is stored in global file system instead of worker's local disk. For master failure, the computation is just aborted and it is client's responsibility to check and retry. Miscellaneous Locality: master attempts to schedule map tasks on or close to the machines that contains corresponding input data(input data managed by GFS is also stored in the cluster). Task granularity: ideally M and R should be large to improve load balancing and speed up failure recovery, but there are practical bounds since master needs to keep O(M\\times R) states in memory and each reduce task produces a separate output file. Backup tasks: when the MapReduce is close to completion, master schedules backup executions for remaining in-progress tasks to alleviate the problem of stragglers. MR takes 44% longer without backup tasks. Refinements Partitioning Function: Typically hash(key) mod R is used for hashing. However, it is useful to allow custom partitioning function so that for example users can have all URLs from the same host end up in the same output file. Combiner Function: In some cases, there is significant repetition in the intermediate keys produced by each map task. For example, word count map produces <word, 1> for each word. All of these counts will be sent over RPC. Better allow user to specify optional Combiner that does partial merging of data before sent to reducers.","title":"mapreduce"},{"location":"raft/raft.html","text":"Raft: In Search of an Understandable Consensus Algorithm A consensus algorithm for managing a replicated log MIT Notes Part1 Part2 MIT FAQ Part1 Part2 Raft Basics Raft decomposes the consensus problem into three relatively independent subproblems Leader Election Log replication Safety Properties Election Safety : at most one leader can be elected in a given term. Leader Append-Only : a leader never overwrites or deletes entries in its log; it only appends new entries. Log Matching : if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. Leader Completeness : if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. State Machine Safety : if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. Raft divides time into terms of arbitrary length. Used extensively in leader election and log matching. 1. Leader Election When servers start up, they begin as followers Leader sends periodic heartbeats (AppendEntries RPC with no log entries) to all followers to maintain authority. If follower receives no communication over election timeout , then it assumes there is no viable leader and begins an election to choose a new leader. Begin an election: follower increments its current term and transitions to candidate state, votes for itself and issue RequestVoteRPCs in parallel to each else Candidate continue in this state until one of three things happens: a. it wins b. other wins c. no one wins Candidate wins if it receives the majority of votes, voting is first-come-first-served. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If its term is larger or equal than itself's, the candidate recognizes the leader as legit and returns to follower state Case : No one wins -> after timeout, new election (150ms - 300ms) - Raft uses randomized election timeouts to ensure that split votes are rare. 2. Log Replication I. Steps Leader appends log entry to its own log Send AppendEntry to every server When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. II. Commit A log entry is committed once the leader that created the entry has replicated it on a majority of the servers Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines This also commits all preceding entries in the leader's log, including entries created by previous leaders Leader keep tracks of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs. Once a follower learns that a log entry is committed, it applies the entry to its local state machine. III. Inconsistency Handling Leader maintains a nextIndex fields for each follower. It initializes nextIndex values to the index just after the last one in its log. If follower's log is inconsistent with the leader's, the AppendEntries RPC consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC 3. Safety Ensure each state machine executes same commands in the same order I. Election Restriction Candidate only elected as leader when its log is at least as up-to-date as any other log in that majority Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. Term1 > Term2 -> Term1 wins Term1 == Term2 -> Log with greater length wins II. Committing entries from previous terms Never commits log entries from previous terms by counting replicas. Only log entries from the leader's current term are committed by counting replicas Once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. Log Compaction InstallSnapshot RPC As Raft log grows without bound, it occupies more space and takes more time to replay on server startup. Snapshotting - saves entire system state to a snapshot on stable storage and the entire log up to that point is discarded. Each server takes snapshots independently covering just the committed entries in its log. Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind. This happens when the leader has already discarded the next log entry that it needs to send to a follower. Summary","title":"raft"},{"location":"spanner/spanner.html","text":"Spanner: Google\u2019s Globally-Distributed Database Google\u2019s scalable, multi-version, globally-distributed and synchronously-replicated database. MIT Notes FAQ Introduction Bigtable is hard to use for applications need complex, evolving schemas or strong consistency in wide-area replication, so Spanner evolved into a temporal, multi-version, semi-relational database, providing distributed transactions over geographically distributed data. With a new TrueTime API that exposes clock uncertainty, Spanner assigns globally-meaningful commit timestamp to even distributed transactions. Spanner shards data across many sets of Paxos state machines in data centers spread all over the world. Workload is dominated by read-only transactions (Table 6). Strong consistency (External consistency / linearizability / serializability) Basic Organization Datacenter A: \"clients\" are web servers e.g. for gmail data is sharded over multiple servers: a-m n-z Datacenter B: has its own local clients and its own copy of the data shards a-m n-z Datacenter C: same setup Replication Replication managed by Paxos; one Paxos group per shard. Replicas are in different data centers. The benefit of shard across data center Sharding allows huge total throughput via parallelism. Datacenters fail independently -- different cities. Clients can read local replica -- fast! Can place replicas near relevant customers. Paxos requires only a majority -- tolerate slow/distant replicas. Challenges Read of local replica must yield fresh data. So it have the same overhead as Write request, vote via majority, It eliminate the benefit of locality. A transaction may involve multiple shards -> multiple Paxos groups. Transactions that read multiple records must be serializable. It means it need the assistance from Xaction Coordinator. System Architecture A Spanner zone has one zonemaster and many spanservers. The former assigns data to spanservers; the latter serve data to clients. A spanserver is responsible for 100~1000 tablets. Each tablet contains a bag of mappings (key:string, timestamp:int64)\\rightarrow string from multiple partitions of the row space(data colocation). A tablet's state is stored in a set of B-tree like files and a write-ahead log on Colossus. A spanserver implements a Paxos state machine on each tablet; each Paxos state machine stores metadata and log in corresponding tablet. At every replica that is a leader, each spanserver implements a lock table and a transaction table to support distributed transactions(two-phase commit on mutliple Paxos groups). TrueTime Main method TT.now() returns an interval [earliest, latest] that is guaranteed to contain the absolute time when TT.now() is invoked. TT.after(t) and TT.before(t) are convenience wrappers. TrueTime uses two forms of time references, GPS and atomic clocks, as they have different failure modes. TrueTime is implemented by a set of time master machines per data center and a time slave daemon per machine. Each daemon polls a variety of masters, apply Marzullo's algorithm to detect liars and synchronize the local machine clock to non-liars. Between synchronizations, a daemon advertises a slowly increasing time uncertainty \\epsilon (average 4ms in production) derived from worst-case local clock drift, time master uncertainty and communication delay to the time masters. Concurrency Control Spanner supports read-write transactions, read-only transactions and snapshot reads. Read-only transactions and snapshot reads execute at a specific timestamp without locking. Spanner maintains following invariants: disjointness: * within a Paxos group, each Paxos leader's lease interval is disjoint from every other leader's. monotonicity: within a Paxos group, Paxos writes have monotonically increasing timestamp. external consistency: if the start of a transaction T_2 occurs after the commit of a transaction T_1 , then the commit timestamp of T_2 must be greater than the commit timestamp of T_1 . For RW transactions, the coordinator assigns a commit timestamp S_i no later than TT.now().latest computed after receiving the commit request, and waits until TT.after(S_i) is true to apply the commit(\"commit wait\"). Sets the commit time to TT.now().latest since TT.now() returns a range time is between, exclusive so latest is guaranteed to not have occured. Keeps calling TT.now() until TT.now().earliest() is greater than above commit time. For RO transactions and snapshot reads, every replica tracks t_{safe} for the maximum at which the replica is up-to-date, which depends on the Paxos state machine and if there are prepared but not committed transactions. R/W Transactions BEGIN x = x + 1 y = y - 1 END We don't want any read or write of x or y sneaking between our two ops. After commit, all reads should see our updates. Two-phase commit (2pc) with Paxos-replicated participants. Client picks a unique transaction id (TID). Client sends each read to Paxos leader of relevant shard (2.1). Each shard first acquires a lock on the relevant record. May have to wait. Separate lock table per shard, in shard leader. Read locks are not replicated via Paxos, so leader failure -> abort. Client keeps writes private until commit. When client commits (4.2.1): Chooses a Paxos group to act as 2pc Transaction Coordinator (TC). Sends writes to relevant shard leaders. Each written shard leader: Acquires lock(s) on the written record(s). Log a \"prepare\" record via Paxos, to replicate lock and new value. Tell TC it is prepared. Or tell TC \"no\" if crashed and thus lost lock table. Transaction Coordinator: Decides commit or abort. Logs the decision to its group via Paxos. Tell participant leaders and client the result. Each participant leader: Log the TC's decision via Paxos. Release the transaction's locks. Locking (two-phase locking) ensures serializability. 2pc widely hated b/c it blocks with locks held if TC fails. Replicating the TC with Paxos solves this problem! R/O Transactions Eliminates two big costs from R/O transactions Read from local replicas, to avoid Paxos and cross-datacenter msgs. But note local replica may not be up to date! No locks, no two-phase commit, no transaction manager. Again to avoid cross-data center msg to Paxos leader. And to avoid slowing down r/w transactions. Tables 3 and 6 show a 10x latency improvement as a result! Achieves this through Snapshot Isolation Synchronize all computers' clocks (to real wall-clock time). Assign every transaction a time-stamp. r/w: commit time. r/o: start time. Execute as if one-at-a-time in time-stamp order. Even if actual reads occur in different order. Each replica stores multiple time-stamped versions of each record. All of a r/w transactions's writes get the same time-stamp. An r/o transaction's reads see version as of xaction's time-stamp. The record version with the highest time-stamp less than the xaction's. Obviously we can't synchronize all computer clocks, so TrueTime is used to give bounds of certainty. Perspective Snapshot Isolation gives you serializable r/o transactions. Timestamps set an order. Snapshot versions (and safe time) implement consistent reads at a timestamp. Xaction sees all writes from lower-TS xactions, none from higher. Any number will do for TS if you don't care about external consistency. Synchronized timestamps yield external consistency . Even among transactions at different data centers. Even though reading from local replicas that might lag. Why is all this useful? Fast r/o transactions: Read from replica in client's datacenter. No locking, no two-phase commit. Thus the 10x latency improvement in Tables 3 and 6. Although: r/o transaction reads may block due to safe time, to catch up. r/w transaction commits may block in Commit Wait. Accurate (small interval) time minimizes these delays.","title":"spanner"},{"location":"tao/tao.html","text":"TAO: Facebook\u2019s Distributed Data Store for the Social Graph A geographically distributed data store that provides efficient and timely access to the social graph. Background The content Facebook presents to each user is highly tailored, so we have to perform filtering and aggregation when the content is viewed instead of when the content is created. Facebook was originally built with MySQL/PHP/memcache. This look-aside key-value cache architecture has problems like inefficient edge lists, distributed control logic and expensive read-after-write consistency. TAO is designed to handle the workload of read-mostly access to a constantly changing graph. Efficiency and availability are explicitly favored over consistency. Data Model and API Object:(id) -> (otype, (key -> value)) Association: (id1, atype, id2) -> (time, (key -> value)*) Association List: (id1, atype) -> [a1, a2, ..., an] Objects are typed nodes. Each object has a globally unique id. Associations are typed directed edges between objects; at most one association can exist between two objects. Both objects and associations may contain data as key->value pairs. Bidirectional edges are modeled as two associations. TAO keeps associations in sync with their inverses. TAO's association queries are organized around association lists(associations in descending order by the \"time\" field). A per-atype upper bound for each query is enforced( assoc_ranage(id1, atype, pos, limit) ), so the client must issue multiple queries to enumerate a longer association list. Architecture Basics TAO uses MySQL as persistent storage. Data is divided into logical shards and each database server is responsible for one or more shards. Each object id has an embedded shard_id(fixed for entire lifetime). An association is stored on the shard of its id1. Multiple caching servers together form a tier. Shards are mapped onto cache servers with consistent hashing. The TAO in-memory cache contains objects, association lists and association counts. Cache servers understand the semantics of the contents and can use them to answer queries not processed before(example: cache count zero for range query). Client issues requests directly to the appropriate cache server, which will complete the read/write request(contact other cache servers/databases if necessary). For write operation on an association with inverse, the cache server for id1 will contact the cache server for id2 and issue write to database after the inverse write is complete. TAO does not provide atomicity between two updates; hanging associations are repaired by an asynchronous job. Scaling Leader and Follower Tiers Larger tiers are problematic because they are prone to hot spots and have O(N^2) growth in all-to-all connections. We split the cache into two levels: a leader tier and multiple follow tiers. Leaders behave as the basic case, while followers will instead forward read misses and writes to leaders. To keep cache consistent, leaders asynchronously send cache maintenance messages to followers. A version number in message allows it to be ignored when arriving late. Geographically Cluster data center locations into a few regions where the intra-region latency is small. Store one copy of the social graph per region. Followers behave identically in all regions, forwarding read misses and writes to the local region's tier leader. Leaders query the local region's database regardless of whether master or slave. Writes are forwarded by the local leader to the leader in the master region. Master region is controlled separately for each shard and is automatically switched to recover from database failure. We prefer to locate all of the master databases in a single region, otherwise inverse write may introduce inter-region latency. Consistency After a write, TAO guarantees the eventual delivery of an invalidation or refill to all tiers. TAO reads marked as critical will be proxied to the master region for stronger consistency. Fault Tolerance Database failures: When a master database is down, one of its slave is promoted to be the new master. When a slave database is down, cache misses are redirected to leaders in the master region. Leader failures: Followers reroute read misses directly to the database and writes to a random replacement leader. Invalidation and refill failures: If a follower is unreachable, leader persists messages and redeliver them later. If these messages are lost due to permanent leader failure, a bulk invalidation operation is used to invalidate all data of specific shard in followers. Follower failures: Client requests are failed over to followers in other tiers.","title":"tao"},{"location":"zookeeper/zookeeper.html","text":"ZooKeeper: Wait-free coordination for Internet-scale systems A service for coordinating processes of distributed applications. MIT Notes , FAQ Basics ZooKeeper provides a coordination kernel for clients to implement primitives for dynamic configuration, group membership, leader election and distributed lock. ZooKeeper implements non-blocking API, so a client can have multiple outstanding operations at a time. ZooKeeper relaxes the conditions provided by Raft: reads are eventually consistent and can be served by replicas which increases read throughput. Zookeeper guarantees : 1) linearizable writes 2) FIFO client ordering for all operations, all requests from a given client are executed in the order that they were sent by the client. ZooKeeper target workload read to write ratio is 2:1 to 100:1 with data in the MB range. Best for Read Heavy applications such as storing configuration information since many servers will read this data and the data is small. Also, servers will pass a watch flag on reads on the configuration files and will be notified whenever they change. Service Overview znode ZooKeeper provides the abstraction of a set of data nodes(znodes) organized by hierarchical namespaces. znodes are in-memory data node stored in ZooKeeper. Three types of znodes: regular ephemeral (automatically removed when corresponding session terminates). sequential (when a file is created with a given name, ZooKeeper appends a number. ZooKeeper guarantees to never repeat a number if several clients try to write.) znodes are not for general data storage. Instead, they are used to store metadata or configuration of applications(typically 1MB). Client API ZooKeeper provides API for client to manipulating znodes like in a file system. For read methods, ZooKeeper implements watches to allow client to receive notification of changes. Watches are one-time triggers associated with a session(for example, getData(path, watch) ). For write methods, ZooKeeper accepts an optional expected version number(for example, setData(path, data, version) ). If set, the write succeeds only if the actual version number of znode matches the expected one. ZooKeeper client maintains session with ZooKeeper through heartbeat messages. Operations create(path, data, flags(regular, ephemeral, sequential)): returns error if alraedy exists unless sequential create delete(path, version): deletes if version matches exists(path, watch): watch is bool getData(path, watch) setData(path, data, version): sets if version matches getChildren(path, watch) sync(path): waits for all pending writes to complete Implementation ZooKeeper service comprises an ensemble of servers that each has replicated ZooKeeper data. One is leader and the rest are followers. Read requests are handled locally at each server, so it may return stale data since some committed transactions are not applied on that server yet. Write requests are forwarded to leader. Leader (1) calculates the new system state to transform write requests into idempotent transactions and (2) broadcast the state changes to other servers through atomic broadcast protocol ZAB. ZooKeeper uses TCP so message order is maintained by network. ZooKeeper uses replay log and periodic snapshots for recoverability. Snapshots are fuzzy since ZooKeeper state is not locked when taking the snapshot. After reboot, ZooKeeper constructs a consistent snapshot by replaying all log entries from the point at which the snapshot started. Because updates in Zookeeper are idempotent and delivered in the same order, the application-state will be correct after reboot. Atomic Broadcast Zab is an atomic broadcast protocol, uses simple majority quorums to decide on a proposal. Leader executes the requests and broadcasts the change to the ZooKeeper state through Zab. Zab guarantees the changes broadcast by a leader are delivered in order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes. TCP for transport so message order is maintained by the network. Use log to keep track of proposals as the write-ahead log for the in-memory database. Client Server Interactions Read is handled locally in memory. Each read request is processed and tagged with a zxid that corresponds to the last transaction seen by the server. Zxid defines the partial order of the read requests with respect to the write requests. Drawback: not guaranteeing precedence order for read operations, read may return a stale vlue. Should use sync flag to indicate follower to sync with leader. Sync: place sync operation at the end of the queue of requests between the leader and the server executing the call to sync. If pending queue is empty, the leader needs to issue a null transaction to commit and orders the sync after that transaction. Heartbeat: send heartbeat after the session has been idle for s/3 ms and switch to a new server if it has not heard from a server for 2s/3 ms. s is session timeout in ms. Use Cases Dynamic Configuration In A Distributed Application. Processes startup with the full pathname of z c , a znode storing dynamic configuration. Set watch flag to true, read config file, upon notified and read new configuration, again set the watch flag to true Rendezvous Client creates rendezvous node, z r and the full pathnameof z r as a startup parameter of the master and worker processes. When the master starts it fills in zr with information about addresses and ports it is using. When workers start, they read zr with watch set to true. Group Membership Designate node, z g to represent the group. When a process member of the group starts, it creates an ephemeral child znode under z g . Processes can obtain group information by simply listing the children of z g . Mini Transactions - Effect is that we can achieve atomic operations. Example of atomic counter: while true : x , v = getData ( \"f\" ) if setData ( \"f\" , x + 1 , v ): break sleep Simple Locks Without Herd Effect (Scalable Locks)","title":"zookeeper"}]}